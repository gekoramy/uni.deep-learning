{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gekoramy/uni.deep-learning/blob/attention/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchinfo\n",
        "torchvision\n",
        "tqdm\n",
        "ultralytics\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "JrotexbA6n1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d19861-54b8-494e-da33-547d4f1a0769"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftfy\n",
            "jaxtyping\n",
            "jupyter\n",
            "matplotlib\n",
            "pydantic\n",
            "regex\n",
            "torch\n",
            "torchinfo\n",
            "torchvision\n",
            "tqdm\n",
            "ultralytics\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "import itertools as it\n",
        "import math\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torchinfo import summary\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "EUJ0Q3416lFn"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "device"
      ],
      "metadata": {
        "id": "ol8I22EE3J0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dfa57ab6-6265-4c33-8130-76e015f02c91"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "z6eNRTGUj3Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "v1kcr5gHj2nL"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args:\n",
        "#  - predictionList: [Prediction]\n",
        "#  - numPred: int :: if numPred==-1 (default) consider all the predictions in predictionList\n",
        "def display_predictions(predictionList, numPred=-1):\n",
        "  limit = 0\n",
        "  for p in predictionList:\n",
        "    if numPred!=-1 and limit >= numPred:\n",
        "      return;\n",
        "    limit += 1\n",
        "\n",
        "    p_image = p.image\n",
        "\n",
        "    if(not isinstance(p_image, torch.Tensor)):\n",
        "      p_image = torchvision.transforms.PILToTensor()(p_image)\n",
        "\n",
        "    p_description = p.description\n",
        "    p_ground_truth_bbox = p.ground_truth_bbox\n",
        "    p_output_bbox = p.output_bbox\n",
        "\n",
        "    # TODO: concatenate\n",
        "    p_image = draw_bounding_boxes(p_image, p_ground_truth_bbox.unsqueeze(0), colors=\"green\", width=5)\n",
        "    p_image = draw_bounding_boxes(p_image, p_output_bbox.unsqueeze(0), colors=\"red\", width=5)\n",
        "\n",
        "    tensor_to_pil = transforms.ToPILImage()\n",
        "    image_pil = tensor_to_pil(p_image)\n",
        "    display(image_pil)\n",
        "    print(p_description)\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "kZ_LB8njj6ng"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and type declaration"
      ],
      "metadata": {
        "id": "LuZuAEl69bK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "mPp8Dv3F6WW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b87361-f59e-460b-f5c1-9d0c3d221670"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join(\"dataset\", \"refcocog\", \"\")\n",
        "data_instances = os.path.join(root, \"annotations\", \"instances.json\")\n",
        "data_refs = os.path.join(root, \"annotations\", \"refs(umd).p\")\n",
        "data_images = os.path.join(root, \"images\", \"\")"
      ],
      "metadata": {
        "id": "W0wkQ9l961Rh"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar(\"I\")\n",
        "P = TypeVar(\"P\")\n",
        "B = TypeVar(\"B\")\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "Img = UInt[torch.Tensor, \"C W H\"]\n",
        "BBox = UInt[torch.Tensor, \"4\"]\n",
        "Split = Literal[\"train\", \"test\", \"val\"]\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: float  # number of pixel of the described object\n",
        "    iscrowd: Literal[\n",
        "        1, 0\n",
        "    ]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[\n",
        "        float, float, float, float\n",
        "    ]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "cSdOQiLh64XW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prediction:\n",
        "  def __init__(self, image, description, ground_truth_bbox, output_bbox):\n",
        "    self.image = image\n",
        "    self.description = description\n",
        "    self.ground_truth_bbox = ground_truth_bbox\n",
        "    self.output_bbox = output_bbox"
      ],
      "metadata": {
        "id": "pnlXm6nRLJlv"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "\n",
        "    >>> fix_filename('COCO_..._[image_id]_0000000001.jpg')\n",
        "    'COCO_..._[image_id].jpg'\n",
        "\n",
        "    \"\"\"\n",
        "    return re.sub(\"_\\d+\\.jpg$\", \".jpg\", x)"
      ],
      "metadata": {
        "id": "lFitg33d7eCx"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, \"rb\") as f:\n",
        "    raw = pickle.load(f)\n",
        "\n",
        "refs: list[Ref] = [fix_ref(Ref(**ref)) for ref in raw]"
      ],
      "metadata": {
        "id": "x_gy8HFl7gJ5"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, \"r\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "instances: Instances = Instances(**raw)\n",
        "\n",
        "id2annotation: Mapping[int, Annotation] = {x.id: x for x in instances.annotations}"
      ],
      "metadata": {
        "id": "17_PXs1_7h0x"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(\n",
        "        self, index: int\n",
        "    ) -> tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return img, ps, xyxy"
      ],
      "metadata": {
        "id": "CHbNwJ72gGoW"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Coco4CLIPDataset(Dataset[tuple[list[PIL.Image], list[str]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[list[PIL.Image], list[str]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return [img.crop(xyxy.tolist())], ps"
      ],
      "metadata": {
        "id": "EzFS_QvJdccS"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(batch: list[tuple[T, ...]]) -> tuple[list[T], ...]:\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "uLfJi4cQgxbB"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size: int = 3\n",
        "limit: int = 5 * batch_size"
      ],
      "metadata": {
        "id": "R66AVsCnhssx"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=CocoDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        ")"
      ],
      "metadata": {
        "id": "FQ49cCHFgFJB"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl4clip: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=Coco4CLIPDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        "    generator=torch.Generator(device=device), # add for GPU\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0upTJxxkgwle"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs: tuple[PIL.Image, ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "true_xyxy: tuple[Float[torch.Tensor, \"4\"], ...]\n",
        "\n",
        "for imgs, promptss, true_xyxy in dl:\n",
        "    print(imgs)\n",
        "    print(promptss)\n",
        "    print(true_xyxy)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "hvDm3Tf6hpTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a64cc46-34be-47fb-83c3-cc70c7404755"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x376 at 0x791FF09ABA60>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x431 at 0x791FF09A9750>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x791FF09A8460>)\n",
            "(['the man in yellow coat', 'skiier in red pants'], ['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a apple desktop computer', 'the white imac computer that is also turned on'])\n",
            "(tensor([374.31000,  65.06000, 510.34998, 267.00000], device='cuda:0'), tensor([ 93.95000,  83.29000, 598.56000, 373.86002], device='cuda:0'), tensor([338.79999,  82.19000, 486.13998, 239.56000], device='cuda:0'))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x791FF09A8E20>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x275 at 0x791FF09A9570>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x791FF09AB0A0>)\n",
            "(['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'], ['woman in coveralls', 'a person wearing overalls'], ['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'])\n",
            "(tensor([ 45.20000, 166.75999, 192.64999, 346.48999], device='cuda:0'), tensor([496.23999,  82.81000, 579.03998, 251.52000], device='cuda:0'), tensor([375.98001, 196.78000, 437.69000, 375.00000], device='cuda:0'))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x791FF09AA410>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x791FF09ABE50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x791FF09AB0D0>)\n",
            "(['the adult giraffe', 'a mother giraffe lickicking her baby'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'], ['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'])\n",
            "(tensor([ 39.28000, 157.14999, 294.33002, 353.39999], device='cuda:0'), tensor([182.85001, 191.92999, 282.88000, 347.62000], device='cuda:0'), tensor([ 40.36000, 209.00999, 229.19000, 638.56000], device='cuda:0'))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x79211F8F29E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x791FF09AB640>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x791FF09AB9A0>)\n",
            "(['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'], ['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'])\n",
            "(tensor([305.64999, 213.03999, 639.28003, 411.07001], device='cuda:0'), tensor([392.41000, 187.78999, 577.08002, 400.92999], device='cuda:0'), tensor([ 56.13000, 169.39999, 638.50000, 426.00000], device='cuda:0'))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x791FF09A95D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x791FF09AA560>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x791FF09AB130>)\n",
            "(['lower right of couch and black arm of chair', 'a gray couch'], ['a parked white ford suv', 'a light colored ford suv parked along the street'], ['a brown horse wearing a mask getting rode by a jockey'])\n",
            "(tensor([349.25000, 251.88000, 500.00000, 368.56000], device='cuda:0'), tensor([325.54001, 311.23999, 480.00000, 470.32001], device='cuda:0'), tensor([227.73000,  80.81000, 598.71997, 422.41000], device='cuda:0'))\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cropss: tuple[list[PIL.Image], ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "\n",
        "for cropss, promptss in dl4clip:\n",
        "    print(cropss)\n",
        "    print(promptss)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "REpDQACmiWHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bccb83-3842-4f2b-d4e2-899785e9c857"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([<PIL.Image.Image image mode=RGB size=189x430 at 0x791FF09A9DE0>], [<PIL.Image.Image image mode=RGB size=100x156 at 0x791FF09A9E10>], [<PIL.Image.Image image mode=RGB size=148x179 at 0x791FF09A85E0>])\n",
            "(['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'], ['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=582x257 at 0x791FF09A9E40>], [<PIL.Image.Image image mode=RGB size=83x169 at 0x791FF09AADD0>], [<PIL.Image.Image image mode=RGB size=333x198 at 0x791FF09AAF80>])\n",
            "(['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'], ['woman in coveralls', 'a person wearing overalls'], ['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=151x117 at 0x791FF09AA2F0>], [<PIL.Image.Image image mode=RGB size=147x158 at 0x791FF09A8340>], [<PIL.Image.Image image mode=RGB size=255x196 at 0x791FF09ABA00>])\n",
            "(['lower right of couch and black arm of chair', 'a gray couch'], ['a apple desktop computer', 'the white imac computer that is also turned on'], ['the adult giraffe', 'a mother giraffe lickicking her baby'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=185x213 at 0x791FF09AB910>], [<PIL.Image.Image image mode=RGB size=505x291 at 0x791FF09A84F0>], [<PIL.Image.Image image mode=RGB size=371x341 at 0x791FF09AB580>])\n",
            "(['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a brown horse wearing a mask getting rode by a jockey'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=62x178 at 0x791FF09AAF20>], [<PIL.Image.Image image mode=RGB size=136x202 at 0x791FF09AADD0>], [<PIL.Image.Image image mode=RGB size=154x159 at 0x791FF09AA770>])\n",
            "(['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'], ['the man in yellow coat', 'skiier in red pants'], ['a parked white ford suv', 'a light colored ford suv parked along the street'])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov5"
      ],
      "metadata": {
        "id": "Seut_6hFpOa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Yolo_v5(torch.nn.Module):\n",
        "  def __init__(self, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    # load yolo model\n",
        "    self.yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "    self.yolo_model.to(device=device).eval()\n",
        "\n",
        "  def forward(self, img):\n",
        "\n",
        "    # yolo bboxes\n",
        "    predictions = self.yolo_model(img)\n",
        "\n",
        "    # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
        "    # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
        "    bboxes: list[Float[torch.Tensor, 'X 6']] = predictions.xyxy # bboxes[i] contains the bboxes highlighted by yolo in image i\n",
        "\n",
        "    for image_idx, bbox_img in enumerate(bboxes):\n",
        "      # if empty, put a bbox equal to image size\n",
        "      if len(bbox_img) == 0:\n",
        "          bboxes[image_idx] = torch.tensor([[0, 0, img[image_idx].size[0], img[image_idx].size[1], 0, 0]], dtype=torch.float)\n",
        "\n",
        "    return bboxes"
      ],
      "metadata": {
        "id": "W9R_yJWhpM2g"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the region proposal algorithm\n",
        "yolo = Yolo_v5().to(device)"
      ],
      "metadata": {
        "id": "WxMItndhpTRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba21aca-760c-4e91-e10b-587f3b912013"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-8-23 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need\n",
        "In the following of this notebook we try to fine tune CLIP using a self-attention based approach. In this context, we try to refine the latent representations of both visual and textual prompts by means of single head attention mechanism."
      ],
      "metadata": {
        "id": "-GQW7HOMVVhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class attention_CLIP(nn.Module):\n",
        "  def __init__(self, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    # load clip model and preprocessing code\n",
        "    model, preprocess = clip.load('RN50')\n",
        "    model.float() # add for GPU\n",
        "\n",
        "    # freeze all pretrained layers by setting requires_grad=False\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.clip_visual_encoder = model.encode_image\n",
        "    self.clip_text_encoder = model.encode_text\n",
        "    self.clip_visual_preprocess = preprocess\n",
        "    self.clip_text_preprocess = clip.tokenize\n",
        "\n",
        "    # attention operator\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=1024, num_heads=1)\n",
        "\n",
        "  # preprocess input prompts as required by the visual encoder\n",
        "  def visual_preprocess(self, _imgs):\n",
        "    prep_images = torch.stack([\n",
        "        self.clip_visual_preprocess(i)\n",
        "        for i in _imgs\n",
        "    ]).to(device)\n",
        "\n",
        "    return prep_images\n",
        "\n",
        "  # preprocess text prompts as required by the text encoder\n",
        "  def text_preprocess(self, _txts):\n",
        "    prep_texts = self.clip_text_preprocess(_txts)\n",
        "\n",
        "    return prep_texts\n",
        "\n",
        "  # visual encoder\n",
        "  def visual_encoder(self, image):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_visual_encoder(image)\n",
        "    return clipFeatures\n",
        "\n",
        "  # text encoder\n",
        "  def text_encoder(self, text):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_text_encoder(text)\n",
        "    return clipFeatures\n",
        "\n",
        "  def forward(self, image, text):\n",
        "    # image and text preprocessing\n",
        "    with torch.no_grad():\n",
        "      image_pre = self.visual_preprocess(image)\n",
        "      text_pre = self.text_preprocess(text)\n",
        "\n",
        "    # get image and text feature representation\n",
        "    image_features = self.visual_encoder(image_pre)\n",
        "    text_features = self.text_encoder(text_pre)\n",
        "\n",
        "    # store number of images and number of texts for later retrival\n",
        "    num_images = len(image)\n",
        "    num_texts = len(text)\n",
        "\n",
        "    # concatenate image embeedings and prompt embeedings in the same latent context\n",
        "    context_features = torch.cat((image_features, text_features), dim=0)\n",
        "\n",
        "    # refine the latent representation of each text and image according to the overall context by means of the attention mechanism\n",
        "    attn_output, _ = self.attention(context_features, context_features, context_features)\n",
        "\n",
        "    # retrive image_features and text_features by means of the previously stored indexes\n",
        "    image_features = attn_output[:num_images]\n",
        "    text_features = attn_output[-num_texts:]\n",
        "\n",
        "    return image_features, text_features"
      ],
      "metadata": {
        "id": "t0UTB6rLkU4Z"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the network and move it to the chosen device\n",
        "net = attention_CLIP().to(device)"
      ],
      "metadata": {
        "id": "w-f-_CpwsQip"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, _lr, _wd, _momentum):\n",
        "  optimizer = torch.optim.SGD(  params = model.parameters(),\n",
        "                                lr = _lr,\n",
        "                                weight_decay = _wd,\n",
        "                                momentum = _momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "szOrwaAmpD0e"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_function():\n",
        "  def iou_accuracy(bbox_prediction, bbox_groundtruth):\n",
        "\n",
        "    # compute intersection over union between ground truth bboxes and predicted bboxes\n",
        "    iou_accuracy_matrix = torchvision.ops.box_iou(bbox_prediction[:, :4], bbox_groundtruth)\n",
        "\n",
        "    # extract the diagonal elements\n",
        "    iou_accuracy_matrix_diagonal = torch.diag(iou_accuracy_matrix)\n",
        "\n",
        "    # compute the mean of the intersection over union\n",
        "    mean_iou = iou_accuracy_matrix_diagonal.mean()\n",
        "\n",
        "    # compute the iou accuracy\n",
        "    iou_accuracy_output = mean_iou.item()\n",
        "\n",
        "    return iou_accuracy_output\n",
        "  return iou_accuracy"
      ],
      "metadata": {
        "id": "yTDcXkAcpWe4"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input:\n",
        "#   -> retrived_bboxes : bounding boxes proposed by the region proposal model\n",
        "#   -> bbox_groundtruth : ground truth bounding box provided by the training sample\n",
        "# output:\n",
        "#   -> [3, 5] in this case for the first element in the batch the best bbox is the fourth, while for the second element in the batch the best bbox is the sixth. The best bbox is the one characterized by the largest IoU with the ground truth bbox\n",
        "def best_bbox_one_hot_encoding(retrived_bboxes, bbox_groundtruth):\n",
        "  batch_bbox_one_hot_encoding = []\n",
        "  for batch_item_retrived_bboxes, batch_item_bbox_groundtruth in zip(retrived_bboxes, bbox_groundtruth):\n",
        "    iou_matrix = torchvision.ops.box_iou(batch_item_retrived_bboxes[:,:4], batch_item_bbox_groundtruth.unsqueeze(0))\n",
        "    batch_bbox_one_hot_encoding.append(torch.argmax(iou_matrix, dim=0))\n",
        "\n",
        "  batch_bbox_one_hot_encoding = torch.cat(batch_bbox_one_hot_encoding, dim=0)\n",
        "\n",
        "  return batch_bbox_one_hot_encoding"
      ],
      "metadata": {
        "id": "eH8UO3kBpYwo"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z = images_z / images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z = texts_z / texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "1SytMCAo7c5d"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(  model: torch.nn.Module,\n",
        "                    region_proposal_model: torch.nn.Module,\n",
        "                    data_loader: torch.utils.data.DataLoader,\n",
        "                    loss_fn: torch.nn.Module,\n",
        "                    accuracy_fn,\n",
        "                    optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device = device):\n",
        "  train_loss, iou_train_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  region_proposal_model.to(device)\n",
        "\n",
        "  for batch_idx, (imgs, promptss, true_xyxy) in tqdm(enumerate(data_loader)):\n",
        "    # send data to target device\n",
        "    # todo: send data to target device\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"imgs\")\n",
        "    print(imgs)\n",
        "\n",
        "    print(\"promptss\")\n",
        "    print(promptss)\n",
        "\n",
        "    print(\"true_xyxy\")\n",
        "    print(true_xyxy)\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # i. region proposal\n",
        "      bboxes = region_proposal_model(imgs)\n",
        "\n",
        "      # ii. get best bounding box with respect to the ground truth\n",
        "      bbox_groundtruth = best_bbox_one_hot_encoding(bboxes, true_xyxy)\n",
        "\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops = []\n",
        "      for batch_image, batch_image_bboxes in zip(imgs, bboxes):\n",
        "        list_bboxes_image: list[Image] = [\n",
        "            batch_image.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in batch_image_bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        crops.append(list_bboxes_image)\n",
        "\n",
        "    ####print(\"bboxes\")\n",
        "    ####print(bboxes)\n",
        "\n",
        "    # forward pass\n",
        "    cropss_z = []\n",
        "    promptss_z = []\n",
        "    for c, p in zip(crops, promptss):\n",
        "      model_output = model(c, p)\n",
        "      model_output_image_features = model_output[0]\n",
        "      model_output_text_features = model_output[1]\n",
        "\n",
        "      cropss_z.append(model_output_image_features)\n",
        "      promptss_z.append(model_output_text_features)\n",
        "\n",
        "    ####print(\"cropss_z\")\n",
        "    ####print(cropss_z)\n",
        "\n",
        "    ####print(\"promptss_z\")\n",
        "    ####print(promptss_z)\n",
        "\n",
        "    # cosine similarity evaluation\n",
        "    #   cropss_z :: list of BATCH_SIZE tensors: [tensor([bbox_img_1, 1024]), tensor([bbox_img_2, 1024]), ..., tensor([bbox_img_BATCH_SIZE, 1024])]\n",
        "    #   promptss_z :: list of BATCH_SIZE tensors: [tensor([prompts_img_1, 1024]), tensor([prompts_img_2, 1024]), ..., tensor([prompts_img_BATCH_SIZE, 1024])]\n",
        "    bbox_index_pred = []  # for each batch sample this list contains the index of the predicted bbox at the end of the iteration\n",
        "    loss = 0.0\n",
        "    for c_z, p_z, y in zip(cropss_z, promptss_z, bbox_groundtruth):\n",
        "\n",
        "      # rows :: prompts ; columns: crops\n",
        "      cosine_similarity_matrix = cosine_similarity(c_z, p_z)\n",
        "\n",
        "      # for each crop we set the average cosine similarity with the prompts\n",
        "      crop_logits = torch.mean(cosine_similarity_matrix, dim=0)\n",
        "\n",
        "      # calculate loss\n",
        "      loss = loss + loss_fn(crop_logits, y) # TODO: in contrastive manca questo +, c'Ã¨ solo: loss = loss_fn(crop_logits, y)\n",
        "\n",
        "      # get index of the predicted bounding box in order to compute IoU accuracy\n",
        "      bbox_index_pred.append(crop_logits.argmax().item())\n",
        "\n",
        "    loss = loss / len(bbox_groundtruth)  # avg loss\n",
        "\n",
        "    train_loss += loss\n",
        "\n",
        "    # optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # get predicted bounding box for each example in the batch\n",
        "      bbox_pred = [batch_example_bboxes[idx] for batch_example_bboxes, idx in zip(bboxes, bbox_index_pred)]\n",
        "\n",
        "      ###prediction_obj = Prediction(imgs[0], promptss[0], true_xyxy[0], bbox_pred[0][:4])\n",
        "      ###display_predictions([prediction_obj])\n",
        "\n",
        "      # calculate intersection over union train accuracy\n",
        "      acc = accuracy_fn(torch.stack(bbox_pred, dim=0), torch.stack(list(true_xyxy), dim=0))\n",
        "      iou_train_acc += acc\n",
        "\n",
        "  # Adjust metrics and print out\n",
        "  train_loss /= len(data_loader)\n",
        "  iou_train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f} | IoU train accuracy: {iou_train_acc:.5f}\\n\")\n",
        "  return train_loss, iou_train_acc"
      ],
      "metadata": {
        "id": "9MCqw6tbwhFW"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### test loop"
      ],
      "metadata": {
        "id": "4w-xYsXm2619"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(  model: torch.nn.Module,\n",
        "                region_proposal_model: torch.nn.Module,\n",
        "                data_loader: torch.utils.data.DataLoader,\n",
        "                loss_fn: torch.nn.Module,\n",
        "                accuracy_fn,\n",
        "                device: torch.device = device):\n",
        "  test_loss, iou_test_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  region_proposal_model.to(device)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch_idx, (imgs, promptss, true_xyxy) in tqdm(enumerate(data_loader)):\n",
        "      # send data to target device\n",
        "      # todo: send data to target device\n",
        "\n",
        "      # i. region proposal\n",
        "      bboxes = region_proposal_model(imgs)\n",
        "\n",
        "      # ii. get best bounding box with respect to the ground truth\n",
        "      bbox_groundtruth = best_bbox_one_hot_encoding(bboxes, true_xyxy)\n",
        "\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops = []\n",
        "      for batch_image, batch_image_bboxes in zip(imgs, bboxes):\n",
        "        list_bboxes_image: list[Image] = [\n",
        "            batch_image.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in batch_image_bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        crops.append(list_bboxes_image)\n",
        "\n",
        "      # forward pass\n",
        "      cropss_z = []\n",
        "      promptss_z = []\n",
        "      for c, p in zip(crops, promptss):\n",
        "        model_output = model(c, p)\n",
        "        model_output_image_features = model_output[0]\n",
        "        model_output_text_features = model_output[1]\n",
        "\n",
        "        cropss_z.append(model_output_image_features)\n",
        "        promptss_z.append(model_output_text_features)\n",
        "\n",
        "      # cosine similarity evaluation\n",
        "      #   cropss_z :: list of BATCH_SIZE tensors: [tensor([bbox_img_1, 1024]), tensor([bbox_img_2, 1024]), ..., tensor([bbox_img_BATCH_SIZE, 1024])]\n",
        "      #   promptss_z :: list of BATCH_SIZE tensors: [tensor([prompts_img_1, 1024]), tensor([prompts_img_2, 1024]), ..., tensor([prompts_img_BATCH_SIZE, 1024])]\n",
        "      bbox_index_pred = []  # for each batch sample this list contains the index of the predicted bbox at the end of the iteration\n",
        "      loss = 0.0\n",
        "      for c_z, p_z, y in zip(cropss_z, promptss_z, bbox_groundtruth):\n",
        "        crop_logits = []  # for each crop we set the average cosine similarity with the prompts\n",
        "        for vector_c_z in c_z:\n",
        "          vector_c_z_cos_similarities = []\n",
        "          for vector_p_z in p_z:\n",
        "            cosine_similarity = torch.nn.CosineSimilarity()(vector_c_z.unsqueeze(0), vector_p_z.unsqueeze(0)).item()\n",
        "            vector_c_z_cos_similarities.append(cosine_similarity)\n",
        "\n",
        "          mean_cosine_similarity = sum(vector_c_z_cos_similarities) / len(vector_c_z_cos_similarities)\n",
        "\n",
        "          crop_logits.append(mean_cosine_similarity)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss + loss_fn(torch.tensor(crop_logits).to(device), y.to(device))\n",
        "\n",
        "        # get index of the predicted bounding box in order to compute IoU accuracy\n",
        "        bbox_index_pred.append(crop_logits.index(max(crop_logits)))\n",
        "\n",
        "      loss = loss / len(bbox_groundtruth)  # avg loss\n",
        "      test_loss += loss\n",
        "\n",
        "      # get predicted bounding box for each example in the batch\n",
        "      bbox_pred = [batch_example_bboxes[idx] for batch_example_bboxes, idx in zip(bboxes, bbox_index_pred)]\n",
        "\n",
        "      ###prediction_obj = Prediction(imgs[0], promptss[0], true_xyxy[0], bbox_pred[0][:4])\n",
        "      ###display_predictions([prediction_obj])\n",
        "\n",
        "      # calculate intersection over union train accuracy\n",
        "      acc = accuracy_fn(torch.stack(bbox_pred, dim=0), torch.stack(list(true_xyxy), dim=0))\n",
        "      iou_test_acc += acc\n",
        "\n",
        "    # Adjust metrics and print out\n",
        "    test_loss /= len(data_loader)\n",
        "    iou_test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | IoU test accuracy: {iou_test_acc:.5f}\\n\")\n",
        "    return test_loss, iou_test_acc"
      ],
      "metadata": {
        "id": "l0F9QdnqsauT"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### main training-evaluation loop"
      ],
      "metadata": {
        "id": "tkHJHz3s2-73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values_evaluation(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "zkawxiHupb1C"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a manual seed allow us to provide reprudicible results in this notebook\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# create a logger for the experiment\n",
        "writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "LIMIT = 5 * BATCH_SIZE\n",
        "\n",
        "# get dataset instance\n",
        "train_dataset = CocoDataset(split=\"train\", limit=LIMIT)\n",
        "test_dataset = CocoDataset(split=\"test\", limit=LIMIT)\n",
        "val_dataset = CocoDataset(split=\"val\", limit=LIMIT)\n",
        "print(f\"LEN_TRAIN_DATASET: {len(train_dataset)}, LEN_TEST_DATASET: {len(test_dataset)}, LEN_VALIDATION_DATASET: {len(val_dataset)}\")\n",
        "\n",
        "# get dataloaders\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE}.\") # todo: togliere NUM_WORKERS anche da contrastive learning code\n",
        "train_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    generator=torch.Generator(device=device), # add for GPU\n",
        "    collate_fn=unzip,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        ")\n",
        "val_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        ")\n",
        "print(f\"LEN_TRAIN_DATALOADER: {len(train_loader)}, LEN_TEST_DATALOADER: {len(val_loader)}, LEN_VALIDATION_DATALOADER: {len(test_loader)}\")\n",
        "\n",
        "# instantiate the optimizer\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.000001\n",
        "momentum = 0.9\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "# define the cost function\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# define the accuracy function\n",
        "accuracy_function = get_accuracy_function()\n",
        "\n",
        "print('Before training:')\n",
        "train_loss, train_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = train_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "test_loss, test_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = test_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "val_loss, val_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = val_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values_evaluation(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "log_values_evaluation(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "log_values_evaluation(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "print('\\tTest loss {:.5f}, Test accuracy {:.5f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# measure time\n",
        "train_time_start = timer()\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_accuracy = training_step(\n",
        "        model = net,\n",
        "        region_proposal_model = yolo,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = loss_function,\n",
        "        accuracy_fn = accuracy_function,\n",
        "        optimizer = optimizer\n",
        "    )\n",
        "\n",
        "    val_loss, val_accuracy = test_step(\n",
        "        model = net,\n",
        "        region_proposal_model = yolo,\n",
        "        data_loader = val_loader,\n",
        "        loss_fn = loss_function,\n",
        "        accuracy_fn = accuracy_function\n",
        "    )\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values_evaluation(writer, epoch, train_loss, train_accuracy, \"train\")\n",
        "    log_values_evaluation(writer, epoch, val_loss, val_accuracy, \"validation\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "train_time_end = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start,\n",
        "                                            end=train_time_end,\n",
        "                                            device=device)\n",
        "# compute final evaluation results\n",
        "print('After training:')\n",
        "train_loss, train_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = train_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "test_loss, test_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = test_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "val_loss, val_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = val_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values_evaluation(writer, EPOCHS, train_loss, train_accuracy, \"train\")\n",
        "log_values_evaluation(writer, EPOCHS, val_loss, val_accuracy, \"validation\")\n",
        "log_values_evaluation(writer, EPOCHS, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "print('\\tTest loss {:.5f}, Test accuracy {:.5f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# closes the logger\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "L614wDknpdSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4bb731-3185-4ce7-ddac-07fbdb1e15aa"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN_TRAIN_DATASET: 15, LEN_TEST_DATASET: 15, LEN_VALIDATION_DATASET: 15\n",
            "Creating DataLoader's with batch size 3.\n",
            "LEN_TRAIN_DATALOADER: 5, LEN_TEST_DATALOADER: 5, LEN_VALIDATION_DATALOADER: 5\n",
            "Before training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.77988 | IoU test accuracy: 0.29369\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.61133 | IoU test accuracy: 0.39406\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73436 | IoU test accuracy: 0.20977\n",
            "\n",
            "\tTraining loss 1.77988, Training accuracy 0.29369\n",
            "\tValidation loss 1.73436, Validation accuracy 0.20977\n",
            "\tTest loss 1.61133, Test accuracy 0.39406\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  4.90it/s]\u001b[A\n",
            "2it [00:00,  4.15it/s]\u001b[A\n",
            "3it [00:00,  4.53it/s]\u001b[A\n",
            "4it [00:00,  4.59it/s]\u001b[A\n",
            "5it [00:01,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.79551 | IoU train accuracy: 0.29281\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  3.10it/s]\u001b[A\n",
            "2it [00:00,  4.03it/s]\u001b[A\n",
            "3it [00:00,  4.48it/s]\u001b[A\n",
            "4it [00:00,  4.21it/s]\u001b[A\n",
            "5it [00:01,  3.85it/s]\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73436 | IoU test accuracy: 0.20977\n",
            "\n",
            "\tTraining loss 1.79551, Training accuracy 0.29281\n",
            "\tValidation loss 1.73436, Validation accuracy 0.20977\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  3.12it/s]\u001b[A\n",
            "2it [00:00,  2.96it/s]\u001b[A\n",
            "3it [00:01,  2.74it/s]\u001b[A\n",
            "4it [00:01,  2.90it/s]\u001b[A\n",
            "5it [00:01,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.79505 | IoU train accuracy: 0.29281\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  2.26it/s]\u001b[A\n",
            "2it [00:00,  3.38it/s]\u001b[A\n",
            "3it [00:00,  4.05it/s]\u001b[A\n",
            "4it [00:01,  4.30it/s]\u001b[A\n",
            "5it [00:01,  3.96it/s]\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73436 | IoU test accuracy: 0.20977\n",
            "\n",
            "\tTraining loss 1.79505, Training accuracy 0.29281\n",
            "\tValidation loss 1.73436, Validation accuracy 0.20977\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  5.35it/s]\u001b[A\n",
            "2it [00:00,  3.83it/s]\u001b[A\n",
            "3it [00:00,  4.09it/s]\u001b[A\n",
            "4it [00:00,  4.38it/s]\u001b[A\n",
            "5it [00:01,  4.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.73619 | IoU train accuracy: 0.24961\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  3.06it/s]\u001b[A\n",
            "2it [00:00,  3.91it/s]\u001b[A\n",
            "3it [00:00,  4.44it/s]\u001b[A\n",
            "4it [00:00,  4.73it/s]\u001b[A\n",
            "5it [00:01,  4.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73436 | IoU test accuracy: 0.20977\n",
            "\n",
            "\tTraining loss 1.73619, Training accuracy 0.24961\n",
            "\tValidation loss 1.73436, Validation accuracy 0.20977\n",
            "-----------------------------------------------------\n",
            "Train time on cuda: 7.907 seconds\n",
            "After training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73546 | IoU test accuracy: 0.31201\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.61133 | IoU test accuracy: 0.39406\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:01,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.73436 | IoU test accuracy: 0.20977\n",
            "\n",
            "\tTraining loss 1.73546, Training accuracy 0.31201\n",
            "\tValidation loss 1.73436, Validation accuracy 0.20977\n",
            "\tTest loss 1.61133, Test accuracy 0.39406\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}