{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "clip\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchinfo\n",
        "torchvision\n",
        "tqdm\n",
        "ultralytics\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "JrotexbA6n1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "import itertools as it\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torchinfo import summary\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "EUJ0Q3416lFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPp8Dv3F6WW8"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join(\"dataset\", \"refcocog\", \"\")\n",
        "data_instances = os.path.join(root, \"annotations\", \"instances.json\")\n",
        "data_refs = os.path.join(root, \"annotations\", \"refs(umd).p\")\n",
        "data_images = os.path.join(root, \"images\", \"\")"
      ],
      "metadata": {
        "id": "W0wkQ9l961Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar(\"I\")\n",
        "P = TypeVar(\"P\")\n",
        "B = TypeVar(\"B\")\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "Img = UInt[torch.Tensor, \"C W H\"]\n",
        "BBox = UInt[torch.Tensor, \"4\"]\n",
        "Split = Literal[\"train\", \"test\", \"val\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: float  # number of pixel of the described object\n",
        "    iscrowd: Literal[\n",
        "        1, 0\n",
        "    ]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[\n",
        "        float, float, float, float\n",
        "    ]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "cSdOQiLh64XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "\n",
        "    >>> fix_filename('COCO_..._[image_id]_0000000001.jpg')\n",
        "    'COCO_..._[image_id].jpg'\n",
        "\n",
        "    \"\"\"\n",
        "    return re.sub(\"_\\d+\\.jpg$\", \".jpg\", x)"
      ],
      "metadata": {
        "id": "lFitg33d7eCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, \"rb\") as f:\n",
        "    raw = pickle.load(f)\n",
        "\n",
        "refs: list[Ref] = [fix_ref(Ref(**ref)) for ref in raw]"
      ],
      "metadata": {
        "id": "x_gy8HFl7gJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, \"r\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "instances: Instances = Instances(**raw)\n",
        "\n",
        "id2annotation: Mapping[int, Annotation] = {x.id: x for x in instances.annotations}"
      ],
      "metadata": {
        "id": "17_PXs1_7h0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(\n",
        "        self, index: int\n",
        "    ) -> tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return img, ps, xyxy"
      ],
      "metadata": {
        "id": "CHbNwJ72gGoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Coco4CLIPDataset(Dataset[tuple[list[PIL.Image], list[str]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[list[PIL.Image], list[str]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return [img.crop(xyxy.tolist())], ps"
      ],
      "metadata": {
        "id": "EzFS_QvJdccS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(batch: list[tuple[T, ...]]) -> tuple[list[T], ...]:\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "uLfJi4cQgxbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size: int = 3\n",
        "limit: int = 5 * batch_size"
      ],
      "metadata": {
        "id": "R66AVsCnhssx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=CocoDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        ")"
      ],
      "metadata": {
        "id": "FQ49cCHFgFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl4clip: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=Coco4CLIPDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0upTJxxkgwle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs: tuple[PIL.Image, ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "true_xyxy: tuple[Float[torch.Tensor, \"4\"], ...]\n",
        "\n",
        "for imgs, promptss, true_xyxy in dl:\n",
        "    print(imgs)\n",
        "    print(promptss)\n",
        "    print(true_xyxy)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "hvDm3Tf6hpTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cropss: tuple[list[PIL.Image], ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "\n",
        "for cropss, promptss in dl4clip:\n",
        "    print(cropss)\n",
        "    print(promptss)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "REpDQACmiWHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
