{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gekoramy/uni.deep-learning/blob/attention/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchinfo\n",
        "torchvision\n",
        "tqdm\n",
        "ultralytics\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "JrotexbA6n1G",
        "outputId": "812a5b51-bfcc-47a9-a3df-4ab4f374f91b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftfy\n",
            "jaxtyping\n",
            "jupyter\n",
            "matplotlib\n",
            "pydantic\n",
            "regex\n",
            "torch\n",
            "torchinfo\n",
            "torchvision\n",
            "tqdm\n",
            "ultralytics\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m953.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m609.6/609.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "import itertools as it\n",
        "import math\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torchinfo import summary\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "EUJ0Q3416lFn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "device"
      ],
      "metadata": {
        "id": "ol8I22EE3J0D",
        "outputId": "b3706161-db61-4d89-8176-721abba69bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "z6eNRTGUj3Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "v1kcr5gHj2nL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args:\n",
        "#  - predictionList: [Prediction]\n",
        "#  - numPred: int :: if numPred==-1 (default) consider all the predictions in predictionList\n",
        "def display_predictions(predictionList, numPred=-1):\n",
        "  limit = 0\n",
        "  for p in predictionList:\n",
        "    if numPred!=-1 and limit >= numPred:\n",
        "      return;\n",
        "    limit += 1\n",
        "\n",
        "    p_image = p.image\n",
        "\n",
        "    if(not isinstance(p_image, torch.Tensor)):\n",
        "      p_image = torchvision.transforms.PILToTensor()(p_image)\n",
        "\n",
        "    p_description = p.description\n",
        "    p_ground_truth_bbox = p.ground_truth_bbox\n",
        "    p_output_bbox = p.output_bbox\n",
        "\n",
        "    # TODO: concatenate\n",
        "    p_image = draw_bounding_boxes(p_image, p_ground_truth_bbox.unsqueeze(0), colors=\"green\", width=5)\n",
        "    p_image = draw_bounding_boxes(p_image, p_output_bbox.unsqueeze(0), colors=\"red\", width=5)\n",
        "\n",
        "    tensor_to_pil = transforms.ToPILImage()\n",
        "    image_pil = tensor_to_pil(p_image)\n",
        "    display(image_pil)\n",
        "    print(p_description)\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "kZ_LB8njj6ng"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and type declaration"
      ],
      "metadata": {
        "id": "LuZuAEl69bK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mPp8Dv3F6WW8",
        "outputId": "0bb4b216-7e8b-48c7-b430-7c29930e58d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [01:57<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join(\"dataset\", \"refcocog\", \"\")\n",
        "data_instances = os.path.join(root, \"annotations\", \"instances.json\")\n",
        "data_refs = os.path.join(root, \"annotations\", \"refs(umd).p\")\n",
        "data_images = os.path.join(root, \"images\", \"\")"
      ],
      "metadata": {
        "id": "W0wkQ9l961Rh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar(\"I\")\n",
        "P = TypeVar(\"P\")\n",
        "B = TypeVar(\"B\")\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "Img = UInt[torch.Tensor, \"C W H\"]\n",
        "BBox = UInt[torch.Tensor, \"4\"]\n",
        "Split = Literal[\"train\", \"test\", \"val\"]\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: float  # number of pixel of the described object\n",
        "    iscrowd: Literal[\n",
        "        1, 0\n",
        "    ]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[\n",
        "        float, float, float, float\n",
        "    ]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "cSdOQiLh64XW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prediction:\n",
        "  def __init__(self, image, description, ground_truth_bbox, output_bbox):\n",
        "    self.image = image\n",
        "    self.description = description\n",
        "    self.ground_truth_bbox = ground_truth_bbox\n",
        "    self.output_bbox = output_bbox"
      ],
      "metadata": {
        "id": "pnlXm6nRLJlv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "\n",
        "    >>> fix_filename('COCO_..._[image_id]_0000000001.jpg')\n",
        "    'COCO_..._[image_id].jpg'\n",
        "\n",
        "    \"\"\"\n",
        "    return re.sub(\"_\\d+\\.jpg$\", \".jpg\", x)"
      ],
      "metadata": {
        "id": "lFitg33d7eCx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, \"rb\") as f:\n",
        "    raw = pickle.load(f)\n",
        "\n",
        "refs: list[Ref] = [fix_ref(Ref(**ref)) for ref in raw]"
      ],
      "metadata": {
        "id": "x_gy8HFl7gJ5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, \"r\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "instances: Instances = Instances(**raw)\n",
        "\n",
        "id2annotation: Mapping[int, Annotation] = {x.id: x for x in instances.annotations}"
      ],
      "metadata": {
        "id": "17_PXs1_7h0x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(\n",
        "        self, index: int\n",
        "    ) -> tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return img, ps, xyxy"
      ],
      "metadata": {
        "id": "CHbNwJ72gGoW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Coco4CLIPDataset(Dataset[tuple[list[PIL.Image], list[str]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[list[PIL.Image], list[str]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return [img.crop(xyxy.tolist())], ps"
      ],
      "metadata": {
        "id": "EzFS_QvJdccS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(batch: list[tuple[T, ...]]) -> tuple[list[T], ...]:\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "uLfJi4cQgxbB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size: int = 3\n",
        "limit: int = 5 * batch_size"
      ],
      "metadata": {
        "id": "R66AVsCnhssx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=CocoDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        ")"
      ],
      "metadata": {
        "id": "FQ49cCHFgFJB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl4clip: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=Coco4CLIPDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0upTJxxkgwle"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs: tuple[PIL.Image, ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "true_xyxy: tuple[Float[torch.Tensor, \"4\"], ...]\n",
        "\n",
        "for imgs, promptss, true_xyxy in dl:\n",
        "    print(imgs)\n",
        "    print(promptss)\n",
        "    print(true_xyxy)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "hvDm3Tf6hpTb",
        "outputId": "45d93d90-7b7b-40c0-87e6-d2d5b26a50b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x376 at 0x7F1EF76EF910>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x431 at 0x7F1EF74F8A30>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7F1EF74F8A90>)\n",
            "(['the man in yellow coat', 'skiier in red pants'], ['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a apple desktop computer', 'the white imac computer that is also turned on'])\n",
            "(tensor([374.3100,  65.0600, 510.3500, 267.0000]), tensor([ 93.9500,  83.2900, 598.5600, 373.8600]), tensor([338.8000,  82.1900, 486.1400, 239.5600]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F1EF76EF940>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x275 at 0x7F1EF74F8FD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7F1EF74F9060>)\n",
            "(['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'], ['woman in coveralls', 'a person wearing overalls'], ['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'])\n",
            "(tensor([ 45.2000, 166.7600, 192.6500, 346.4900]), tensor([496.2400,  82.8100, 579.0400, 251.5200]), tensor([375.9800, 196.7800, 437.6900, 375.0000]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7F1EF74F8DF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7F1EF74F91B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x7F1EF74F9240>)\n",
            "(['the adult giraffe', 'a mother giraffe lickicking her baby'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'], ['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'])\n",
            "(tensor([ 39.2800, 157.1500, 294.3300, 353.4000]), tensor([182.8500, 191.9300, 282.8800, 347.6200]), tensor([ 40.3600, 209.0100, 229.1900, 638.5600]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F1EF74F8D60>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F1EF74F9300>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7F1EF74F8F70>)\n",
            "(['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'], ['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'])\n",
            "(tensor([305.6500, 213.0400, 639.2800, 411.0700]), tensor([392.4100, 187.7900, 577.0800, 400.9300]), tensor([ 56.1300, 169.4000, 638.5000, 426.0000]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7F1EF74F90F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x7F1EF74F8BB0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7F1EF74F9240>)\n",
            "(['lower right of couch and black arm of chair', 'a gray couch'], ['a parked white ford suv', 'a light colored ford suv parked along the street'], ['a brown horse wearing a mask getting rode by a jockey'])\n",
            "(tensor([349.2500, 251.8800, 500.0000, 368.5600]), tensor([325.5400, 311.2400, 480.0000, 470.3200]), tensor([227.7300,  80.8100, 598.7200, 422.4100]))\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cropss: tuple[list[PIL.Image], ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "\n",
        "for cropss, promptss in dl4clip:\n",
        "    print(cropss)\n",
        "    print(promptss)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "REpDQACmiWHC",
        "outputId": "3184fd7f-bf43-45db-a25e-6a2125e23fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([<PIL.Image.Image image mode=RGB size=371x341 at 0x7F1EF76EF790>], [<PIL.Image.Image image mode=RGB size=189x430 at 0x7F1EF76EE6B0>], [<PIL.Image.Image image mode=RGB size=62x178 at 0x7F1F0334AA70>])\n",
            "(['a brown horse wearing a mask getting rode by a jockey'], ['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'], ['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=255x196 at 0x7F1EF76EFF40>], [<PIL.Image.Image image mode=RGB size=185x213 at 0x7F1EF76EFC40>], [<PIL.Image.Image image mode=RGB size=136x202 at 0x7F1EF76EE860>])\n",
            "(['the adult giraffe', 'a mother giraffe lickicking her baby'], ['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['the man in yellow coat', 'skiier in red pants'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=147x158 at 0x7F1EF76EED10>], [<PIL.Image.Image image mode=RGB size=333x198 at 0x7F1EF76ED150>], [<PIL.Image.Image image mode=RGB size=582x257 at 0x7F1EF76EDF60>])\n",
            "(['a apple desktop computer', 'the white imac computer that is also turned on'], ['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'], ['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=151x117 at 0x7F1EF76EEFE0>], [<PIL.Image.Image image mode=RGB size=505x291 at 0x7F1EF76EE200>], [<PIL.Image.Image image mode=RGB size=148x179 at 0x7F1EF76EE860>])\n",
            "(['lower right of couch and black arm of chair', 'a gray couch'], ['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=83x169 at 0x7F1EF76EFC40>], [<PIL.Image.Image image mode=RGB size=154x159 at 0x7F1EF76EF790>], [<PIL.Image.Image image mode=RGB size=100x156 at 0x7F1EF76EFF40>])\n",
            "(['woman in coveralls', 'a person wearing overalls'], ['a parked white ford suv', 'a light colored ford suv parked along the street'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov5"
      ],
      "metadata": {
        "id": "Seut_6hFpOa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Yolo_v5(torch.nn.Module):\n",
        "  def __init__(self, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    # load yolo model\n",
        "    self.yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "    self.yolo_model.to(device=device).eval()\n",
        "\n",
        "  def forward(self, img):\n",
        "\n",
        "    # yolo bboxes\n",
        "    predictions = self.yolo_model(img)\n",
        "\n",
        "    # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
        "    # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
        "    bboxes: list[Float[torch.Tensor, 'X 6']] = predictions.xyxy # bboxes[i] contains the bboxes highlighted by yolo in image i\n",
        "\n",
        "    for image_idx, bbox_img in enumerate(bboxes):\n",
        "      # if empty, put a bbox equal to image size\n",
        "      if len(bbox_img) == 0:\n",
        "          bboxes[image_idx] = torch.tensor([[0, 0, img[image_idx].size[0], img[image_idx].size[1], 0, 0]], dtype=torch.float)\n",
        "\n",
        "    return bboxes"
      ],
      "metadata": {
        "id": "W9R_yJWhpM2g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the region proposal algorithm\n",
        "yolo = Yolo_v5().to(device)"
      ],
      "metadata": {
        "id": "WxMItndhpTRm",
        "outputId": "975e69e7-cf4c-4974-b4f4-bc02ab629825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 4.3 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.9 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.1s, installed 1 package: ['gitpython>=3.1.30']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 2023-8-21 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 105MB/s] \n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need\n",
        "In the following of this notebook we try to fine tune CLIP using a self-attention based approach. In this context, we try to refine the latent representations of both visual and textual prompts by means of single head attention mechanism."
      ],
      "metadata": {
        "id": "-GQW7HOMVVhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class attention_CLIP(nn.Module):\n",
        "  def __init__(self, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    # load clip model and preprocessing code\n",
        "    model, preprocess = clip.load('RN50')\n",
        "\n",
        "    # freeze all pretrained layers by setting requires_grad=False\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.clip_visual_encoder = model.encode_image\n",
        "    self.clip_text_encoder = model.encode_text\n",
        "    self.clip_visual_preprocess = preprocess\n",
        "    self.clip_text_preprocess = clip.tokenize\n",
        "\n",
        "    # attention operator\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=1024, num_heads=1)\n",
        "\n",
        "    # to be removed!!!\n",
        "    self.fc1 = nn.Linear(1024, 1024)\n",
        "\n",
        "  # preprocess input prompts as required by the visual encoder\n",
        "  def visual_preprocess(self, _imgs):\n",
        "    prep_images = torch.stack([\n",
        "        self.clip_visual_preprocess(i)\n",
        "        for i in _imgs\n",
        "    ]).to(device)\n",
        "\n",
        "    return prep_images\n",
        "\n",
        "  # preprocess text prompts as required by the text encoder\n",
        "  def text_preprocess(self, _txts):\n",
        "    prep_texts = self.clip_text_preprocess(_txts)\n",
        "\n",
        "    return prep_texts\n",
        "\n",
        "  # visual encoder\n",
        "  def visual_encoder(self, image):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_visual_encoder(image)\n",
        "    return clipFeatures\n",
        "\n",
        "  # text encoder\n",
        "  def text_encoder(self, text):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_text_encoder(text)\n",
        "    return clipFeatures\n",
        "\n",
        "  def forward(self, image, text):\n",
        "    # image and text preprocessing\n",
        "    with torch.no_grad():\n",
        "      image_pre = self.visual_preprocess(image)\n",
        "      text_pre = self.text_preprocess(text)\n",
        "\n",
        "    # get image and text feature representation\n",
        "    image_features = self.visual_encoder(image_pre)\n",
        "    text_features = self.text_encoder(text_pre)\n",
        "\n",
        "    print(\"image_features\")\n",
        "    print(image_features)\n",
        "    print(\"text_features\")\n",
        "    print(text_features)\n",
        "\n",
        "    # store number of images and number of texts for later retrival\n",
        "    ###num_images = len(image)\n",
        "    ###num_texts = len(text)\n",
        "\n",
        "    ####print(\"num_images\")\n",
        "    ####print(num_images)\n",
        "    ####print(\"num_texts\")\n",
        "    ####print(num_texts)\n",
        "\n",
        "    # concatenate image embeedings and prompt embeedings in the same latent context\n",
        "    ####context_features = torch.cat((image_features, text_features), dim=0)\n",
        "\n",
        "    # refine the latent representation of each text and image according to the overall context by means of the attention mechanism\n",
        "    ####attn_output, _ = self.attention(context_features, context_features, context_features)\n",
        "\n",
        "\n",
        "    # retrive image_features and text_features by means of the previously stored indexes\n",
        "    #####image_features = attn_output[:num_images]\n",
        "    #####text_features = attn_output[-num_texts:]\n",
        "\n",
        "    image_features = self.fc1(image_features)\n",
        "\n",
        "    return image_features, text_features"
      ],
      "metadata": {
        "id": "t0UTB6rLkU4Z"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the network and move it to the chosen device\n",
        "net = attention_CLIP().to(device)"
      ],
      "metadata": {
        "id": "w-f-_CpwsQip"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, _lr, _wd, _momentum):\n",
        "  optimizer = torch.optim.SGD(  params = model.parameters(),\n",
        "                                lr = _lr,\n",
        "                                weight_decay = _wd,\n",
        "                                momentum = _momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "szOrwaAmpD0e"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_function():\n",
        "  def iou_accuracy(bbox_prediction, bbox_groundtruth):\n",
        "\n",
        "    # compute intersection over union between ground truth bboxes and predicted bboxes\n",
        "    iou_accuracy_matrix = torchvision.ops.box_iou(bbox_prediction[:, :4], bbox_groundtruth)\n",
        "\n",
        "    # extract the diagonal elements\n",
        "    iou_accuracy_matrix_diagonal = torch.diag(iou_accuracy_matrix)\n",
        "\n",
        "    # compute the mean of the intersection over union\n",
        "    mean_iou = iou_accuracy_matrix_diagonal.mean()\n",
        "\n",
        "    # compute the iou accuracy\n",
        "    iou_accuracy_output = mean_iou.item()\n",
        "\n",
        "    return iou_accuracy_output\n",
        "  return iou_accuracy"
      ],
      "metadata": {
        "id": "yTDcXkAcpWe4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input:\n",
        "#   -> retrived_bboxes : bounding boxes proposed by the region proposal model\n",
        "#   -> bbox_groundtruth : ground truth bounding box provided by the training sample\n",
        "# output:\n",
        "#   -> [3, 5] in this case for the first element in the batch the best bbox is the fourth, while for the second element in the batch the best bbox is the sixth. The best bbox is the one characterized by the largest IoU with the ground truth bbox\n",
        "def best_bbox_one_hot_encoding(retrived_bboxes, bbox_groundtruth):\n",
        "  batch_bbox_one_hot_encoding = []\n",
        "  for batch_item_retrived_bboxes, batch_item_bbox_groundtruth in zip(retrived_bboxes, bbox_groundtruth):\n",
        "    iou_matrix = torchvision.ops.box_iou(batch_item_retrived_bboxes[:,:4], batch_item_bbox_groundtruth.unsqueeze(0))\n",
        "    batch_bbox_one_hot_encoding.append(torch.argmax(iou_matrix, dim=0))\n",
        "\n",
        "  batch_bbox_one_hot_encoding = torch.cat(batch_bbox_one_hot_encoding, dim=0)\n",
        "\n",
        "  return batch_bbox_one_hot_encoding"
      ],
      "metadata": {
        "id": "eH8UO3kBpYwo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity.cpu()"
      ],
      "metadata": {
        "id": "1SytMCAo7c5d"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(  model: torch.nn.Module,\n",
        "                    region_proposal_model: torch.nn.Module,\n",
        "                    data_loader: torch.utils.data.DataLoader,\n",
        "                    loss_fn: torch.nn.Module,\n",
        "                    accuracy_fn,\n",
        "                    optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device = device):\n",
        "  train_loss, iou_train_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  region_proposal_model.to(device)\n",
        "\n",
        "  for batch_idx, (imgs, promptss, true_xyxy) in tqdm(enumerate(data_loader)):\n",
        "    # send data to target device\n",
        "    # todo: send data to target device\n",
        "\n",
        "    print(\"imgs\")\n",
        "    print(imgs)\n",
        "\n",
        "    print(\"promptss\")\n",
        "    print(promptss)\n",
        "\n",
        "    print(\"true_xyxy\")\n",
        "    print(true_xyxy)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # i. region proposal\n",
        "      bboxes = region_proposal_model(imgs)\n",
        "\n",
        "      # ii. get best bounding box with respect to the ground truth\n",
        "      bbox_groundtruth = best_bbox_one_hot_encoding(bboxes, true_xyxy)\n",
        "\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops = []\n",
        "      for batch_image, batch_image_bboxes in zip(imgs, bboxes):\n",
        "        list_bboxes_image: list[Image] = [\n",
        "            batch_image.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in batch_image_bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        crops.append(list_bboxes_image)\n",
        "\n",
        "    print(\"bboxes\")\n",
        "    print(bboxes)\n",
        "\n",
        "    # forward pass\n",
        "    cropss_z = []\n",
        "    promptss_z = []\n",
        "    for c, p in zip(crops, promptss):\n",
        "      model_output = model(c, p)\n",
        "      model_output_image_features = model_output[0]\n",
        "      model_output_text_features = model_output[1]\n",
        "\n",
        "      cropss_z.append(model_output_image_features)\n",
        "      promptss_z.append(model_output_text_features)\n",
        "\n",
        "    print(\"cropss_z\")\n",
        "    print(cropss_z)\n",
        "\n",
        "    print(\"promptss_z\")\n",
        "    print(promptss_z)\n",
        "\n",
        "    # cosine similarity evaluation\n",
        "    #   cropss_z :: list of BATCH_SIZE tensors: [tensor([bbox_img_1, 1024]), tensor([bbox_img_2, 1024]), ..., tensor([bbox_img_BATCH_SIZE, 1024])]\n",
        "    #   promptss_z :: list of BATCH_SIZE tensors: [tensor([prompts_img_1, 1024]), tensor([prompts_img_2, 1024]), ..., tensor([prompts_img_BATCH_SIZE, 1024])]\n",
        "    bbox_index_pred = []  # for each batch sample this list contains the index of the predicted bbox at the end of the iteration\n",
        "    for c_z, p_z, y in zip(cropss_z, promptss_z, bbox_groundtruth):\n",
        "\n",
        "      print(\"c_z\")\n",
        "      print(c_z)\n",
        "\n",
        "      print(\"p_z\")\n",
        "      print(p_z)\n",
        "\n",
        "      # rows :: prompts ; columns: crops\n",
        "      cosine_similarity_matrix = cosine_similarity(c_z, p_z)\n",
        "\n",
        "      print(\"cosine_similarity_matrix\")\n",
        "      print(cosine_similarity_matrix)\n",
        "\n",
        "      # for each crop we set the average cosine similarity with the prompts\n",
        "      crop_logits = torch.mean(cosine_similarity_matrix, dim=0)\n",
        "\n",
        "      print(\"crop_logits\")\n",
        "      print(crop_logits)\n",
        "\n",
        "      # calculate loss\n",
        "      #####loss = loss_fn(crop_logits.to(device), y.to(device))\n",
        "      loss = loss + loss_fn(crop_logits, y) ### ARRIVATO QUI\n",
        "\n",
        "      print(\"loss PLEASE GRADIENT\")\n",
        "      print(loss)\n",
        "\n",
        "      # get index of the predicted bounding box in order to compute IoU accuracy\n",
        "      bbox_index_pred.append(crop_logits.argmax().item())\n",
        "\n",
        "    ###loss = loss / len(bbox_groundtruth)  # avg loss\n",
        "\n",
        "    print(\"big loss PLEASE GRADIENT\")\n",
        "    print(loss)\n",
        "\n",
        "    train_loss += loss\n",
        "\n",
        "    # optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # get predicted bounding box for each example in the batch\n",
        "      bbox_pred = [batch_example_bboxes[idx] for batch_example_bboxes, idx in zip(bboxes, bbox_index_pred)]\n",
        "\n",
        "      prediction_obj = Prediction(imgs[0], promptss[0], true_xyxy[0], bbox_pred[0][:4])\n",
        "      display_predictions([prediction_obj])\n",
        "\n",
        "      # calculate intersection over union train accuracy\n",
        "      acc = accuracy_fn(torch.stack(bbox_pred, dim=0), torch.stack(list(true_xyxy), dim=0))\n",
        "      iou_train_acc += acc\n",
        "\n",
        "    # Adjust metrics and print out\n",
        "    train_loss /= len(data_loader)\n",
        "    iou_train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | IoU train accuracy: {iou_train_acc:.5f}\\n\")\n",
        "    return train_loss, iou_train_acc"
      ],
      "metadata": {
        "id": "9MCqw6tbwhFW"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### test loop"
      ],
      "metadata": {
        "id": "4w-xYsXm2619"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(  model: torch.nn.Module,\n",
        "                region_proposal_model: torch.nn.Module,\n",
        "                data_loader: torch.utils.data.DataLoader,\n",
        "                loss_fn: torch.nn.Module,\n",
        "                accuracy_fn,\n",
        "                device: torch.device = device):\n",
        "  test_loss, iou_test_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  region_proposal_model.to(device)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch_idx, (imgs, promptss, true_xyxy) in tqdm(enumerate(data_loader)):\n",
        "      # send data to target device\n",
        "      # todo: send data to target device\n",
        "\n",
        "      # i. region proposal\n",
        "      bboxes = region_proposal_model(imgs)\n",
        "\n",
        "      # ii. get best bounding box with respect to the ground truth\n",
        "      bbox_groundtruth = best_bbox_one_hot_encoding(bboxes, true_xyxy)\n",
        "\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops = []\n",
        "      for batch_image, batch_image_bboxes in zip(imgs, bboxes):\n",
        "        list_bboxes_image: list[Image] = [\n",
        "            batch_image.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in batch_image_bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        crops.append(list_bboxes_image)\n",
        "\n",
        "      # forward pass\n",
        "      cropss_z = []\n",
        "      promptss_z = []\n",
        "      for c, p in zip(crops, promptss):\n",
        "        model_output = model(c, p)\n",
        "        model_output_image_features = model_output[0]\n",
        "        model_output_text_features = model_output[1]\n",
        "\n",
        "        cropss_z.append(model_output_image_features)\n",
        "        promptss_z.append(model_output_text_features)\n",
        "\n",
        "      # cosine similarity evaluation\n",
        "      #   cropss_z :: list of BATCH_SIZE tensors: [tensor([bbox_img_1, 1024]), tensor([bbox_img_2, 1024]), ..., tensor([bbox_img_BATCH_SIZE, 1024])]\n",
        "      #   promptss_z :: list of BATCH_SIZE tensors: [tensor([prompts_img_1, 1024]), tensor([prompts_img_2, 1024]), ..., tensor([prompts_img_BATCH_SIZE, 1024])]\n",
        "      bbox_index_pred = []  # for each batch sample this list contains the index of the predicted bbox at the end of the iteration\n",
        "      for c_z, p_z, y in zip(cropss_z, promptss_z, bbox_groundtruth):\n",
        "        crop_logits = []  # for each crop we set the average cosine similarity with the prompts\n",
        "        for vector_c_z in c_z:\n",
        "          vector_c_z_cos_similarities = []\n",
        "          for vector_p_z in p_z:\n",
        "            cosine_similarity = torch.nn.CosineSimilarity()(vector_c_z.unsqueeze(0), vector_p_z.unsqueeze(0)).item()\n",
        "            vector_c_z_cos_similarities.append(cosine_similarity)\n",
        "\n",
        "          mean_cosine_similarity = sum(vector_c_z_cos_similarities) / len(vector_c_z_cos_similarities)\n",
        "\n",
        "          crop_logits.append(mean_cosine_similarity)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_fn(torch.tensor(crop_logits).to(device), y.to(device))\n",
        "\n",
        "        # get index of the predicted bounding box in order to compute IoU accuracy\n",
        "        bbox_index_pred.append(crop_logits.index(max(crop_logits)))\n",
        "\n",
        "      loss = loss / len(bbox_groundtruth)  # avg loss\n",
        "      test_loss += loss\n",
        "\n",
        "      # get predicted bounding box for each example in the batch\n",
        "      bbox_pred = [batch_example_bboxes[idx] for batch_example_bboxes, idx in zip(bboxes, bbox_index_pred)]\n",
        "\n",
        "      prediction_obj = Prediction(imgs[0], promptss[0], true_xyxy[0], bbox_pred[0][:4])\n",
        "      display_predictions([prediction_obj])\n",
        "\n",
        "      # calculate intersection over union train accuracy\n",
        "      acc = accuracy_fn(torch.stack(bbox_pred, dim=0), torch.stack(list(true_xyxy), dim=0))\n",
        "      iou_test_acc += acc\n",
        "\n",
        "    # Adjust metrics and print out\n",
        "    test_loss /= len(data_loader)\n",
        "    iou_test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | IoU test accuracy: {iou_test_acc:.5f}\\n\")\n",
        "    return test_loss, iou_test_acc"
      ],
      "metadata": {
        "id": "l0F9QdnqsauT"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### main training-evaluation loop"
      ],
      "metadata": {
        "id": "tkHJHz3s2-73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values_evaluation(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "zkawxiHupb1C"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a manual seed allow us to provide reprudicible results in this notebook\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# create a logger for the experiment\n",
        "writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "LIMIT = 5 * BATCH_SIZE\n",
        "\n",
        "# get dataset instance\n",
        "train_dataset = CocoDataset(split=\"train\", limit=LIMIT)\n",
        "test_dataset = CocoDataset(split=\"test\", limit=LIMIT)\n",
        "val_dataset = CocoDataset(split=\"val\", limit=LIMIT)\n",
        "print(f\"LEN_TRAIN_DATASET: {len(train_dataset)}, LEN_TEST_DATASET: {len(test_dataset)}, LEN_VALIDATION_DATASET: {len(val_dataset)}\")\n",
        "\n",
        "# get dataloaders\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE}.\") # todo: togliere NUM_WORKERS anche da contrastive learning code\n",
        "train_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        ")\n",
        "test_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        ")\n",
        "val_loader: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        ")\n",
        "print(f\"LEN_TRAIN_DATALOADER: {len(train_loader)}, LEN_TEST_DATALOADER: {len(val_loader)}, LEN_VALIDATION_DATALOADER: {len(test_loader)}\")\n",
        "\n",
        "# instantiate the optimizer\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.000001\n",
        "momentum = 0.9\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "# define the cost function\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# define the accuracy function\n",
        "accuracy_function = get_accuracy_function()\n",
        "\n",
        "\"\"\"\n",
        "print('Before training:')\n",
        "train_loss, train_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = train_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "test_loss, test_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = test_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "val_loss, val_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = val_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values_evaluation(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "log_values_evaluation(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "log_values_evaluation(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "print('\\tTest loss {:.5f}, Test accuracy {:.5f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\"\"\"\n",
        "\n",
        "# measure time\n",
        "train_time_start = timer()\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_accuracy = training_step(\n",
        "        model = net,\n",
        "        region_proposal_model = yolo,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = loss_function,\n",
        "        accuracy_fn = accuracy_function,\n",
        "        optimizer = optimizer\n",
        "    )\n",
        "\n",
        "    val_loss, val_accuracy = test_step(\n",
        "        model = net,\n",
        "        region_proposal_model = yolo,\n",
        "        data_loader = val_loader,\n",
        "        loss_fn = loss_function,\n",
        "        accuracy_fn = accuracy_function\n",
        "    )\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values_evaluation(writer, epoch, train_loss, train_accuracy, \"train\")\n",
        "    log_values_evaluation(writer, epoch, val_loss, val_accuracy, \"validation\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "train_time_end = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start,\n",
        "                                            end=train_time_end,\n",
        "                                            device=device)\n",
        "# compute final evaluation results\n",
        "print('After training:')\n",
        "train_loss, train_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = train_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "test_loss, test_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = test_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "val_loss, val_accuracy = test_step( model = net,\n",
        "                        region_proposal_model = yolo,\n",
        "                        data_loader = val_loader,\n",
        "                        loss_fn = loss_function,\n",
        "                        accuracy_fn = accuracy_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values_evaluation(writer, EPOCHS, train_loss, train_accuracy, \"train\")\n",
        "log_values_evaluation(writer, EPOCHS, val_loss, val_accuracy, \"validation\")\n",
        "log_values_evaluation(writer, EPOCHS, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}, Training accuracy {:.5f}'.format(train_loss, train_accuracy))\n",
        "print('\\tValidation loss {:.5f}, Validation accuracy {:.5f}'.format(val_loss, val_accuracy))\n",
        "print('\\tTest loss {:.5f}, Test accuracy {:.5f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# closes the logger\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "L614wDknpdSe",
        "outputId": "aa4ecf82-6097-459b-9ac4-682b2340b372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN_TRAIN_DATASET: 15, LEN_TEST_DATASET: 15, LEN_VALIDATION_DATASET: 15\n",
            "Creating DataLoader's with batch size 3.\n",
            "LEN_TRAIN_DATALOADER: 5, LEN_TEST_DATALOADER: 5, LEN_VALIDATION_DATALOADER: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imgs\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F1EAC5F2080>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=612x612 at 0x7F1EAC5F1FC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7F1EAC5F0BB0>)\n",
            "promptss\n",
            "(['two woman one in black eatting and the other has a white shirt at the desk', 'woman in white shirt looking down at laptop computer'], ['a tv with a woman being interviewed on it', 'a woman with sunglasses on her head on the television being interviewed'], ['a young boy doing a skateboard trick on a blue board', 'a man jumping with a skateboard'])\n",
            "true_xyxy\n",
            "(tensor([  0.00000,  45.95000, 238.92000, 454.59003]), tensor([213.72000, 456.51001, 405.72998, 590.26001]), tensor([ 93.82000,  45.79000, 442.27002, 275.54001]))\n",
            "bboxes\n",
            "[tensor([[0.00000e+00, 4.21853e+01, 2.42700e+02, 4.63766e+02, 8.32214e-01, 0.00000e+00],\n",
            "        [1.87698e+02, 2.06740e+02, 4.95460e+02, 4.46894e+02, 7.16692e-01, 6.30000e+01],\n",
            "        [1.35497e+02, 3.92444e+01, 2.87448e+02, 1.56055e+02, 6.98335e-01, 0.00000e+00],\n",
            "        [3.78798e+02, 6.16455e-03, 6.38474e+02, 4.76598e+02, 6.67971e-01, 0.00000e+00],\n",
            "        [2.61253e+02, 4.23078e+02, 3.63732e+02, 4.79481e+02, 6.40877e-01, 3.90000e+01],\n",
            "        [4.61809e+02, 4.23382e+02, 4.85186e+02, 4.52319e+02, 5.31116e-01, 7.40000e+01],\n",
            "        [1.63502e+02, 1.19179e+02, 2.03839e+02, 1.48867e+02, 2.63107e-01, 4.80000e+01]]), tensor([[3.16598e+02, 2.58913e+01, 4.72824e+02, 2.29413e+02, 7.67241e-01, 0.00000e+00],\n",
            "        [1.43688e+02, 2.73175e+02, 1.92059e+02, 3.64573e+02, 7.64104e-01, 0.00000e+00],\n",
            "        [9.74342e+01, 3.54308e+02, 1.83907e+02, 4.19326e+02, 7.28791e-01, 0.00000e+00],\n",
            "        [2.17812e+01, 2.95363e+02, 9.04576e+01, 4.14122e+02, 6.87295e-01, 0.00000e+00],\n",
            "        [1.82381e+02, 3.41459e+02, 2.64446e+02, 4.24030e+02, 6.54268e-01, 0.00000e+00],\n",
            "        [4.59971e+02, 3.60442e+01, 5.97506e+02, 2.24882e+02, 6.06000e-01, 0.00000e+00],\n",
            "        [2.50175e+02, 3.75675e+02, 2.94119e+02, 4.26213e+02, 5.72907e-01, 0.00000e+00],\n",
            "        [2.82065e+02, 4.83369e+02, 3.99590e+02, 5.79941e+02, 5.55482e-01, 0.00000e+00],\n",
            "        [2.03380e+02, 3.02023e+02, 2.44928e+02, 3.75383e+02, 5.24525e-01, 0.00000e+00],\n",
            "        [5.47982e+02, 1.87416e+02, 5.94250e+02, 3.16334e+02, 4.58951e-01, 3.90000e+01],\n",
            "        [8.78914e+01, 2.89529e+02, 1.19312e+02, 3.47413e+02, 4.31158e-01, 0.00000e+00],\n",
            "        [4.55903e+01, 7.11099e+01, 1.58836e+02, 2.24050e+02, 3.99178e-01, 4.00000e+01],\n",
            "        [2.09089e+02, 3.68011e+02, 2.94567e+02, 4.26731e+02, 3.78162e-01, 0.00000e+00],\n",
            "        [3.43546e+02, 2.32338e+02, 5.14617e+02, 2.93432e+02, 3.69066e-01, 5.30000e+01],\n",
            "        [2.70620e+02, 4.81030e+02, 4.01566e+02, 5.81761e+02, 3.50972e-01, 6.20000e+01],\n",
            "        [1.10366e+02, 2.32376e+01, 2.24123e+02, 1.11735e+02, 3.46719e-01, 6.20000e+01],\n",
            "        [2.40325e+02, 1.03850e+02, 3.22765e+02, 2.52287e+02, 3.05877e-01, 3.90000e+01]]), tensor([[348.26996, 262.69727, 459.87433, 314.64752,   0.88253,  36.00000],\n",
            "        [400.39575,  90.83870, 521.83044, 204.41019,   0.82978,   0.00000],\n",
            "        [203.62079, 288.60620, 329.23407, 417.85150,   0.80985,   0.00000],\n",
            "        [106.87759,  53.72916, 439.87863, 285.81454,   0.80174,   0.00000]])]\n",
            "image_features\n",
            "tensor([[-0.04120,  0.02963, -0.01115,  ..., -0.04073,  0.05233,  0.07636],\n",
            "        [ 0.01621, -0.00170,  0.02963,  ..., -0.06148,  0.05906,  0.17773],\n",
            "        [-0.03865,  0.06518, -0.01884,  ..., -0.03097,  0.01092,  0.06817],\n",
            "        ...,\n",
            "        [ 0.01550,  0.04478, -0.03512,  ..., -0.02480,  0.00619, -0.04464],\n",
            "        [-0.02230,  0.03316, -0.01286,  ..., -0.08095,  0.02964, -0.13764],\n",
            "        [-0.02854,  0.06443, -0.01724,  ..., -0.08321,  0.02560, -0.03737]])\n",
            "text_features\n",
            "tensor([[ 0.13312,  0.27029,  0.39542,  ...,  0.15302, -0.01365, -0.28731],\n",
            "        [ 0.20616,  0.45671, -0.08638,  ..., -0.17303, -0.20943, -0.15172]])\n",
            "image_features\n",
            "tensor([[-0.02358,  0.04409,  0.00762,  ..., -0.03822, -0.02113, -0.00418],\n",
            "        [ 0.01716,  0.03934, -0.03482,  ..., -0.02192,  0.03060, -0.05566],\n",
            "        [ 0.01807,  0.05378,  0.00211,  ..., -0.05195,  0.01978, -0.10011],\n",
            "        ...,\n",
            "        [-0.02374, -0.03960, -0.02717,  ..., -0.00960, -0.00151, -0.06979],\n",
            "        [ 0.00615, -0.00899, -0.00627,  ..., -0.00681,  0.00309, -0.20021],\n",
            "        [ 0.01695,  0.03621, -0.03353,  ..., -0.01910, -0.01031, -0.10810]])\n",
            "text_features\n",
            "tensor([[-0.00087,  0.06477, -0.42073,  ..., -0.31026, -0.26949, -0.35376],\n",
            "        [ 0.28796,  0.19022, -0.37033,  ..., -0.61677, -0.07988, -0.10196]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:09, ?it/s]\n",
            "  0%|          | 0/3 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_features\n",
            "tensor([[-0.02052,  0.03498, -0.00313,  ..., -0.08768, -0.02622, -0.06750],\n",
            "        [-0.00253,  0.01081, -0.00388,  ..., -0.02971,  0.00254, -0.07382],\n",
            "        [ 0.02408,  0.03914,  0.00347,  ..., -0.02745, -0.01212, -0.02312],\n",
            "        [ 0.02563,  0.00623,  0.01962,  ..., -0.00854,  0.00791,  0.01737]])\n",
            "text_features\n",
            "tensor([[-0.14392, -0.02109, -0.13265,  ...,  0.23622,  0.12136, -0.08925],\n",
            "        [-0.01313,  0.15650, -0.14343,  ...,  0.25467,  0.44390,  0.08826]])\n",
            "cropss_z\n",
            "[tensor([[-0.05367, -0.06149,  0.01372,  ..., -0.00717,  0.01416, -0.01856],\n",
            "        [-0.02598, -0.04922,  0.02730,  ...,  0.02116,  0.02391,  0.00659],\n",
            "        [-0.06484, -0.03503,  0.02063,  ..., -0.03896, -0.01666,  0.00303],\n",
            "        ...,\n",
            "        [-0.02982, -0.06851,  0.03154,  ..., -0.05120,  0.00577, -0.00577],\n",
            "        [-0.03166, -0.09545, -0.02331,  ..., -0.04340, -0.01100, -0.00700],\n",
            "        [-0.01432, -0.07083, -0.00480,  ..., -0.03041, -0.01674,  0.00979]], grad_fn=<AddmmBackward0>), tensor([[-0.02970, -0.07154,  0.00180,  ..., -0.00996,  0.05623,  0.02214],\n",
            "        [-0.04397, -0.09162, -0.01295,  ..., -0.04576,  0.03480, -0.01010],\n",
            "        [-0.03892, -0.04309, -0.01020,  ..., -0.03675,  0.00469, -0.03555],\n",
            "        ...,\n",
            "        [-0.04755, -0.05993, -0.05505,  ..., -0.06511,  0.04596, -0.02888],\n",
            "        [-0.03384, -0.04158, -0.03054,  ..., -0.03862,  0.02794, -0.02500],\n",
            "        [-0.03541, -0.06397, -0.01639,  ..., -0.03999,  0.02869, -0.00255]], grad_fn=<AddmmBackward0>), tensor([[-0.04225, -0.07932, -0.01577,  ..., -0.07055, -0.00230, -0.00769],\n",
            "        [-0.02021, -0.04615,  0.00444,  ..., -0.01615,  0.03719,  0.00645],\n",
            "        [-0.00843, -0.05475,  0.01010,  ..., -0.01158,  0.02858,  0.02117],\n",
            "        [-0.00525, -0.07454, -0.00410,  ..., -0.02380,  0.03667,  0.02796]], grad_fn=<AddmmBackward0>)]\n",
            "promptss_z\n",
            "[tensor([[ 0.13312,  0.27029,  0.39542,  ...,  0.15302, -0.01365, -0.28731],\n",
            "        [ 0.20616,  0.45671, -0.08638,  ..., -0.17303, -0.20943, -0.15172]]), tensor([[-0.00087,  0.06477, -0.42073,  ..., -0.31026, -0.26949, -0.35376],\n",
            "        [ 0.28796,  0.19022, -0.37033,  ..., -0.61677, -0.07988, -0.10196]]), tensor([[-0.14392, -0.02109, -0.13265,  ...,  0.23622,  0.12136, -0.08925],\n",
            "        [-0.01313,  0.15650, -0.14343,  ...,  0.25467,  0.44390,  0.08826]])]\n",
            "c_z\n",
            "tensor([[-0.05367, -0.06149,  0.01372,  ..., -0.00717,  0.01416, -0.01856],\n",
            "        [-0.02598, -0.04922,  0.02730,  ...,  0.02116,  0.02391,  0.00659],\n",
            "        [-0.06484, -0.03503,  0.02063,  ..., -0.03896, -0.01666,  0.00303],\n",
            "        ...,\n",
            "        [-0.02982, -0.06851,  0.03154,  ..., -0.05120,  0.00577, -0.00577],\n",
            "        [-0.03166, -0.09545, -0.02331,  ..., -0.04340, -0.01100, -0.00700],\n",
            "        [-0.01432, -0.07083, -0.00480,  ..., -0.03041, -0.01674,  0.00979]], grad_fn=<AddmmBackward0>)\n",
            "p_z\n",
            "tensor([[ 0.13312,  0.27029,  0.39542,  ...,  0.15302, -0.01365, -0.28731],\n",
            "        [ 0.20616,  0.45671, -0.08638,  ..., -0.17303, -0.20943, -0.15172]])\n",
            "cosine_similarity_matrix\n",
            "tensor([[ 0.03707,  0.02734,  0.03583,  0.05401,  0.05478,  0.03354,  0.02737],\n",
            "        [ 0.02453,  0.00859,  0.01263,  0.02853,  0.01733,  0.01168, -0.00775]], grad_fn=<MmBackward0>)\n",
            "crop_logits\n",
            "tensor([0.03080, 0.01796, 0.02423, 0.04127, 0.03606, 0.02261, 0.00981], grad_fn=<MeanBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-727829ecb006>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     train_loss, train_accuracy = training_step(\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mregion_proposal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-fef76b833b69>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, region_proposal_model, data_loader, loss_fn, accuracy_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;31m#####loss = loss_fn(crop_logits.to(device), y.to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss PLEASE GRADIENT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "vzfISJ_tpdoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"ciao\", \"come\", \"va\"]\n",
        "\n",
        "model, preprocess = clip.load('RN50')\n",
        "\n",
        "with torch.no_grad():\n",
        "  texts_pre = clip.tokenize(texts)\n",
        "  texts_z = model.encode_text(texts_pre)\n",
        "\n",
        "print(texts_z)\n",
        "print(texts_z.shape)"
      ],
      "metadata": {
        "id": "wMFWE_B8VXzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_z[-2:]"
      ],
      "metadata": {
        "id": "SHy5H59Ul7fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = nn.MultiheadAttention(1024, 1)\n",
        "attn_output, attn_output_weights = multihead_attn(texts_z, texts_z, texts_z)\n"
      ],
      "metadata": {
        "id": "xldYBdywZMFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(multihead_attn)"
      ],
      "metadata": {
        "id": "J5WUqw_-dHHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output"
      ],
      "metadata": {
        "id": "LObOTIaogqPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output.shape"
      ],
      "metadata": {
        "id": "hsGEsLhDgkzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_weights"
      ],
      "metadata": {
        "id": "JIzyLNLzglYX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}