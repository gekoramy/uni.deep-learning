{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gekoramy/uni.deep-learning/blob/finetune-like-you-pretrain/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchinfo\n",
        "torchvision\n",
        "tqdm\n",
        "ultralytics\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "JrotexbA6n1G",
        "outputId": "9241051b-9e35-484c-aa7a-7e3d0a2fc2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftfy\n",
            "jaxtyping\n",
            "jupyter\n",
            "matplotlib\n",
            "pydantic\n",
            "regex\n",
            "torch\n",
            "torchinfo\n",
            "torchvision\n",
            "tqdm\n",
            "ultralytics\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "import itertools as it\n",
        "import math\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torchinfo import summary\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "EUJ0Q3416lFn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "device"
      ],
      "metadata": {
        "id": "ol8I22EE3J0D",
        "outputId": "b3f07991-c377-4918-96e5-785c01f24ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "qy1KOKNV9PEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "pv9ZjGq89SmG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and type declaration"
      ],
      "metadata": {
        "id": "LuZuAEl69bK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mPp8Dv3F6WW8",
        "outputId": "5dc830af-205c-49c6-f50b-42ecb78ea8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join(\"dataset\", \"refcocog\", \"\")\n",
        "data_instances = os.path.join(root, \"annotations\", \"instances.json\")\n",
        "data_refs = os.path.join(root, \"annotations\", \"refs(umd).p\")\n",
        "data_images = os.path.join(root, \"images\", \"\")"
      ],
      "metadata": {
        "id": "W0wkQ9l961Rh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar(\"I\")\n",
        "P = TypeVar(\"P\")\n",
        "B = TypeVar(\"B\")\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "Img = UInt[torch.Tensor, \"C W H\"]\n",
        "BBox = UInt[torch.Tensor, \"4\"]\n",
        "Split = Literal[\"train\", \"test\", \"val\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: float  # number of pixel of the described object\n",
        "    iscrowd: Literal[\n",
        "        1, 0\n",
        "    ]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[\n",
        "        float, float, float, float\n",
        "    ]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "cSdOQiLh64XW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "\n",
        "    >>> fix_filename('COCO_..._[image_id]_0000000001.jpg')\n",
        "    'COCO_..._[image_id].jpg'\n",
        "\n",
        "    \"\"\"\n",
        "    return re.sub(\"_\\d+\\.jpg$\", \".jpg\", x)"
      ],
      "metadata": {
        "id": "lFitg33d7eCx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, \"rb\") as f:\n",
        "    raw = pickle.load(f)\n",
        "\n",
        "refs: list[Ref] = [fix_ref(Ref(**ref)) for ref in raw]"
      ],
      "metadata": {
        "id": "x_gy8HFl7gJ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, \"r\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "instances: Instances = Instances(**raw)\n",
        "\n",
        "id2annotation: Mapping[int, Annotation] = {x.id: x for x in instances.annotations}"
      ],
      "metadata": {
        "id": "17_PXs1_7h0x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(\n",
        "        self, index: int\n",
        "    ) -> tuple[PIL.Image, list[str], Float[torch.Tensor, \"4\"]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return img, ps, xyxy"
      ],
      "metadata": {
        "id": "CHbNwJ72gGoW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Coco4CLIPDataset(Dataset[tuple[list[PIL.Image], list[str]]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        limit: int = -1,\n",
        "    ):\n",
        "        self.__init__\n",
        "        self.items: list[tuple[str, list[str], Float[torch.Tensor, \"4\"]]] = [\n",
        "            (i, [s.sent for s in ss], xywh)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ss in [ref.sentences]\n",
        "            for xywh in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "        self.len: int = len(self.items) if limit < 0 else min(limit, len(self.items))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[list[PIL.Image], list[str]]:\n",
        "        i, ps, xywh = self.items[index]\n",
        "        xyxy: Float[torch.Tensor, \"4\"] = torchvision.ops.box_convert(xywh, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        with PIL.Image.open(i) as img:\n",
        "            img.load()\n",
        "            return [img.crop(xyxy.tolist())], ps"
      ],
      "metadata": {
        "id": "EzFS_QvJdccS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(batch: list[tuple[T, ...]]) -> tuple[list[T], ...]:\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "uLfJi4cQgxbB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size: int = 3\n",
        "limit: int = 5 * batch_size"
      ],
      "metadata": {
        "id": "R66AVsCnhssx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl: DataLoader[tuple[list[PIL.Image], list[list[str]], list[Float[torch.Tensor, \"4\"]]]] = DataLoader(\n",
        "    dataset=CocoDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        ")"
      ],
      "metadata": {
        "id": "FQ49cCHFgFJB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl4clip: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=Coco4CLIPDataset(split=\"test\", limit=limit),\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0upTJxxkgwle"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs: tuple[PIL.Image, ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "true_xyxy: tuple[Float[torch.Tensor, \"4\"], ...]\n",
        "\n",
        "for imgs, promptss, true_xyxy in dl:\n",
        "    print(imgs)\n",
        "    print(promptss)\n",
        "    print(true_xyxy)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "hvDm3Tf6hpTb",
        "outputId": "2ce38d9f-4c40-4922-f82f-26417e43418d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x376 at 0x78130528A7D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x431 at 0x7813050F53C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7813050F5420>)\n",
            "(['the man in yellow coat', 'skiier in red pants'], ['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a apple desktop computer', 'the white imac computer that is also turned on'])\n",
            "(tensor([374.3100,  65.0600, 510.3500, 267.0000]), tensor([ 93.9500,  83.2900, 598.5600, 373.8600]), tensor([338.8000,  82.1900, 486.1400, 239.5600]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7813050F5720>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x275 at 0x7813050F5780>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7813050F5900>)\n",
            "(['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'], ['woman in coveralls', 'a person wearing overalls'], ['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'])\n",
            "(tensor([ 45.2000, 166.7600, 192.6500, 346.4900]), tensor([496.2400,  82.8100, 579.0400, 251.5200]), tensor([375.9800, 196.7800, 437.6900, 375.0000]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7813050F5B10>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7813050F5B70>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x7813050F59F0>)\n",
            "(['the adult giraffe', 'a mother giraffe lickicking her baby'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'], ['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'])\n",
            "(tensor([ 39.2800, 157.1500, 294.3300, 353.4000]), tensor([182.8500, 191.9300, 282.8800, 347.6200]), tensor([ 40.3600, 209.0100, 229.1900, 638.5600]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7813050F5930>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7813050F5570>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7813050F5810>)\n",
            "(['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'], ['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'])\n",
            "(tensor([305.6500, 213.0400, 639.2800, 411.0700]), tensor([392.4100, 187.7900, 577.0800, 400.9300]), tensor([ 56.1300, 169.4000, 638.5000, 426.0000]))\n",
            "--------------------------------------------------\n",
            "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7813050F5960>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x640 at 0x7813050F5A80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427 at 0x7813050F5870>)\n",
            "(['lower right of couch and black arm of chair', 'a gray couch'], ['a parked white ford suv', 'a light colored ford suv parked along the street'], ['a brown horse wearing a mask getting rode by a jockey'])\n",
            "(tensor([349.2500, 251.8800, 500.0000, 368.5600]), tensor([325.5400, 311.2400, 480.0000, 470.3200]), tensor([227.7300,  80.8100, 598.7200, 422.4100]))\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cropss: tuple[list[PIL.Image], ...]\n",
        "promptss: tuple[list[str], ...]\n",
        "\n",
        "for cropss, promptss in dl4clip:\n",
        "    print(cropss)\n",
        "    print(promptss)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "REpDQACmiWHC",
        "outputId": "f3635926-c931-4413-95f3-9fc705ad25d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([<PIL.Image.Image image mode=RGB size=185x213 at 0x78130528AAD0>], [<PIL.Image.Image image mode=RGB size=100x156 at 0x78130528B070>], [<PIL.Image.Image image mode=RGB size=582x257 at 0x781305289930>])\n",
            "(['a brown bear near a soda bottle', 'a without hairy brown color teddy bear'], ['a lady in blue t - shirt and white shorts sitting on a park bench', 'a couple of friends are sitting on a bench and hanging out'], ['a table with pizza , drinks , and seasonings on it', 'a table of food , with plates , a pizza , pitchers , and glasses'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=505x291 at 0x7813050F5D20>], [<PIL.Image.Image image mode=RGB size=62x178 at 0x7813050F5900>], [<PIL.Image.Image image mode=RGB size=83x169 at 0x7813050F59F0>])\n",
            "(['there is red colored truck in between the other trucks', 'a shiny red vintage pickup truck'], ['a man standing next to a young girl on a grassy hillside', 'a man in a black jacket'], ['woman in coveralls', 'a person wearing overalls'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=136x202 at 0x78130528A860>], [<PIL.Image.Image image mode=RGB size=148x179 at 0x78130528A290>], [<PIL.Image.Image image mode=RGB size=255x196 at 0x78130528B2B0>])\n",
            "(['the man in yellow coat', 'skiier in red pants'], ['a girl wearing glasses and a pink shirt', 'an asian girl with a pink shirt eating at the table'], ['the adult giraffe', 'a mother giraffe lickicking her baby'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=154x159 at 0x7813050F5BD0>], [<PIL.Image.Image image mode=RGB size=371x341 at 0x78130528A7D0>], [<PIL.Image.Image image mode=RGB size=189x430 at 0x7813050F5240>])\n",
            "(['a parked white ford suv', 'a light colored ford suv parked along the street'], ['a brown horse wearing a mask getting rode by a jockey'], ['a blonde woman in a white shirt and long black skirt', 'there is one small girl wearing white top is touching the elephant'])\n",
            "--------------------------------------------------\n",
            "([<PIL.Image.Image image mode=RGB size=333x198 at 0x7813050F5900>], [<PIL.Image.Image image mode=RGB size=151x117 at 0x7813050F5AB0>], [<PIL.Image.Image image mode=RGB size=147x158 at 0x7813050F59C0>])\n",
            "(['the truck covered in the snow furthest to the right', 'an old truck covered in snow except for the grill and door'], ['lower right of couch and black arm of chair', 'a gray couch'], ['a apple desktop computer', 'the white imac computer that is also turned on'])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune like you pretrain\n",
        "In the following we try to fine tune CLIP image and text encoders using contrastive learning as proposed by the original paper."
      ],
      "metadata": {
        "id": "f-lK6RuEx1Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FLYP_CLIP(nn.Module):\n",
        "  def __init__(self, device=device):  #TODO: aggiungere device=device anche nelle architetture dello standard fine tuning\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    model, preprocess = clip.load('RN50')\n",
        "\n",
        "    # freeze all pretrained layers by setting requires_grad=False\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.clip_visual_encoder = model.encode_image\n",
        "    self.clip_text_encoder = model.encode_text\n",
        "    self.clip_visual_preprocess = preprocess\n",
        "    self.clip_text_preprocess = clip.tokenize\n",
        "\n",
        "    self.visual_encoder_linearHead = nn.Linear(1024, 1024)\n",
        "    self.text_encoder_linearHead = nn.Linear(1024, 1024)\n",
        "\n",
        "    # the temperature parameter is added as suggested by the original paper in order to prevent training instability\n",
        "    self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "  # preprocess input prompts as required by the visual encoder\n",
        "  def visual_preprocess(self, _imgs):\n",
        "    prep_images = torch.stack([\n",
        "        self.clip_visual_preprocess(i)\n",
        "        for i in _imgs\n",
        "    ]).to(device)\n",
        "\n",
        "    return prep_images\n",
        "\n",
        "  # preprocess text prompts as required by the text encoder\n",
        "  def text_preprocess(self, _txts):\n",
        "    prep_texts = self.clip_text_preprocess(_txts)\n",
        "\n",
        "    return prep_texts\n",
        "\n",
        "  # visual encoder\n",
        "  def visual_encoder(self, image):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_visual_encoder(image)\n",
        "\n",
        "    x = F.relu(clipFeatures)\n",
        "    x = self.visual_encoder_linearHead(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  # text encoder\n",
        "  def text_encoder(self, text):\n",
        "    with torch.no_grad():\n",
        "      clipFeatures = self.clip_text_encoder(text)\n",
        "\n",
        "    x = F.relu(clipFeatures)\n",
        "    x = self.text_encoder_linearHead(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self, image, text):\n",
        "    with torch.no_grad():\n",
        "      image_pre = self.visual_preprocess(image)\n",
        "      text_pre = self.text_preprocess(text)\n",
        "\n",
        "    image_features = self.visual_encoder(image_pre)\n",
        "    text_features = self.text_encoder(text_pre)\n",
        "\n",
        "    return image_features, text_features, self.logit_scale.exp()"
      ],
      "metadata": {
        "id": "Bf79gJreyzix"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, _lr, _wd, _momentum):\n",
        "  optimizer = torch.optim.SGD(  params = model.parameters(),\n",
        "                                lr = _lr,\n",
        "                                weight_decay = _wd,\n",
        "                                momentum = _momentum)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "baLVQGDZ6fue"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def get_ground_truth(self, num_logits):\n",
        "        labels = torch.arange(num_logits)\n",
        "        return labels\n",
        "\n",
        "    def get_logits(self, image_features, text_features, logit_scale):\n",
        "        logits_per_image = logit_scale * image_features @ text_features.T\n",
        "        logits_per_text = logit_scale * text_features @ image_features.T\n",
        "\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "    def forward(self, image_features, text_features, logit_scale):\n",
        "\n",
        "        # compute logits per image and logits per text\n",
        "        logits_per_image, logits_per_text = self.get_logits(image_features, text_features, logit_scale)\n",
        "\n",
        "        # get ground truth labels for the computation of the cross entropy loss\n",
        "        labels = self.get_ground_truth(logits_per_image.shape[0])\n",
        "\n",
        "        total_loss = (\n",
        "            F.cross_entropy(logits_per_image, labels) +\n",
        "            F.cross_entropy(logits_per_text, labels)\n",
        "        ) / 2\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "Rxo4acqy6umv"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(\n",
        "    model: torch.nn.Module,                   # neural network to be trained\n",
        "    data_loader: torch.utils.data.DataLoader, # data loader to be iterated\n",
        "    loss_fn: torch.nn.Module,                 # loss function\n",
        "    optimizer: torch.optim.Optimizer,         # optimizer\n",
        "    device: torch.device = device             #target device\n",
        "):\n",
        "  train_loss = 0.0\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (cropss, promptss) in tqdm(enumerate(data_loader)):\n",
        "\n",
        "    # for this implementation we consider only one prompt for each crop\n",
        "    model_input_crops = [c[0] for c in cropss]\n",
        "    model_input_prompts = [p[0] for p in promptss]\n",
        "\n",
        "    # send data to target device\n",
        "    ####cropss = cropss.to(device)\n",
        "    ####promptss = promptss.to(device)\n",
        "\n",
        "    # forward computation\n",
        "    model_out = model(model_input_crops, model_input_prompts)\n",
        "    image_features = model_out[0]\n",
        "    text_features = model_out[1]\n",
        "    logit_scale = model_out[2]\n",
        "\n",
        "    # calculate loss\n",
        "    loss = loss_fn(image_features, text_features, logit_scale)\n",
        "    train_loss += loss\n",
        "\n",
        "    # optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Note: we clamp to 4.6052 = ln(100), as in the original paper.\n",
        "    with torch.no_grad():\n",
        "        model.logit_scale.clamp_(0, math.log(100))\n",
        "\n",
        "\n",
        "  # Calculate loss per epoch and print out what's happening\n",
        "  train_loss /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f}\\n\")\n",
        "  return train_loss"
      ],
      "metadata": {
        "id": "bfAw4-gF9qHX"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(\n",
        "    model: torch.nn.Module,                   # neural network to be evaluated\n",
        "    data_loader: torch.utils.data.DataLoader, # data loader to be iterated\n",
        "    loss_fn: torch.nn.Module,                 # loss function\n",
        "    device: torch.device = device             #target device\n",
        "):\n",
        "  test_loss = 0.0\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    #for batch_idx, cropss, promptss in tqdm(enumerate(data_loader)):\n",
        "    for batch_idx, (cropss, promptss) in tqdm(enumerate(data_loader)):\n",
        "\n",
        "      # for this implementation we consider only one prompt for each crop\n",
        "      model_input_crops = [c[0] for c in cropss]\n",
        "      model_input_prompts = [p[0] for p in promptss]\n",
        "\n",
        "      # send data to target device\n",
        "      ####cropss = cropss.to(device)\n",
        "      ####promptss = promptss.to(device)\n",
        "\n",
        "      # forward computation\n",
        "      model_out = model(model_input_crops, model_input_prompts)\n",
        "      image_features = model_out[0]\n",
        "      text_features = model_out[1]\n",
        "      logit_scale = model_out[2]\n",
        "\n",
        "      # calculate loss\n",
        "      loss = loss_fn(image_features, text_features, logit_scale)\n",
        "      test_loss += loss\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f}\\n\")\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "xXcCO0HvB5Kc"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the network and move it to the chosen device\n",
        "net = FLYP_CLIP().to(device)"
      ],
      "metadata": {
        "id": "SYR8nmNKC8OW"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)"
      ],
      "metadata": {
        "id": "eM6VGn0YJIdm"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a manual seed allow us to provide reprudicible results in this notebook\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# create a logger for the experiment\n",
        "writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "LIMIT = 5 * BATCH_SIZE\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "# get dataset instance\n",
        "train_dataset = Coco4CLIPDataset(split=\"train\", limit=LIMIT)\n",
        "test_dataset = Coco4CLIPDataset(split=\"test\", limit=LIMIT)\n",
        "val_dataset = Coco4CLIPDataset(split=\"val\", limit=LIMIT)\n",
        "print(f\"LEN_TRAIN_DATASET: {len(train_dataset)}, LEN_TEST_DATASET: {len(test_dataset)}, LEN_VALIDATION_DATASET: {len(val_dataset)}\")\n",
        "\n",
        "# get dataloaders\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "train_loader: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=True,\n",
        ")\n",
        "test_loader: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=False,\n",
        ")\n",
        "val_loader: DataLoader[tuple[list[PIL.Image], list[str]]] = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=unzip,\n",
        "    shuffle=False,\n",
        ")\n",
        "print(f\"LEN_TRAIN_DATALOADER: {len(train_loader)}, LEN_TEST_DATALOADER: {len(val_loader)}, LEN_VALIDATION_DATALOADER: {len(test_loader)}\")\n",
        "\n",
        "# instantiate the optimizer\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.000001\n",
        "momentum = 0.9\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "# define the cost function\n",
        "cost_function = ClipLoss().to(device)\n",
        "\n",
        "print('Before training:')\n",
        "train_loss = test_step(model = net,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = cost_function)\n",
        "val_loss = test_step(model = net,\n",
        "        data_loader = val_loader,\n",
        "        loss_fn = cost_function)\n",
        "test_loss = test_step(model = net,\n",
        "        data_loader = test_loader,\n",
        "        loss_fn = cost_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values(writer, -1, train_loss, \"train\")\n",
        "log_values(writer, -1, val_loss, \"validation\")\n",
        "log_values(writer, -1, test_loss, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}'.format(train_loss))\n",
        "print('\\tValidation loss {:.5f}'.format(val_loss))\n",
        "print('\\tTest loss {:.5f}'.format(test_loss))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# measure time\n",
        "train_time_start = timer()\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss = training_step(\n",
        "        model = net,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = cost_function,\n",
        "        optimizer = optimizer\n",
        "    )\n",
        "\n",
        "    val_loss = test_step(\n",
        "        model = net,\n",
        "        data_loader = val_loader,\n",
        "        loss_fn = cost_function\n",
        "    )\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, epoch, train_loss, \"train\")\n",
        "    log_values(writer, epoch, val_loss, \"validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(epoch+1))\n",
        "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
        "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "train_time_end = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start,\n",
        "                                            end=train_time_end,\n",
        "                                            device=device)\n",
        "# compute final evaluation results\n",
        "print('After training:')\n",
        "train_loss = test_step(model = net,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = cost_function)\n",
        "val_loss = test_step(model = net,\n",
        "        data_loader = val_loader,\n",
        "        loss_fn = cost_function)\n",
        "test_loss = test_step(model = net,\n",
        "        data_loader = test_loader,\n",
        "        loss_fn = cost_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values(writer, EPOCHS, train_loss, \"train\")\n",
        "log_values(writer, EPOCHS, val_loss, \"validation\")\n",
        "log_values(writer, EPOCHS, test_loss, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}'.format(train_loss))\n",
        "print('\\tValidation loss {:.5f}'.format(val_loss))\n",
        "print('\\tTest loss {:.5f}'.format(test_loss))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# closes the logger\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "olpCg4V6G2kZ",
        "outputId": "74983d91-18a2-4956-b4e9-5ac3661195b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN_TRAIN_DATASET: 15, LEN_TEST_DATASET: 15, LEN_VALIDATION_DATASET: 15\n",
            "Creating DataLoader's with batch size 3 and 1 workers.\n",
            "LEN_TRAIN_DATALOADER: 5, LEN_TEST_DATALOADER: 5, LEN_VALIDATION_DATALOADER: 5\n",
            "Before training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:05,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.43407\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:05,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.29528\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:06,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.27050\n",
            "\n",
            "\tTraining loss 1.43407\n",
            "\tValidation loss 1.29528\n",
            "\tTest loss 1.27050\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.05s/it]\u001b[A\n",
            "2it [00:02,  1.09s/it]\u001b[A\n",
            "3it [00:03,  1.31s/it]\u001b[A\n",
            "4it [00:04,  1.21s/it]\u001b[A\n",
            "5it [00:05,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.80077\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.02s/it]\u001b[A\n",
            "2it [00:02,  1.16s/it]\u001b[A\n",
            "3it [00:04,  1.42s/it]\u001b[A\n",
            "4it [00:05,  1.48s/it]\u001b[A\n",
            "5it [00:06,  1.40s/it]\n",
            " 33%|███▎      | 1/3 [00:12<00:25, 12.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.79844\n",
            "\n",
            "Epoch: 1\n",
            "\tTraining loss 1.80077\n",
            "\tValidation loss 1.79844\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.72s/it]\u001b[A\n",
            "2it [00:03,  1.59s/it]\u001b[A\n",
            "3it [00:04,  1.34s/it]\u001b[A\n",
            "4it [00:05,  1.23s/it]\u001b[A\n",
            "5it [00:06,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.31575\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.03s/it]\u001b[A\n",
            "2it [00:02,  1.03s/it]\u001b[A\n",
            "3it [00:03,  1.32s/it]\u001b[A\n",
            "4it [00:05,  1.45s/it]\u001b[A\n",
            "5it [00:06,  1.32s/it]\n",
            " 67%|██████▋   | 2/3 [00:25<00:12, 12.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 2.28227\n",
            "\n",
            "Epoch: 2\n",
            "\tTraining loss 2.31575\n",
            "\tValidation loss 2.28227\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.08s/it]\u001b[A\n",
            "2it [00:02,  1.07s/it]\u001b[A\n",
            "3it [00:03,  1.06s/it]\u001b[A\n",
            "4it [00:04,  1.06s/it]\u001b[A\n",
            "5it [00:05,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.21034\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:01,  1.04s/it]\u001b[A\n",
            "2it [00:02,  1.04s/it]\u001b[A\n",
            "3it [00:03,  1.04s/it]\u001b[A\n",
            "4it [00:04,  1.07s/it]\u001b[A\n",
            "5it [00:05,  1.19s/it]\n",
            "100%|██████████| 3/3 [00:37<00:00, 12.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 3.36217\n",
            "\n",
            "Epoch: 3\n",
            "\tTraining loss 3.21034\n",
            "\tValidation loss 3.36217\n",
            "-----------------------------------------------------\n",
            "Train time on cpu: 37.231 seconds\n",
            "After training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:05,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 4.83593\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:05,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 3.36217\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:06,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 2.81674\n",
            "\n",
            "\tTraining loss 4.83593\n",
            "\tValidation loss 3.36217\n",
            "\tTest loss 2.81674\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}