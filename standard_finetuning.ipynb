{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMywnrrJwtY4oPStCXDS8Uj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gekoramy/uni.deep-learning/blob/standard-finetuning/standard_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard finetuning\n",
        "In this notebook we propose the straightforward solution to fine tune CLIP. The general idea is to add linear layer(s) on top of the 1024 visual features of CLIP."
      ],
      "metadata": {
        "id": "f2-hDNWnh17V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependences"
      ],
      "metadata": {
        "id": "wvl6tsfDnYF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchvision\n",
        "torchinfo\n",
        "tqdm\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "pip install -q git+https://github.com/openai/CLIP.git\n",
        "pip install -q ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fea1Tqrjna9r",
        "outputId": "3a910b85-0b2f-4268-85db-c543ee837133"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftfy\n",
            "jaxtyping\n",
            "jupyter\n",
            "matplotlib\n",
            "pydantic\n",
            "regex\n",
            "torch\n",
            "torchvision\n",
            "torchinfo\n",
            "tqdm\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.0/613.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "import itertools as it\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torchinfo import summary\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "8y5MDf2Znfhz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-EhQiGFOnh19",
        "outputId": "48fbe602-b929-44da-e0bc-564e1599ef51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "First of all we have to load the dataset."
      ],
      "metadata": {
        "id": "uxmYL_4Fm5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DaKg6AMnlxI",
        "outputId": "d692bb45-353f-4527-b451-9db2ca1c0e0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [05:29<00:00, 40.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folder paths"
      ],
      "metadata": {
        "id": "n4GX6q90oYsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join('dataset', 'refcocog', '')\n",
        "data_instances = os.path.join(root, 'annotations', 'instances.json')\n",
        "data_refs = os.path.join(root, 'annotations', 'refs(umd).p')\n",
        "data_images = os.path.join(root, 'images', '')"
      ],
      "metadata": {
        "id": "JJbFTbIUoaj1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type declaration"
      ],
      "metadata": {
        "id": "MoOugvH1oeDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar('I')\n",
        "P = TypeVar('P')\n",
        "B = TypeVar('B')\n",
        "T = TypeVar('T')\n",
        "\n",
        "Img = UInt[torch.Tensor, 'C W H']\n",
        "BBox = UInt[torch.Tensor, '4']\n",
        "Split = Literal['train', 'test', 'val']\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    #segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: float  # number of pixel of the described object\n",
        "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[float, float, float, float]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "OvQy6d2zofr9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@dataclass\n",
        "#class Prediction:\n",
        "#  image\n",
        "#  description: list[str]  # natural language descriptions of the area of interest\n",
        "#  ground_truth_bbox: tuple[float, float, float, float] # ground truth bounding box\n",
        "#  output_bbox: tuple[float, float, float, float] # predicted bounding box\n",
        "\n",
        "class Prediction:\n",
        "  def __init__(self, image, description, ground_truth_bbox, output_bbox):\n",
        "    self.image = image\n",
        "    self.description = description\n",
        "    self.ground_truth_bbox = ground_truth_bbox\n",
        "    self.output_bbox = output_bbox"
      ],
      "metadata": {
        "id": "i-oyJk7-tCyK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset infos"
      ],
      "metadata": {
        "id": "3su2K8c-ooJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "    \"\"\"\n",
        "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
      ],
      "metadata": {
        "id": "PnnSRWu7op9F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, 'rb') as f:\n",
        "    raw = pickle.load(f)"
      ],
      "metadata": {
        "id": "RYmWj74WowGu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refs: list[Ref] = [\n",
        "    fix_ref(Ref(**ref))\n",
        "    for ref in raw\n",
        "]"
      ],
      "metadata": {
        "id": "BsEw2-a4ozSz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, 'r') as f:\n",
        "    raw = json.load(f)"
      ],
      "metadata": {
        "id": "YhDW2ixWo1f3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instances: Instances = Instances(**raw)"
      ],
      "metadata": {
        "id": "ZsXZOVAeo3pk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2annotation: Mapping[int, Annotation] = {\n",
        "    x.id: x\n",
        "    for x in instances.annotations\n",
        "}"
      ],
      "metadata": {
        "id": "9ouuDD9do6DA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define custom dataset"
      ],
      "metadata": {
        "id": "F8643oAXo7en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[I, P, B]]):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        img_transform: Callable[[Img], I] = lambda x: x,\n",
        "        prompt_transform: Callable[[list[Sentence]], P] = lambda ps: [ p.sent for p in ps ],\n",
        "        bb_transform: Callable[[Float[torch.Tensor, '4']], B] = lambda x: x\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param split: train, test or val\n",
        "        :param img_transform: apply transformation on the processed images\n",
        "        :param prompt_transform: apply transformation on the prompts\n",
        "        :param bb_transform: apply transformation on the bounding box\n",
        "        \"\"\"\n",
        "        self.img_transform = img_transform\n",
        "        self.prompt_transform = prompt_transform\n",
        "        self.bb_transform = bb_transform\n",
        "\n",
        "        # Internally the dataset is a list of tuple[str, list[Sentence], UInt[torch.Tensor, '4']]\n",
        "        # Such that:\n",
        "        # str                     : image filename\n",
        "        # list[Sentence]          : list of reference expression objects\n",
        "        # UInt[torch.Tensor, '4'] : bounding box\n",
        "        self.items: list[tuple[str, list[Sentence], Float[torch.Tensor, '4']]] = [\n",
        "            (i, ps, o)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ps in [ref.sentences]\n",
        "            for o in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.float)]\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.items)\n",
        "\n",
        "\n",
        "    def __getitem__(self, item: int) -> tuple[I, P, B]:\n",
        "        i, ps, b = self.items[item]\n",
        "        return (\n",
        "            self.img_transform(read_image(i)),\n",
        "            self.prompt_transform(ps),\n",
        "            self.bb_transform(b),\n",
        "        )"
      ],
      "metadata": {
        "id": "ila4lq8fpBew"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training free CLIP results\n",
        "For the sake of comparison with the implementations below, we have to evaluate CLIP training free with the same portion of the dataset."
      ],
      "metadata": {
        "id": "zLALd5hoioXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load yolo model"
      ],
      "metadata": {
        "id": "Mk0DZ5N4p4DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "yolo_model.to(device=device).eval()"
      ],
      "metadata": {
        "id": "Fua3LH2Sp634"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CLIP model"
      ],
      "metadata": {
        "id": "fbQn4ZHkp9hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, preprocess = clip.load('RN50')\n",
        "clip_model = clip_model.to(device=device).eval()"
      ],
      "metadata": {
        "id": "y9arYDB1qB0O",
        "outputId": "e692508f-5217-4e05-c501-96c87f5d717e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 244M/244M [00:02<00:00, 111MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline evaluation"
      ],
      "metadata": {
        "id": "Kdd3m0OjqH5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='test')"
      ],
      "metadata": {
        "id": "CTc9kALQqRYI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "7SMcpZbbqYag",
        "outputId": "475b569c-e51d-4645-d1f1-33a0431d4e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating DataLoader's with batch size 1 and 2 workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ITER: int = 10 # max number of iterations\n",
        "current_iteration: int = 0\n",
        "\n",
        "stored_predictions: list[Prediction] = []\n",
        "\n",
        "ious: list[float] = []\n",
        "coss: list[float] = []\n",
        "euds: list[float] = []\n",
        "\n",
        "batch: tuple[UInt[torch.Tensor, '1 C W H'], tuple[list[tuple[str]]], UInt[torch.Tensor, '1 4']]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(it.islice(iter(test_dataloader), min(MAX_ITER, len(test_dataloader))), total=min(MAX_ITER, len(test_dataloader))):\n",
        "        [img], (prompts), [true_xywh] = batch\n",
        "\n",
        "        [true_xyxy] = torchvision.ops.box_convert(true_xywh.unsqueeze(0), in_fmt='xywh', out_fmt='xyxy')\n",
        "\n",
        "        img_pil: Image = transforms.ToPILImage()(img)\n",
        "\n",
        "        # yolo bboxes\n",
        "        predictions = yolo_model(img_pil)\n",
        "\n",
        "        # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
        "        # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
        "        bboxes: Float[torch.Tensor, 'X 6'] = predictions.xyxy[0]\n",
        "\n",
        "        # if empty, put a bbox equal to image size\n",
        "        if len(bboxes) == 0:\n",
        "            bboxes = torch.tensor([[0, 0, img.size()[1], img.size()[2], 0, 0]], dtype=torch.float)\n",
        "\n",
        "        # from yolo bboxes to cropped images\n",
        "        crops: list[Image] = [\n",
        "            img_pil.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        # clip preprocess on cropped images\n",
        "        preprocess_crops: Float[torch.Tensor, 'X 3 244 244'] = torch.stack([\n",
        "            preprocess(crop)\n",
        "            for crop in crops\n",
        "        ]).to(device=device)\n",
        "\n",
        "        # format each available prompt\n",
        "        prompts_tokens: Int[torch.Tensor, 'P 77'] = clip.tokenize([\n",
        "            template.format(prompt)\n",
        "            for template in [\"{}\", \"A photo of {}\", \"We can see {}\"]\n",
        "            for (prompt,) in prompts  # <- ¯\\_(ツ)_/¯\n",
        "        ])\n",
        "\n",
        "        # clip scores\n",
        "        ass_z: tuple[Float[torch.Tensor, 'X P'], Float[torch.Tensor, 'P X']] = clip_model(preprocess_crops, prompts_tokens)\n",
        "        _, logits_per_prompt = ass_z\n",
        "\n",
        "        # final prediction\n",
        "        best_match: int = torch.argmax(torch.max(logits_per_prompt, 0).values).item()\n",
        "        prediction_bbox: Float[torch.Tensor, '4'] = bboxes[best_match][:4]\n",
        "\n",
        "        # metrics\n",
        "        iou: float = torchvision.ops.box_iou(true_xyxy.unsqueeze(0), prediction_bbox.unsqueeze(0)).item()\n",
        "        ious.append(iou)\n",
        "\n",
        "        rectangle: tuple[int, int, int, int] = true_xyxy.tolist()\n",
        "        ground_truth_crop = img_pil.crop(rectangle)\n",
        "\n",
        "        rectangle: tuple[int, int, int, int] = torch.tensor(prediction_bbox, dtype=torch.int).tolist()\n",
        "        prediction_crop = img_pil.crop(rectangle)\n",
        "\n",
        "        # from float16 to float32\n",
        "        X: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "            clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).unsqueeze(0)),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "        Y: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "            clip_model.encode_image(torch.tensor(preprocess(prediction_crop)).unsqueeze(0)),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "\n",
        "        cos: float = F.cosine_similarity(X, Y).item()\n",
        "        coss.append(cos)\n",
        "\n",
        "        eud: float = torch.cdist(X, Y, p=2).item()\n",
        "        euds.append(eud)\n",
        "\n",
        "        # store the prediction\n",
        "        pred : Prediction = Prediction(\n",
        "              image = img,\n",
        "              description = [p[0] for p in prompts],\n",
        "              ground_truth_bbox = true_xyxy,\n",
        "              output_bbox = prediction_bbox\n",
        "            )\n",
        "        stored_predictions.append(pred)\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uhBHV1tOsNnQ",
        "outputId": "ab84f45b-cc93-4c4f-cc7d-9c2a8c935b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:41<00:00,  4.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_tokens.shape"
      ],
      "metadata": {
        "id": "vKDl8y9VHZGR",
        "outputId": "65feffb8-1bef-4cc4-c0a9-d901c55991a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 77])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_crops.shape"
      ],
      "metadata": {
        "id": "TjI1GRSCHcJR",
        "outputId": "83436927-55e8-43ca-83fd-e3627209c58d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stored_predictions)"
      ],
      "metadata": {
        "id": "0MjpvuYuwOqa",
        "outputId": "51299f8a-b497-4b6d-fe88-bb8809b8c068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance:"
      ],
      "metadata": {
        "id": "r42zXJCU9ZKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ious: {torch.mean(torch.tensor(ious, dtype=torch.float))}\")\n",
        "print(f\"coss: {torch.mean(torch.tensor(coss, dtype=torch.float))}\")\n",
        "print(f\"euds: {torch.mean(torch.tensor(euds, dtype=torch.float))}\")"
      ],
      "metadata": {
        "id": "djZmlT_99bCO",
        "outputId": "709ddc7e-ed65-47a4-d60b-809c34a0c84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ious: 0.8920486569404602\n",
            "coss: 0.9736908078193665\n",
            "euds: 0.46590811014175415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to display a random sample of predictions."
      ],
      "metadata": {
        "id": "whuSrASMjyQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# args:\n",
        "#  - predictionList: [Prediction]\n",
        "#  - numPred: int :: if numPred==-1 (default) consider all the predictions in predictionList\n",
        "def display_predictions(predictionList, numPred=-1):\n",
        "  limit = 0\n",
        "  for p in predictionList:\n",
        "    if numPred!=-1 and limit >= numPred:\n",
        "      return;\n",
        "    limit += 1\n",
        "\n",
        "    p_image = p.image\n",
        "    p_description = p.description\n",
        "    p_ground_truth_bbox = p.ground_truth_bbox\n",
        "    p_output_bbox = p.output_bbox\n",
        "\n",
        "    # TODO: concatenate\n",
        "    p_image = draw_bounding_boxes(p_image, p_ground_truth_bbox.unsqueeze(0), colors=\"green\", width=5)\n",
        "    p_image = draw_bounding_boxes(p_image, p_output_bbox.unsqueeze(0), colors=\"red\", width=5)\n",
        "\n",
        "    tensor_to_pil = transforms.ToPILImage()\n",
        "    image_pil = tensor_to_pil(p_image)\n",
        "    display(image_pil)\n",
        "    print(p_description)\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "tMr7_75mj1lc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_predictions(stored_predictions, 3)"
      ],
      "metadata": {
        "id": "WTTWDphPk0lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Region proposal networks"
      ],
      "metadata": {
        "id": "vLtQfgxbllEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv5"
      ],
      "metadata": {
        "id": "otahfdYBlpee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Yolo_v5(torch.nn.Module):\n",
        "  def __init__(self, device=device):\n",
        "    super().__init__()\n",
        "\n",
        "    # load yolo model\n",
        "    yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "    yolo_model.to(device=device).eval()\n",
        "\n",
        "  def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
        "    # convert image tensor to image PIL\n",
        "    pil_transormation = transforms.ToPILImage()\n",
        "    img_pil = [pil_transormation(_img) for _img in img]\n",
        "\n",
        "    #######print(f\"img_pil 0: {type(img_pil)}\")\n",
        "    #######print(f\"img_pil 1: {len(img_pil)}\")\n",
        "\n",
        "    # yolo bboxes\n",
        "    predictions = yolo_model(img_pil)\n",
        "    #######print(f\"predictions type: {type(predictions)}\")\n",
        "    #######predictions.show()\n",
        "\n",
        "    #######print(\"predictions.xyxy\")\n",
        "    #######print(predictions.xyxy)\n",
        "    #######print(\"predictions.xyxy len\")\n",
        "    #######print(len(predictions.xyxy))\n",
        "\n",
        "    # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
        "    # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
        "    bboxes: list[Float[torch.Tensor, 'X 6']] = predictions.xyxy # bboxes[i] contains the bboxes highlighted by yolo in image i\n",
        "\n",
        "    #######print(\"len bboxes\")\n",
        "    #######print(len(bboxes))\n",
        "\n",
        "    for image_idx, bbox_img in enumerate(bboxes):\n",
        "      # if empty, put a bbox equal to image size\n",
        "      if len(bbox_img) == 0:\n",
        "          bbox_img = torch.tensor([[0, 0, img_pil[image_idx].size()[1], img_pil[image_idx].size()[2], 0, 0]], dtype=torch.float)  # TODO: test this piece of code\n",
        "\n",
        "    return bboxes"
      ],
      "metadata": {
        "id": "mIPlXyDYluhh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of image encoder"
      ],
      "metadata": {
        "id": "7gkTu_CPiODe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summary(clip_model.visual, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
      ],
      "metadata": {
        "id": "dHkbnXViCNO-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summary(clip_model.transformer, input_size=(77,512), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
      ],
      "metadata": {
        "id": "7np9w3zxIZ5o"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell we create a neural network that builds a linear head on top of the visual encoder of CLIP."
      ],
      "metadata": {
        "id": "2zrJ7zGnefl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_SF_image_encoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual\n",
        "\n",
        "    # freeze all pretrained layers by setting requires_grad=False\n",
        "    for param in self.encoder.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # add a linear layer\n",
        "    self.fc1 = nn.Linear(1024, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 1024)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # visual encoder\n",
        "    with torch.no_grad():\n",
        "      x = self.encoder(x)\n",
        "    # ---\n",
        "\n",
        "    # linear head\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    # ---\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "uzIAPPVb-zVZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = CLIP_SF_image_encoder().to(device)\n",
        "summary(net, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
      ],
      "metadata": {
        "id": "kBylgcYPfE-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, _lr, _wd, _momentum):\n",
        "  optimizer = torch.optim.SGD(  params = model.parameters(),\n",
        "                                lr = _lr,\n",
        "                                weight_decay = _wd,\n",
        "                                momentum = _momentum)\n",
        "\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "4eijWkD2hsFl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  def iou_loss(bbox_prediction, bbox_groundtruth):\n",
        "    return -1*torchvision.ops.box_iou(bbox_groundtruth, bbox_prediction).item()\n",
        "\n",
        "  return iou_loss"
      ],
      "metadata": {
        "id": "IqhMhbwKhv79"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_function():\n",
        "  def iou_accuracy(bbox_prediction, bbox_groundtruth):\n",
        "    return torchvision.ops.box_iou(bbox_groundtruth, bbox_prediction).item()\n",
        "  return iou_accuracy"
      ],
      "metadata": {
        "id": "zPaGE1cFtCa3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with different size tensors in dataloader\n",
        "def custom_collate(batch) -> tuple[list[Img], list[str], list[Float[torch.Tensor, '4']]]:\n",
        "    images = [item[0] for item in batch]\n",
        "    prompts = [item[1] for item in batch]\n",
        "    bboxes = [item[2] for item in batch]\n",
        "\n",
        "    return images, prompts, bboxes"
      ],
      "metadata": {
        "id": "hMXTKpuh_pgF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(\n",
        "    model: torch.nn.Module,                     # neural network to be trained\n",
        "    region_proposal_model: torch.nn.Module,     # region proposal model\n",
        "    data_loader: torch.utils.data.DataLoader,   # [train_dataset]\n",
        "    loss_fn: torch.nn.Module,                   # todo: in our case it is not correct nn.Module, test data type\n",
        "    optimizer: torch.optim.Optimizer,           # optimizer\n",
        "    accuracy_fn,                                # accuracy function\n",
        "    max_sample: int = -1,                       # useful during the experiments to set an upper bound on the number of samples to be evaluated (-1 :: no limit)\n",
        "    device: torch.device = device               # target device\n",
        "    ):\n",
        "\n",
        "  print(\"training_step\")\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0 #todo riflettere anche su questo\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  num_iteration: int = 0  # keep track of the number of iterations\n",
        "  for batch_idx, (img, prompts, true_xywh) in tqdm(it.islice(enumerate(data_loader), (len(data_loader) if max_sample == -1 else max_sample))):\n",
        "\n",
        "    \"\"\"\n",
        "    print()\n",
        "    print(f\"batch_idx: {batch_idx}\")\n",
        "    print(f\"img: {img}\")\n",
        "    print(f\"type img: {type(img)}\")\n",
        "    print(f\"prompts: {prompts}\")\n",
        "    print(f\"true_xywh: {true_xywh}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # check number of iteration\n",
        "    if max_sample!=-1 and num_iteration>=max_sample:\n",
        "      print(\"\\n STOP TRAINING LOOP FOR MAX ITERATION PARAMETER \\n\")\n",
        "      break\n",
        "    num_iteration+=1\n",
        "\n",
        "    # send data to target device\n",
        "    for _img in img:\n",
        "      _img = _img.to(device)\n",
        "\n",
        "    true_xywh = torch.stack((true_xywh)).to(device)\n",
        "\n",
        "    print(f\"true_xywh new: {true_xywh}\")\n",
        "    print(f\"true_xywh new shape: {true_xywh.shape}\")\n",
        "\n",
        "    # convert bbox to the proper format\n",
        "    [true_xyxy] = torchvision.ops.box_convert(true_xywh.unsqueeze(0), in_fmt='xywh', out_fmt='xyxy')\n",
        "\n",
        "    print(\"len: \")\n",
        "    print(len(true_xyxy))\n",
        "    print(true_xyxy)\n",
        "\n",
        "    # forward pass\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # i. region proposal\n",
        "      bboxes = region_proposal_model(img)\n",
        "\n",
        "      print(f\"\\n\\n REGION PROPOSAL ALGORITHM DONE bounding boxes: {bboxes}\\n\\n\")\n",
        "\n",
        "      \"\"\"\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops: list[Image] = [\n",
        "          img_pil.crop((xmin, ymin, xmax, ymax))\n",
        "          for bbox in bboxes\n",
        "          for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "      ]\n",
        "      \"\"\"\n",
        "\n",
        "      # from yolo bboxes to cropped images\n",
        "      crops = []\n",
        "      for batch_image, batch_image_bboxes in zip(img, bboxes):\n",
        "        print(f\"batch_image: {batch_image.shape}\")\n",
        "        print(f\"batch_image_bboxes: {batch_image_bboxes}\")\n",
        "        batch_image_pil: Image = transforms.ToPILImage()(batch_image)\n",
        "\n",
        "        list_bboxes_image: list[Image] = [\n",
        "            batch_image_pil.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in batch_image_bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        crops.append(list_bboxes_image)\n",
        "\n",
        "      # ii. CLIP preprocessing  # todo: riflettere se mettere il preprocessing di CLIP nel nn.Module\n",
        "      # clip preprocess on cropped images\n",
        "      preproceessed_crops_batch = list()\n",
        "      for image_crops in crops:\n",
        "        preprocess_crops: Float[torch.Tensor, 'X 3 244 244'] = torch.stack([\n",
        "            preprocess(crop)\n",
        "            for crop in image_crops\n",
        "        ]).to(device=device)\n",
        "        preproceessed_crops_batch.append(preprocess_crops)\n",
        "\n",
        "      \"\"\"\n",
        "      preprocessed_crops_tensor = torch.stack(preproceessed_crops_batch)\n",
        "      print(\"\\npreproceess_crops_batch\")\n",
        "      print(len(preproceessed_crops_batch))\n",
        "      print(preproceessed_crops_batch[0].shape)\n",
        "      print(preproceessed_crops_batch[1].shape)\n",
        "      print()\n",
        "      print(\"\\preprocessed_crops_tensor\")\n",
        "      print(len(preprocessed_crops_tensor))\n",
        "      print(preprocessed_crops_tensor.shape)\n",
        "      \"\"\"\n",
        "\n",
        "      # format each available prompt\n",
        "      prompts_tokens = list()\n",
        "      for batch_item_prompt in prompts:\n",
        "\n",
        "        print(\"!!!!batch_item_prompt!!!\")\n",
        "        print(batch_item_prompt)\n",
        "\n",
        "        batch_item_prompt_tokens: Int[torch.Tensor, 'P 77'] = clip.tokenize([\n",
        "            template.format(prompt)\n",
        "            for template in [\"{}\", \"A photo of {}\", \"We can see {}\"]\n",
        "            for prompt in batch_item_prompt  # <- ¯\\_(ツ)_/¯\n",
        "        ])\n",
        "        prompts_tokens.append(batch_item_prompt_tokens)\n",
        "\n",
        "      print(\"\\prompts_tokens\")\n",
        "      print(len(prompts_tokens))\n",
        "      print(prompts_tokens[0].shape)\n",
        "      print(prompts_tokens[1].shape)\n",
        "\n",
        "    # iii. process visual prompts\n",
        "    model_encoding = model(preproceess_crops_batch)\n",
        "\n",
        "    print(\"model_encoding\")\n",
        "    print(model_encoding)\n",
        "\n",
        "    # iv. evaluate similarity bbox - prompt\n",
        "\n",
        "    # v. final prediction\n",
        "\n",
        "    # loss and accuracy computation\n",
        "\n",
        "    # optimizer zero grad\n",
        "\n",
        "    # loss backward\n",
        "\n",
        "    # optimizer step\n",
        "\n",
        "    ass_z: tuple[Float[torch.Tensor, 'X P'], Float[torch.Tensor, 'P X']] = clip_model(preprocess_crops, prompts_tokens)\n",
        "    _, logits_per_prompt = ass_z\n",
        "\n",
        "    # final prediction\n",
        "    best_match: int = torch.argmax(torch.max(logits_per_prompt, 0).values).item()\n",
        "    prediction_bbox: Float[torch.Tensor, '4'] = bboxes[best_match][:4]\n",
        "\n",
        "    # metrics\n",
        "    iou: float = torchvision.ops.box_iou(true_xyxy.unsqueeze(0), prediction_bbox.unsqueeze(0)).item()\n",
        "    ious.append(iou)\n",
        "\n",
        "    rectangle: tuple[int, int, int, int] = true_xyxy.tolist()\n",
        "    ground_truth_crop = img_pil.crop(rectangle)\n",
        "\n",
        "    rectangle: tuple[int, int, int, int] = torch.tensor(prediction_bbox, dtype=torch.int).tolist()\n",
        "    prediction_crop = img_pil.crop(rectangle)\n",
        "\n",
        "    # from float16 to float32\n",
        "    X: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "        clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).unsqueeze(0)),\n",
        "        dtype=torch.float\n",
        "    )\n",
        "    Y: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "        clip_model.encode_image(torch.tensor(preprocess(prediction_crop)).unsqueeze(0)),\n",
        "        dtype=torch.float\n",
        "    )\n",
        "\n",
        "    cos: float = F.cosine_similarity(X, Y).item()\n",
        "    coss.append(cos)\n",
        "\n",
        "    eud: float = torch.cdist(X, Y, p=2).item()\n",
        "    euds.append(eud)\n",
        "\n",
        "  # Calculate loss (todo: and accuracy) per epoch and print out what's happening\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train accuracy: --\")\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "J_kXMhb3liJ4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.eval() # put model in eval mode\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      for X, y in data_loader:\n",
        "          # Send data to GPU\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred = model(X)\n",
        "\n",
        "          # 2. Calculate loss and accuracy\n",
        "          test_loss += loss_fn(test_pred, y)\n",
        "          test_acc += accuracy_fn(y_true=y,\n",
        "              y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "          )\n",
        "\n",
        "      # Adjust metrics and print out\n",
        "      test_loss /= len(data_loader)\n",
        "      test_acc /= len(data_loader)\n",
        "      print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
        "      return test_loss, test_acc"
      ],
      "metadata": {
        "id": "_gTttBpugajz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put all together in a training loop."
      ],
      "metadata": {
        "id": "qhpk3G1FiK6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "EbT8N4tIij_s"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the network and move it to the chosen device (GPU)\n",
        "net = CLIP_SF_image_encoder().to(device)\n",
        "\n",
        "# instantiate the region proposal algorithm\n",
        "yolo = Yolo_v5().to(device)"
      ],
      "metadata": {
        "id": "do9PU7qLBj0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a manual seed allow us to provide reprudicible results in this notebook\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# measure time\n",
        "train_time_start = timer()  # todo: forse misurando il tempo possiamo far apprezzare la differenza di tempo di esecuzione del training quando abbiamo fatto il preprocessing delle bounding box vs senza\n",
        "\n",
        "# create a logger for the experiment\n",
        "writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "# get dataset instance\n",
        "train_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='train')\n",
        "test_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='test')\n",
        "val_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='val')\n",
        "print(f\"LEN_TRAIN_DATASET: {len(train_dataset)}, LEN_TEST_DATASET: {len(test_dataset)}, LEN_VALIDATION_DATASET: {len(val_dataset)}\")\n",
        "\n",
        "# get dataloaders\n",
        "BATCH_SIZE = 2\n",
        "#NUM_WORKERS = os.cpu_count()\n",
        "NUM_WORKERS = 1\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=custom_collate,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False\n",
        ")\n",
        "print(f\"LEN_TRAIN_DATALOADER: {len(train_loader)}, LEN_TEST_DATALOADER: {len(val_loader)}, LEN_VALIDATION_DATALOADER: {len(test_loader)}\")\n",
        "\n",
        "# instantiate the optimizer\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.000001\n",
        "momentum = 0.9\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "# define the cost function\n",
        "cost_function = get_cost_function()\n",
        "\n",
        "# define the accuracy function\n",
        "accuracy_fn = get_accuracy_function()\n",
        "\n",
        "# computes evaluation results before training\n",
        "print('Before training:')\n",
        "#train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "#val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "#test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "#log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "#log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "#log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "#print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "#print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "#print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_loss, train_accuracy = training_step(\n",
        "        model = net,\n",
        "        region_proposal_model = yolo,\n",
        "        data_loader = train_loader,\n",
        "        loss_fn = cost_function,\n",
        "        optimizer = optimizer,\n",
        "        accuracy_fn = accuracy_fn,\n",
        "        max_sample = 5\n",
        "    )\n",
        "    val_loss, val_accuracy = test_step(data_loader=test_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)\n",
        "# compute final evaluation results\n",
        "print('After training:')\n",
        "train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "# log to TensorBoard\n",
        "log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "# closes the logger\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKuhrvQmhouV",
        "outputId": "dc86997f-eacf-4046-c653-3ef3a6008472"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN_TRAIN_DATASET: 42226, LEN_TEST_DATASET: 5023, LEN_VALIDATION_DATASET: 2573\n",
            "Creating DataLoader's with batch size 2 and 1 workers.\n",
            "LEN_TRAIN_DATALOADER: 21113, LEN_TEST_DATALOADER: 1287, LEN_VALIDATION_DATALOADER: 2512\n",
            "Before training:\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "training_step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true_xywh new: tensor([[231.17999, 133.42999, 135.05000, 181.49001],\n",
            "        [  2.09000, 143.55000, 198.84000, 225.61000]])\n",
            "true_xywh new shape: torch.Size([2, 4])\n",
            "len: \n",
            "2\n",
            "tensor([[231.17999, 133.42999, 366.22998, 314.91998],\n",
            "        [  2.09000, 143.55000, 200.92999, 369.16000]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:01, ?it/s]\n",
            "  0%|          | 0/3 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " REGION PROPOSAL ALGORITHM DONE bounding boxes: [tensor([[232.87865, 133.94928, 368.31976, 317.77301,   0.90299,  23.00000],\n",
            "        [452.63312,  88.27678, 506.02020, 302.33929,   0.80900,  23.00000]]), tensor([[2.79421e+02, 2.73158e+02, 4.64872e+02, 3.56336e+02, 9.51477e-01, 6.60000e+01],\n",
            "        [4.55808e+02, 3.38968e+02, 4.90929e+02, 3.71173e+02, 9.21552e-01, 6.40000e+01],\n",
            "        [2.40336e+02, 1.28781e+02, 4.22512e+02, 2.50913e+02, 9.04321e-01, 6.20000e+01],\n",
            "        [3.22876e-01, 5.32130e+00, 1.81828e+02, 1.78073e+02, 9.01048e-01, 6.20000e+01],\n",
            "        [4.27706e+02, 1.75113e+02, 6.31619e+02, 3.62734e+02, 8.22683e-01, 6.30000e+01],\n",
            "        [9.42519e+00, 1.46898e+02, 1.97371e+02, 3.61229e+02, 8.12262e-01, 6.20000e+01],\n",
            "        [4.22054e+02, 2.05124e+02, 4.49704e+02, 2.76256e+02, 6.74767e-01, 3.90000e+01],\n",
            "        [4.65951e+02, 2.69281e+02, 5.93840e+02, 3.32168e+02, 5.00137e-01, 6.60000e+01],\n",
            "        [5.47626e+02, 3.32291e+02, 6.40000e+02, 4.17268e+02, 4.13105e-01, 7.30000e+01],\n",
            "        [4.37540e+02, 1.78253e+02, 6.28032e+02, 3.60341e+02, 2.81529e-01, 6.60000e+01]])]\n",
            "\n",
            "\n",
            "batch_image: torch.Size([3, 427, 640])\n",
            "batch_image_bboxes: tensor([[232.87865, 133.94928, 368.31976, 317.77301,   0.90299,  23.00000],\n",
            "        [452.63312,  88.27678, 506.02020, 302.33929,   0.80900,  23.00000]])\n",
            "batch_image: torch.Size([3, 426, 640])\n",
            "batch_image_bboxes: tensor([[2.79421e+02, 2.73158e+02, 4.64872e+02, 3.56336e+02, 9.51477e-01, 6.60000e+01],\n",
            "        [4.55808e+02, 3.38968e+02, 4.90929e+02, 3.71173e+02, 9.21552e-01, 6.40000e+01],\n",
            "        [2.40336e+02, 1.28781e+02, 4.22512e+02, 2.50913e+02, 9.04321e-01, 6.20000e+01],\n",
            "        [3.22876e-01, 5.32130e+00, 1.81828e+02, 1.78073e+02, 9.01048e-01, 6.20000e+01],\n",
            "        [4.27706e+02, 1.75113e+02, 6.31619e+02, 3.62734e+02, 8.22683e-01, 6.30000e+01],\n",
            "        [9.42519e+00, 1.46898e+02, 1.97371e+02, 3.61229e+02, 8.12262e-01, 6.20000e+01],\n",
            "        [4.22054e+02, 2.05124e+02, 4.49704e+02, 2.76256e+02, 6.74767e-01, 3.90000e+01],\n",
            "        [4.65951e+02, 2.69281e+02, 5.93840e+02, 3.32168e+02, 5.00137e-01, 6.60000e+01],\n",
            "        [5.47626e+02, 3.32291e+02, 6.40000e+02, 4.17268e+02, 4.13105e-01, 7.30000e+01],\n",
            "        [4.37540e+02, 1.78253e+02, 6.28032e+02, 3.60341e+02, 2.81529e-01, 6.60000e+01]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-1d59078dbd98>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch}\\n---------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     train_loss, train_accuracy = training_step(\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mregion_proposal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-f59d43dd62ac>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, region_proposal_model, data_loader, loss_fn, optimizer, accuracy_fn, max_sample, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mpreproceessed_crops_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       \u001b[0mpreprocessed_crops_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproceessed_crops_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\npreproceess_crops_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproceessed_crops_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3, 224, 224] at entry 0 and [10, 3, 224, 224] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaulate our trained model using `eval_model()` function."
      ],
      "metadata": {
        "id": "YZN6speahsmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}"
      ],
      "metadata": {
        "id": "qF_q94NRiBik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Note: This will error due to `eval_model()` not using device agnostic code\n",
        "model_1_results = eval_model(model=model_1,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "aovaihRXhsIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of text encoder"
      ],
      "metadata": {
        "id": "OWZqXoCZiSvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of image encoder and on top of text encoder"
      ],
      "metadata": {
        "id": "QcyLAD4tiWmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bottleneck"
      ],
      "metadata": {
        "id": "BiuUlbSSiaxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYB7nWO8huJZ"
      },
      "outputs": [],
      "source": []
    }
  ]
}