{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhKYMQUll00txkuSXCMqwq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gekoramy/uni.deep-learning/blob/standard-finetuning/standard_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard finetuning\n",
        "In this notebook we propose the straightforward solution to fine tune CLIP. The general idea is to add linear layer(s) on top of the 1024 visual features of CLIP."
      ],
      "metadata": {
        "id": "f2-hDNWnh17V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependences"
      ],
      "metadata": {
        "id": "wvl6tsfDnYF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "tee requirements.txt << END\n",
        "ftfy\n",
        "jaxtyping\n",
        "jupyter\n",
        "matplotlib\n",
        "pydantic\n",
        "regex\n",
        "torch\n",
        "torchvision\n",
        "tqdm\n",
        "END\n",
        "\n",
        "pip install -q -r requirements.txt\n",
        "pip install -q git+https://github.com/openai/CLIP.git\n",
        "pip install -q ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fea1Tqrjna9r",
        "outputId": "862155f9-c779-4874-a2aa-54a267243e16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftfy\n",
            "jaxtyping\n",
            "jupyter\n",
            "matplotlib\n",
            "pydantic\n",
            "regex\n",
            "torch\n",
            "torchvision\n",
            "tqdm\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.5/617.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from datetime import datetime\n",
        "from jaxtyping import Float, UInt, Int\n",
        "from pydantic.dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from typing import Literal, Callable, Mapping, TypeVar\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8y5MDf2Znfhz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-EhQiGFOnh19",
        "outputId": "d074e54f-c896-468f-f881-7f59ddffeb86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "First of all we have to load the dataset."
      ],
      "metadata": {
        "id": "uxmYL_4Fm5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if ! [ -d dataset ]; then\n",
        "  mkdir dataset &&\n",
        "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
        "  tar -xf refcocog.tar.gz -C dataset &&\n",
        "  rm refcocog.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DaKg6AMnlxI",
        "outputId": "21845092-9b0c-439d-d6db-960323acd40c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [02:36<00:00, 86.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folder paths"
      ],
      "metadata": {
        "id": "n4GX6q90oYsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.path.join('dataset', 'refcocog', '')\n",
        "data_instances = os.path.join(root, 'annotations', 'instances.json')\n",
        "data_refs = os.path.join(root, 'annotations', 'refs(umd).p')\n",
        "data_images = os.path.join(root, 'images', '')"
      ],
      "metadata": {
        "id": "JJbFTbIUoaj1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type declaration"
      ],
      "metadata": {
        "id": "MoOugvH1oeDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = TypeVar('I')\n",
        "P = TypeVar('P')\n",
        "B = TypeVar('B')\n",
        "T = TypeVar('T')\n",
        "\n",
        "Img = UInt[torch.Tensor, 'C W H']\n",
        "BBox = UInt[torch.Tensor, '4']\n",
        "Split = Literal['train', 'test', 'val']\n",
        "\n",
        "@dataclass\n",
        "class Info:\n",
        "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
        "    url: str  # http://mscoco.org/\n",
        "    version: str  # 1.0\n",
        "    year: int  # 2014\n",
        "    contributor: str  # Microsoft COCO group\n",
        "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    license: int  # each image has an associated licence id\n",
        "    file_name: str  # file name of the image\n",
        "    coco_url: str  # example http://mscoco.org/images/131074\n",
        "    height: int\n",
        "    width: int\n",
        "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
        "    id: int  # id of the imag\n",
        "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
        "\n",
        "@dataclass\n",
        "class License:\n",
        "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
        "    id: int  # id of the licence\n",
        "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
        "    area: int  # number of pixel of the described object\n",
        "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
        "    image_id: int  # id of the target image\n",
        "    bbox: tuple[int, int, int, int]  # bounding box coordinates [xmin, ymin, width, height]\n",
        "    category_id: int\n",
        "    id: int  # annotation id\n",
        "\n",
        "@dataclass\n",
        "class Category:\n",
        "    supercategory: str  # example 'vehicle'\n",
        "    id: int  # category id\n",
        "    name: str  # example 'airplane'\n",
        "\n",
        "@dataclass\n",
        "class Instances:\n",
        "    info: Info\n",
        "    images: list[Image]\n",
        "    licenses: list[License]\n",
        "    annotations: list[Annotation]\n",
        "    categories: list[Category]\n",
        "\n",
        "@dataclass\n",
        "class Sentence:\n",
        "    tokens: list[str]  # tokenized version of referring expression\n",
        "    raw: str  # unprocessed referring expression\n",
        "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
        "    sent_id: int  # unique referring expression id\n",
        "\n",
        "@dataclass\n",
        "class Ref:\n",
        "    image_id: int  # unique image id\n",
        "    split: Split\n",
        "    sentences: list[Sentence]\n",
        "    file_name: str  # file name of image relative to img_root\n",
        "    category_id: int  # object category label\n",
        "    ann_id: int  # id of object annotation in instance.json\n",
        "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
        "    ref_id: int  # unique id for refering expression"
      ],
      "metadata": {
        "id": "OvQy6d2zofr9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Prediction:\n",
        "  image: Image  # image on which the prediction has been computed\n",
        "  description: str  # natural language description of the area of interest\n",
        "  ground_truth_bbox: tuple[int, int, int, int] # ground truth bounding box\n",
        "  output_bbox: tuple[int, int, int, int] # predicted bounding box"
      ],
      "metadata": {
        "id": "i-oyJk7-tCyK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset infos"
      ],
      "metadata": {
        "id": "3su2K8c-ooJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ref(x: Ref) -> Ref:\n",
        "    x.file_name = fix_filename(x.file_name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def fix_filename(x: str) -> str:\n",
        "    \"\"\"\n",
        "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
        "    :return:  COCO_..._[image_id].jpg\n",
        "    \"\"\"\n",
        "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
      ],
      "metadata": {
        "id": "PnnSRWu7op9F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_refs, 'rb') as f:\n",
        "    raw = pickle.load(f)"
      ],
      "metadata": {
        "id": "RYmWj74WowGu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refs: list[Ref] = [\n",
        "    fix_ref(Ref(**ref))\n",
        "    for ref in raw\n",
        "]"
      ],
      "metadata": {
        "id": "BsEw2-a4ozSz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_instances, 'r') as f:\n",
        "    raw = json.load(f)"
      ],
      "metadata": {
        "id": "YhDW2ixWo1f3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instances: Instances = Instances(**raw)"
      ],
      "metadata": {
        "id": "ZsXZOVAeo3pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2annotation: Mapping[int, Annotation] = {\n",
        "    x.id: x\n",
        "    for x in instances.annotations\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "9ouuDD9do6DA",
        "outputId": "a7daeae1-3678-47cf-d480-0ea5797dfe9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e3305d8e9590>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m id2annotation: Mapping[int, Annotation] = {\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'instances' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define custom dataset"
      ],
      "metadata": {
        "id": "F8643oAXo7en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset[tuple[I, P, B]]):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split: Split,\n",
        "        img_transform: Callable[[Img], I] = lambda x: x,\n",
        "        prompt_transform: Callable[[list[Sentence]], P] = lambda ps: [ p.sent for p in ps ],\n",
        "        bb_transform: Callable[[UInt[torch.Tensor, '4']], B] = lambda x: x\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param split: train, test or val\n",
        "        :param img_transform: apply transformation on the processed images\n",
        "        :param prompt_transform: apply transformation on the prompts\n",
        "        :param bb_transform: apply transformation on the bounding box\n",
        "        \"\"\"\n",
        "        self.img_transform = img_transform\n",
        "        self.prompt_transform = prompt_transform\n",
        "        self.bb_transform = bb_transform\n",
        "\n",
        "        # Internally the dataset is a list of tuple[str, list[Sentence], UInt[torch.Tensor, '4']]\n",
        "        # Such that:\n",
        "        # str                     : image filename\n",
        "        # list[Sentence]          : list of reference expression objects\n",
        "        # UInt[torch.Tensor, '4'] : bounding box\n",
        "        self.items: list[tuple[str, list[Sentence], UInt[torch.Tensor, '4']]] = [\n",
        "            (i, ps, o)\n",
        "            for ref in refs\n",
        "            if ref.split == split\n",
        "            for i in [os.path.join(data_images, ref.file_name)]\n",
        "            for ps in [ref.sentences]\n",
        "            for o in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.int)]\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.items)\n",
        "\n",
        "\n",
        "    def __getitem__(self, item: int) -> tuple[I, P, B]:\n",
        "        i, ps, b = self.items[item]\n",
        "        return (\n",
        "            self.img_transform(read_image(i)),\n",
        "            self.prompt_transform(ps),\n",
        "            self.bb_transform(b),\n",
        "        )"
      ],
      "metadata": {
        "id": "ila4lq8fpBew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training free CLIP results\n",
        "For the sake of comparison with the implementations below, we have to evaluate CLIP training free with the same portion of the dataset."
      ],
      "metadata": {
        "id": "zLALd5hoioXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load yolo model"
      ],
      "metadata": {
        "id": "Mk0DZ5N4p4DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "yolo_model.to(device=device).eval()"
      ],
      "metadata": {
        "id": "Fua3LH2Sp634"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CLIP model"
      ],
      "metadata": {
        "id": "fbQn4ZHkp9hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, preprocess = clip.load('RN50')\n",
        "clip_model = clip_model.to(device=device).eval()"
      ],
      "metadata": {
        "id": "y9arYDB1qB0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline evaluation"
      ],
      "metadata": {
        "id": "Kdd3m0OjqH5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='test')"
      ],
      "metadata": {
        "id": "CTc9kALQqRYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "7SMcpZbbqYag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ITER: int = 50 # max number of iterations\n",
        "current_iteration: int = 0\n",
        "\n",
        "stored_predictions: list[Prediction] = []\n",
        "\n",
        "ious: list[float] = []\n",
        "coss: list[float] = []\n",
        "euds: list[float] = []\n",
        "\n",
        "batch: tuple[UInt[torch.Tensor, '1 C W H'], tuple[list[tuple[str]]], UInt[torch.Tensor, '1 4']]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(iter(test_dataloader)):\n",
        "\n",
        "        if current_iteration >= MAX_ITER:\n",
        "          print(\"Stop iteration: MAX_ITER = \"+str(MAX_ITER))\n",
        "          break;\n",
        "\n",
        "        [img], (prompts), [true_xywh] = batch\n",
        "\n",
        "        [true_xyxy] = torchvision.ops.box_convert(true_xywh.unsqueeze(0), in_fmt='xywh', out_fmt='xyxy')\n",
        "\n",
        "        img_pil: Image = transforms.ToPILImage()(img)\n",
        "\n",
        "        # yolo bboxes\n",
        "        predictions = yolo_model(img_pil)\n",
        "\n",
        "        # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
        "        # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
        "        bboxes: Float[torch.Tensor, 'X 6'] = predictions.xyxy[0]\n",
        "\n",
        "        # if empty, put a bbox equal to image size\n",
        "        if len(bboxes) == 0:\n",
        "            bboxes = torch.tensor([[0, 0, img.size()[1], img.size()[2], 0, 0]], dtype=torch.float)\n",
        "\n",
        "        # from yolo bboxes to cropped images\n",
        "        crops: list[Image] = [\n",
        "            img_pil.crop((xmin, ymin, xmax, ymax))\n",
        "            for bbox in bboxes\n",
        "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
        "        ]\n",
        "\n",
        "        # clip preprocess on cropped images\n",
        "        preprocess_crops: Float[torch.Tensor, 'X 3 244 244'] = torch.stack([\n",
        "            preprocess(crop)\n",
        "            for crop in crops\n",
        "        ]).to(device=device)\n",
        "\n",
        "        # format each available prompt\n",
        "        prompts_tokens: Int[torch.Tensor, 'P 77'] = clip.tokenize([\n",
        "            template.format(prompt)\n",
        "            for template in [\"{}\", \"A photo of {}\", \"We can see {}\"]\n",
        "            for (prompt,) in prompts  # <- ¯\\_(ツ)_/¯\n",
        "        ])\n",
        "\n",
        "        # clip scores\n",
        "        ass_z: tuple[Float[torch.Tensor, 'X P'], Float[torch.Tensor, 'P X']] = clip_model(preprocess_crops, prompts_tokens)\n",
        "        _, logits_per_prompt = ass_z\n",
        "\n",
        "        # final prediction\n",
        "        best_match: int = torch.argmax(torch.max(logits_per_prompt, 0).values).item()\n",
        "        prediction_bbox: Float[torch.Tensor, '4'] = bboxes[best_match][:4]\n",
        "\n",
        "        # metrics\n",
        "        iou: float = torchvision.ops.box_iou(true_xyxy.unsqueeze(0), prediction_bbox.unsqueeze(0)).item()\n",
        "        ious.append(iou)\n",
        "\n",
        "        rectangle: tuple[int, int, int, int] = true_xyxy.tolist()\n",
        "        ground_truth_crop = img_pil.crop(rectangle)\n",
        "\n",
        "        rectangle: tuple[int, int, int, int] = torch.tensor(prediction_bbox, dtype=torch.int).tolist()\n",
        "        prediction_crop = img_pil.crop(rectangle)\n",
        "\n",
        "        # from float16 to float32\n",
        "        X: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "            clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).unsqueeze(0)),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "        Y: Float[torch.Tensor, '1'] = torch.tensor(\n",
        "            clip_model.encode_image(torch.tensor(preprocess(prediction_crop)).unsqueeze(0)),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "\n",
        "        cos: float = F.cosine_similarity(X, Y).item()\n",
        "        coss.append(cos)\n",
        "\n",
        "        eud: float = torch.cdist(X, Y, p=2).item()\n",
        "        euds.append(eud)\n",
        "\n",
        "        # store the prediction\n",
        "        pred : Prediction\n",
        "        pred.image = img_pil\n",
        "        pred.description = prompts\n",
        "        pred.ground_truth_bbox = true_xyxy\n",
        "        pred.output_bbox = prediction_bbox\n",
        "        stored_predictions.append(pred)\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uhBHV1tOsNnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stored_predictions)"
      ],
      "metadata": {
        "id": "0MjpvuYuwOqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of image encoder"
      ],
      "metadata": {
        "id": "7gkTu_CPiODe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of text encoder"
      ],
      "metadata": {
        "id": "OWZqXoCZiSvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layers on top of image encoder and on top of text encoder"
      ],
      "metadata": {
        "id": "QcyLAD4tiWmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bottleneck"
      ],
      "metadata": {
        "id": "BiuUlbSSiaxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYB7nWO8huJZ"
      },
      "outputs": [],
      "source": []
    }
  ]
}