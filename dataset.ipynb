{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HflScLT-f2d"
      },
      "source": [
        "# Reading the  RefCOCOg dataset\n",
        "The visual grounding task in this assignment will employ the RefCOCOg dataset, a variant of the Referring Expression Generation (REG) dataset.\n",
        "The RefCOCOg dataset consists of approximately 25,799\n",
        "images, each of which contains an average of 3.7 referring expressions. As with RefCOCO+, location words\n",
        "are not allowed in the referring expressions, which contains only appearance-based descriptions that are\n",
        "independent of viewer perspective. This makes the dataset well-suited for visual grounding tasks, as it\n",
        "necessitates creating a mapping between the visual appearance of an object and its corresponding linguistic\n",
        "label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0YzHFtv_mox"
      },
      "source": [
        "To access and download the dataset, you can use the [Google Drive link provided](https://drive.google.com/uc?id=1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb). Please note that in the annotations folder, there are two available refer files in the pickle format (ref(google).p and ref(umd).p). For this exercise, we will use the second split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sakTg9LGAAE0"
      },
      "source": [
        "Actually, there is no predefined torchvision dataset class appropriate for the visual grounding task. As as a result in this notebook we are going to create a custom dataset class to load and read the dataset correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugwhe8C3reJ5"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "#!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq <-- questa era la risorsa condivisa dal prof\n",
        "!gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "!ls dataset\n",
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect what's in our data directory using the in-built os.walk() to walk through each of the subdirectories and count the files present."
      ],
      "metadata": {
        "id": "jYVUwVq4aPkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "data_path = Path(\"dataset/refcocog/\")\n",
        "for dirpath, dirnames, filenames in os.walk(data_path):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} files in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "-e9YgNDGaaWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joJgXG2ahmKm"
      },
      "source": [
        "# JSON Parsing\n",
        "It is impossible to find something useful online about the RefCOCOg dataset. Moreover, the files are too larged to be inspected with a text editor. Hence, with the following code we are going to explore the structure of instances.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDda4pfGnht1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "data_path = Path(\"dataset/refcocog/\")\n",
        "\n",
        "# Opening JSON file\n",
        "f = open(data_path/\"annotations/instances.json\")\n",
        "  \n",
        "# returns JSON object as a dictionary\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz_kn-JUnfl0"
      },
      "source": [
        "## First level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGxTQm-Rho8v"
      },
      "outputs": [],
      "source": [
        "# Iterating through the json list\n",
        "print(\"FIRST LEVEL\")\n",
        "for i in data:\n",
        "    print(i)\n",
        "print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYLaQXZnnUbE"
      },
      "source": [
        "## INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSdRfKL6nWqx"
      },
      "outputs": [],
      "source": [
        "print(\"INFO\")\n",
        "for i in data[\"info\"]:\n",
        "    print(i)\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - DESCRIPTION\")\n",
        "print(data[\"info\"][\"description\"])\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - URL\")\n",
        "print(data[\"info\"][\"url\"])\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - VERSION\")\n",
        "print(data[\"info\"][\"version\"])\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - YEAR\")\n",
        "print(data[\"info\"][\"year\"])\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - CONTRIBUTOR\")\n",
        "print(data[\"info\"][\"contributor\"])\n",
        "print(\"==============\")\n",
        "\n",
        "print(\"INFO - DATA_CREATED\")\n",
        "print(data[\"info\"][\"date_created\"])\n",
        "print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIVUrM_BnqWN"
      },
      "source": [
        "## IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNxeCTX0n0Zy"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow #this module is required otherwise cv2.imshow() is disabled in Colab, because it causes Jupyter sessions to crash\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"IMAGES\")\n",
        "print(\"data['images'].len : \"+str(len(data[\"images\"]))) #25799 (train 21899 + val 1300 + test 2600)\n",
        "\n",
        "print(\"Sample\") #get only three random examples since the whole dataset is too big\n",
        "\n",
        "print(data[\"images\"][0])\n",
        "cv_image = cv.imread(\"dataset/refcocog/images/\"+data[\"images\"][0][\"file_name\"])\n",
        "cv2_imshow(cv_image);\n",
        "\n",
        "print(data[\"images\"][1])\n",
        "cv_image = cv.imread(\"dataset/refcocog/images/\"+data[\"images\"][1][\"file_name\"])\n",
        "cv2_imshow(cv_image);\n",
        "\n",
        "print(data[\"images\"][2])\n",
        "cv_image = cv.imread(\"dataset/refcocog/images/\"+data[\"images\"][2][\"file_name\"])\n",
        "cv2_imshow(cv_image);\n",
        "\n",
        "print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoM9OTjnsUd"
      },
      "source": [
        "## LICENSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1GmfrCjn0-X"
      },
      "outputs": [],
      "source": [
        "print(\"LICENSES\")\n",
        "for i in data[\"licenses\"]:\n",
        "    print(i)\n",
        "print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ZbDYvfnutk"
      },
      "source": [
        "## ANNOTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2vDHZ05n1jx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "print(\"ANNOTATIONS\")\n",
        "print(\"data['annotations'].len : \"+str(len(data[\"annotations\"]))) #208960\n",
        "\n",
        "print(\"Sample\") #get only three random examples since the whole dataset is too big\n",
        "\n",
        "#bbox: [xmin, ymin, width, height]\n",
        "\n",
        "for j in range(3):\n",
        "  random_number = random.randint(1, 20000)\n",
        "  print(\"data['annotations'][\"+str(random_number)+\"]\")\n",
        "  for i in data[\"annotations\"][random_number]:\n",
        "      print(i)\n",
        "      print(data[\"annotations\"][random_number][i])\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)."
      ],
      "metadata": {
        "id": "bBWPD_wyzf81"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18mLZNX6nxJZ"
      },
      "source": [
        "## CATEGORIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmMEmLR4n16t"
      },
      "outputs": [],
      "source": [
        "print(\"CATEGORIES\")\n",
        "for i in data[\"categories\"]:\n",
        "    print(i)\n",
        "print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the annotations of a random image in the dataset"
      ],
      "metadata": {
        "id": "g8L_RzZN1Eww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow #this module is required otherwise cv2.imshow() is disabled in Colab, because it causes Jupyter sessions to crash\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random_number = random.randint(1, 20000)\n",
        "image_id = data[\"images\"][random_number][\"id\"]\n",
        "image_filename = data[\"images\"][random_number][\"file_name\"]\n",
        "print(\"image number: \"+str(random_number))\n",
        "print(\"image id: \"+str(image_id))\n",
        "print(image_filename)\n",
        "\n",
        "cv_image = cv.imread(\"dataset/refcocog/images/\"+image_filename)\n",
        "cv2_imshow(cv_image);"
      ],
      "metadata": {
        "id": "wQVhLduB1JZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get annotations about this image\n",
        "#and store the bounding boxes\n",
        "bbox_list = list()\n",
        "for ann in data[\"annotations\"]:\n",
        "    if ann['image_id']==image_id:\n",
        "      print(\"ann_id: \"+str(ann[\"id\"]))\n",
        "      #for i in ann:\n",
        "        #print(i)\n",
        "        #print(ann[i])\n",
        "      bbox_list.append(ann[\"bbox\"])\n",
        "\n",
        "for i in range(len(bbox_list)):\n",
        "  bbox = bbox_list[i] #bbox: [xmin, ymin, width, height]\n",
        "\n",
        "  color = (random.randint(0,256), random.randint(0,256), random.randint(0,256)) #select a random bbox color\n",
        "\n",
        "  cv.rectangle(cv_image, (int(bbox[0]), int(bbox[1])), (int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3])), color, 2)\n",
        "\n",
        "cv2_imshow(cv_image);"
      ],
      "metadata": {
        "id": "-IGiKse52MYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `cv::rectangle` draws a rectangle outline or a filled rectangle whose two opposite corners are pt1 and pt2.\n",
        "\n",
        "**Parameters**\n",
        "* img\tImage.\n",
        "* pt1\tVertex of the rectangle.\n",
        "* pt2\tVertex of the rectangle opposite to pt1 .\n",
        "* color\tRectangle color or brightness (grayscale image).\n",
        "* thickness\tThickness of lines that make up the rectangle. Negative values, like FILLED, mean that the function has to draw a filled rectangle.\n",
        "* lineType\tType of the line. See LineTypes\n",
        "* shift\tNumber of fractional bits in the point coordinates.\n"
      ],
      "metadata": {
        "id": "8Iv5WruK4o_b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo9MR7ALnbRy"
      },
      "source": [
        "## Close file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae7L3oomnaG2"
      },
      "outputs": [],
      "source": [
        "# Closing file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the image natural language descriptions\n",
        "Natural language annotations of the images are stored in `/dataset/refcocog/annotations/refs(umd).p`"
      ],
      "metadata": {
        "id": "MVLyaFNXSekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "annotationRoot = \"dataset/refcocog/annotations/\"\n",
        "pickleFile = open(annotationRoot+\"refs(umd).p\", \"rb\") #open a file, where you stored the pickled data\n",
        "\n",
        "# dump information to that file\n",
        "data = pickle.load(pickleFile)\n",
        "\n",
        "print(len(data))  #49822\n",
        "\n",
        "print(\"Three random objects\")\n",
        "print(\"\")\n",
        "for i in range(3):\n",
        "  random_number = random.randint(1, len(data))\n",
        "  print(\"complete object: \" + str(data[random_number]))\n",
        "  for j in data[random_number]: #explore each field\n",
        "    if j==\"sentences\":  #print sentences in a suitable way\n",
        "      print(\"sentences[\"+str(len(data[random_number][\"sentences\"]))+\"]:\")\n",
        "      for k in data[random_number][\"sentences\"]:\n",
        "        for sentence_element in k:\n",
        "          print(sentence_element+\": \"+str(k[sentence_element]))\n",
        "        print(\".\")\n",
        "    else:\n",
        "      print(j+\": \"+str(data[random_number][j]))\n",
        "  print(\"\")\n",
        "  print(\"---\")\n",
        "  print(\"\")\n",
        "\n",
        "# close the file\n",
        "pickleFile.close()"
      ],
      "metadata": {
        "id": "nFLdO2u2W_MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get description of the target image."
      ],
      "metadata": {
        "id": "cUIlvLwk2vxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for re in data:\n",
        "  if re[\"image_id\"] == image_id:\n",
        "    print(re[\"file_name\"])\n",
        "    for sentence in re[\"sentences\"]:\n",
        "      print(sentence[\"sent\"])"
      ],
      "metadata": {
        "id": "b3PeaTll2ynT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RvQFP37hg79"
      },
      "source": [
        "# Dataset example: FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcr6HnwsHtFb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj5QGIQMIYgI"
      },
      "outputs": [],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w551cR5zK6Kq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1hJsBvNK1y2"
      },
      "outputs": [],
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SOfXJzTi73c"
      },
      "source": [
        "# Create custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#annotations_file: refcocog/annotations/instances.json\n",
        "#img_dir: refcocog/images\n",
        "#reference_exp_file: refcocog/annotations/refs(umd).p\n",
        "\n",
        "img_dir = Path(\"dataset/refcocog/images/\")\n",
        "annotations_file = Path(\"dataset/refcocog/annotations/instances.json\")\n",
        "reference_exp_file = Path(\"dataset/refcocog/annotations/refs(umd).p\")\n",
        "\n",
        "f = open(annotations_file)\n",
        "annotations_json = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ktGnOGpnntSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_id2index = {\n",
        "    ann['id']: index\n",
        "    for index, ann in enumerate(annotations_json[\"annotations\"])\n",
        "}"
      ],
      "metadata": {
        "id": "2PMxnXaiqSG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1nVFHBWi-0M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List\n",
        "import pickle\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "\n",
        "    #split: train, test or val\n",
        "    #img_transform: apply list of transformations on the processed images\n",
        "    #exp_transform: apply list of transformations on the processed reference expressions\n",
        "    #target_transform: apply list of transformations on the bounding box\n",
        "    #limit: number of dataset elements which we want to consider\n",
        "    def __init__(\n",
        "        self,\n",
        "        split,\n",
        "        img_transform=None,\n",
        "        exp_transform=None,\n",
        "        target_transform=None,\n",
        "        limit=None\n",
        "    ):\n",
        "        self.img_transform = img_transform\n",
        "        self.exp_transform = exp_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        # Internally the dataset is a list of couples (X,Y)\n",
        "        # X: index of the reference expression object in refs(umd).p\n",
        "        # Y: index of the annotation object in instances.json\n",
        "        self.items = self.load_dataset_index(split, limit)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    #return ((image, sentences), bounding box)\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.getImage(idx)\n",
        "        bbox = self.getBoundingBox(idx)\n",
        "        sentences = self.getSentences(idx)\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.exp_transform:\n",
        "            for s in sentences:\n",
        "              s = self.exp_transform(s)\n",
        "\n",
        "        if self.target_transform:\n",
        "            bbox = self.target_transform(bbox)\n",
        "\n",
        "        return (image, sentences), bbox\n",
        "\n",
        "    #according to the split [train, test, val]\n",
        "    #return a list of couples (X,Y) such that:\n",
        "    # X: index of the reference expression object in refs(umd).p\n",
        "    # Y: index of the annotation object in instances.json\n",
        "    def load_dataset_index(self, split, limit):\n",
        "\n",
        "      pickleFile = open(reference_exp_file, \"rb\")  # open the file, where you stored the pickled data\n",
        "      ref_exp_data = pickle.load(pickleFile)       # dump information from that file\n",
        "      pickleFile.close()                           # close the file\n",
        "\n",
        "      #iterate over all the reference expressions related to the\n",
        "      #the target split\n",
        "      return [\n",
        "          (ref_index, ann_id2index[ref['ann_id']])\n",
        "          for ref_index, ref in enumerate(ref_exp_data)\n",
        "          if ref['split'] == split\n",
        "      ]\n",
        "\n",
        "\n",
        "    def getImage(self, idx):\n",
        "      pickleFile = open(reference_exp_file, \"rb\")  #open the file, where you stored the pickled data\n",
        "      ref_exp_data = pickle.load(pickleFile)            #dump information from that file\n",
        "\n",
        "      obj_index = self.items[idx][0]\n",
        "\n",
        "      file_name = ref_exp_data[obj_index][\"file_name\"]\n",
        "      # In refs(umd).p the image file name is stored with the following\n",
        "      # format:\n",
        "      #   - COCO_train2014_[image_id]_[annotation_id].jpg\n",
        "      #   - Example: `COCO_train2014_000000130518_104426.jpg`\n",
        "      # However in the folder images there is not a distinct image\n",
        "      # file for each annotation. As a consequence the files in\n",
        "      # images folder are characterized by the following structure:\n",
        "      #   - COCO_train2014_[image_id].jpg\n",
        "      # so we have to split the file_name to remove the last part\n",
        "      # about the annotation id\n",
        "      spl_filename = file_name.split(\"_\")\n",
        "      file_name = '_'.join(spl_filename[:-1])+\".jpg\"\n",
        "\n",
        "      image = read_image(os.path.join(img_dir,file_name))\n",
        "\n",
        "      pickleFile.close()\n",
        "\n",
        "      return image\n",
        "\n",
        "    def getBoundingBox(self, idx):\n",
        "      f = open(annotations_file) #open JSON file\n",
        "      instances_data = json.load(f) # returns JSON object as a dictionary\n",
        "\n",
        "      obj_index = self.items[idx][1]\n",
        "      bbox = instances_data[\"annotations\"][obj_index][\"bbox\"]\n",
        "\n",
        "      f.close()\n",
        "\n",
        "      return bbox\n",
        "\n",
        "\n",
        "    def getSentences(self, idx):\n",
        "      pickleFile = open(reference_exp_file, \"rb\")  #open the file, where you stored the pickled data\n",
        "      ref_exp_data = pickle.load(pickleFile)            #dump information to that file\n",
        "\n",
        "      obj_index = self.items[idx][0]\n",
        "\n",
        "      sentences = ref_exp_data[obj_index][\"sentences\"]\n",
        "\n",
        "      pickleFile.close()\n",
        "\n",
        "      return sentences\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following we experiment some interactions with the custom dataset"
      ],
      "metadata": {
        "id": "fA8nXTmQe1u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "mUKDYYiBgCRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment train data (https://www.learnpytorch.io/04_pytorch_custom_datasets/)\n",
        "train_transforms = transforms.Compose([\n",
        "    #transforms.Resize((64, 64)),\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Don't augment test data, only reshape\n",
        "test_transforms = transforms.Compose([\n",
        "    #transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "u56U6oEVfB7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom = CocoDataset(\n",
        "    split=\"train\",\n",
        "    img_transform=None,\n",
        "    exp_transform=None,\n",
        "    target_transform=None,\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "test_data_custom = CocoDataset(\n",
        "    split=\"test\",\n",
        "    img_transform=None,\n",
        "    exp_transform=None,\n",
        "    target_transform=None,\n",
        "    limit=3\n",
        ")"
      ],
      "metadata": {
        "id": "0bjQkTnifMuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display 2 random dataset instances"
      ],
      "metadata": {
        "id": "043dPG_ihtrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset):\n",
        "    \n",
        "    # Get random sample indexes\n",
        "    random_samples_idx = random.sample(range(len(dataset)), k=2)\n",
        "\n",
        "    # Setup plot\n",
        "    plt.figure(figsize=(50, 50))\n",
        "\n",
        "    # Loop through samples and display random samples \n",
        "    for i, targ_sample in enumerate(random_samples_idx):\n",
        "        input, output = dataset[targ_sample]\n",
        "\n",
        "        img = input[0]\n",
        "        sentences = input[1]\n",
        "        bbox = output #[xmin, ymin, width, height]\n",
        "\n",
        "        # add bounding box\n",
        "        # Parameters\n",
        "        # image: image of type Tensor of shape (C x H x W).\n",
        "        # boxes: tensor of size [N,4] containing bounding boxes coordinates in (xmin, ymin, xmax, ymax) format. N is the number of bounding boxes\n",
        "        # ...it also accepts more optional parameters such as labels, colors, fill, width, etc.\n",
        "  \n",
        "        color = (random.randint(0,256), random.randint(0,256), random.randint(0,256)) #select a random bbox color\n",
        "\n",
        "        # convert bbox from (xmin, ymin, width, height) format\n",
        "        # to (xmin, ymin, xmax, ymax) format\n",
        "        bbox[2] += bbox[0]\n",
        "        bbox[3] += bbox[1]\n",
        "\n",
        "        #avoid deprecation warning\n",
        "        for b in range(4):\n",
        "          bbox[b] = int(bbox[b])\n",
        "\n",
        "        bbox = torch.tensor(bbox, dtype=torch.int)\n",
        "        #print(bbox)\n",
        "        #print(bbox.size()) #[4]\n",
        "        bbox = bbox.unsqueeze(0)\n",
        "        #print(bbox.size()) #[1,4]\n",
        "\n",
        "        # draw bounding box on the input image\n",
        "        img=draw_bounding_boxes(img, bbox, width=3, colors=color)\n",
        "\n",
        "        # Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "        targ_image_adjust = img.permute(1, 2, 0)\n",
        "\n",
        "        # Plot adjusted samples\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(targ_image_adjust)\n",
        "        plt.axis(\"off\")\n",
        "        title = sentences[0]['raw']\n",
        "        plt.title(title)\n",
        "\n",
        "display_random_images(train_data_custom)"
      ],
      "metadata": {
        "id": "1t8dyXjzhvpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement dataloaders"
      ],
      "metadata": {
        "id": "MJ16WKrYlb14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader_custom = DataLoader(\n",
        "    dataset=train_data_custom, # use custom created train Dataset\n",
        "    batch_size=1, # how many samples per batch?\n",
        "    num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "    shuffle=True, # shuffle the data?\n",
        ") \n",
        "\n",
        "test_dataloader_custom = DataLoader(\n",
        "    dataset=test_data_custom, # use custom created test Dataset\n",
        "    batch_size=1,\n",
        "    num_workers=0,\n",
        "    shuffle=False, # usually there is no need to shuffle testing data\n",
        ")\n",
        "\n",
        "train_dataloader_custom, test_dataloader_custom"
      ],
      "metadata": {
        "id": "JmLfhSRFlikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image and label from custom DataLoader\n",
        "input, output = next(iter(train_dataloader_custom))\n",
        "\n",
        "# input is a list of two elements\n",
        "# input[0] is the image torch.Size([1, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
        "# input[1] are the sentences\n",
        "\n",
        "# output is a list of tensors\n",
        "# [tensor([0.], dtype=torch.float64), tensor([45.9500], dtype=torch.float64), tensor([238.9200], dtype=torch.float64), tensor([408.6400], dtype=torch.float64)]\n",
        "# [xmin, ymin, width, height]\n",
        "\n",
        "\n",
        "# Setup plot\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "img = input[0][0]\n",
        "sentences = input[1]\n",
        "\n",
        "#conver output in a list of int\n",
        "bbox = list()\n",
        "for b in range(4):\n",
        "  bbox.append(int(output[b].item()))\n",
        "\n",
        "# add bounding box\n",
        "# Parameters\n",
        "# image: image of type Tensor of shape (C x H x W).\n",
        "# boxes: tensor of size [N,4] containing bounding boxes coordinates in (xmin, ymin, xmax, ymax) format. N is the number of bounding boxes\n",
        "# ...it also accepts more optional parameters such as labels, colors, fill, width, etc.\n",
        "\n",
        "color = (random.randint(0,256), random.randint(0,256), random.randint(0,256)) #select a random bbox color\n",
        "\n",
        "# convert bbox from (xmin, ymin, width, height) format\n",
        "# to (xmin, ymin, xmax, ymax) format\n",
        "bbox[2] += bbox[0]\n",
        "bbox[3] += bbox[1]\n",
        "\n",
        "bbox = torch.tensor(bbox, dtype=torch.int)\n",
        "bbox = bbox.unsqueeze(0)\n",
        "\n",
        "# draw bounding box on the input image\n",
        "img=draw_bounding_boxes(img, bbox, width=3, colors=color)\n",
        "\n",
        "# Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "targ_image_adjust = img.permute(1, 2, 0)\n",
        "\n",
        "\n",
        "# Plot adjusted samples\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.imshow(targ_image_adjust)\n",
        "plt.axis(\"off\")\n",
        "title = sentences[0]['raw'][0]\n",
        "plt.title(title)"
      ],
      "metadata": {
        "id": "cWgx_tJrlsTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "joJgXG2ahmKm",
        "MVLyaFNXSekC",
        "5RvQFP37hg79"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}