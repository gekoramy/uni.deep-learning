{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from typing import Literal, Callable, Tuple, TypedDict\n",
    "from jaxtyping import Array, Float, UInt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&confirm=t&id=1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\" -O \"dataset/refcocog.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf dataset/refcocog.tar.gz -C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dataset/refcocog.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split = Literal['train', 'test', 'val']\n",
    "\n",
    "\n",
    "class Info(TypedDict, total=True):\n",
    "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
    "    url: str  # http://mscoco.org/\n",
    "    version: str  # 1.0\n",
    "    year: int  # 2014\n",
    "    contributor: str  # Microsoft COCO group\n",
    "    data_created: str  # 2015-01-27 09:11:52.357475\n",
    "\n",
    "\n",
    "class Image(TypedDict, total=True):\n",
    "    license: int  # each image has an associated licence id\n",
    "    file_name: str  # file name of the image\n",
    "    coco_url: str  # example http://mscoco.org/images/131074\n",
    "    height: int\n",
    "    width: int\n",
    "    data_captured: str  # example '2013-11-21 01:03:06'\n",
    "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
    "    id: int  # id of the image\n",
    "\n",
    "\n",
    "class License(TypedDict, total=True):\n",
    "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
    "    id: int  # id of the licence\n",
    "    name: str  # example 'Attribution-NonCommercial-ShareAlike License'\n",
    "\n",
    "\n",
    "class Annotation(TypedDict, total=True):\n",
    "    segmentation: str  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
    "    area: int  # number of pixel of the described object\n",
    "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
    "    image_id: int  # id of the target image\n",
    "    bbox: UInt[Array, '4']  # bounding box coordinates [xmin, ymin, width, height]\n",
    "    category_id: int\n",
    "    id: int  # annotation id\n",
    "\n",
    "\n",
    "class Category(TypedDict, total=True):\n",
    "    supercategory: str  # example 'vehicle'\n",
    "    id: int  # category id\n",
    "    name: str  # example 'airplane'\n",
    "\n",
    "\n",
    "class Instances(TypedDict, total=True):\n",
    "    info: Info\n",
    "    images: list[Image]\n",
    "    licenses: list[License]\n",
    "    annotations: list[Annotation]\n",
    "    categories: list[Category]\n",
    "\n",
    "\n",
    "class Sentence(TypedDict, total=True):\n",
    "    tokens: list[str]  # tokenized version of referring expression\n",
    "    raw: str  # unprocessed referring expression\n",
    "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
    "    sent_it: int  # unique referring expression id\n",
    "\n",
    "\n",
    "class Ref(TypedDict, total=True):\n",
    "    image_id: int  # unique image id\n",
    "    split: Split\n",
    "    sentences: list[Sentence]\n",
    "    file_name: str  # file name of image relative to img_root\n",
    "    category_id: int  # object category label\n",
    "    ann_id: int  # id of object annotation in instance.json\n",
    "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
    "    ref_id: int  # unique id for refering expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ref(x: Ref) -> Ref:\n",
    "    x['file_name'] = fix_filename(x['file_name'])\n",
    "    return x\n",
    "\n",
    "\n",
    "def fix_filename(x: str) -> str:\n",
    "    \"\"\"\n",
    "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
    "    :return:  COCO_..._[image_id].jpg\n",
    "    \"\"\"\n",
    "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset/refcocog/annotations/refs(umd).p', 'rb')\n",
    "refs: list[Ref] = [\n",
    "    fix_ref(ref)\n",
    "    for ref in pickle.load(f)\n",
    "]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset/refcocog/annotations/instances.json', 'r')\n",
    "instances: Instances = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2annotation = {\n",
    "    x['id']: x\n",
    "    for x in instances['annotations']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = UInt[torch.Tensor, 'R G B']\n",
    "P = Sentence\n",
    "B = Float[torch.Tensor, '4']\n",
    "\n",
    "\n",
    "class CocoDataset(Dataset[Tuple[Tuple[I, str], B]]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            split: Split,\n",
    "            img_transform: Callable[[I], I] = lambda x: x,\n",
    "            prompt_transform: Callable[[list[P]], str] = lambda x: x[0]['sent'],\n",
    "            bb_transform: Callable[[B], B] = lambda x: x,\n",
    "    ):\n",
    "        self.img_transform = img_transform\n",
    "        self.prompt_transform = prompt_transform\n",
    "        self.bb_transform = bb_transform\n",
    "\n",
    "        self.items: list[tuple[tuple[str, list[P]], B]] = [\n",
    "            ((i, ps), o)\n",
    "            for ref in refs\n",
    "            if ref['split'] == split\n",
    "            for i in [ref['file_name']]\n",
    "            for ps in [ref['sentences']]\n",
    "            for o in [torch.tensor(id2annotation[ref['ann_id']]['bbox'])]\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[Tuple[I, str], B]:\n",
    "        ((i, ps), b) = self.items[item]\n",
    "        img = read_image('dataset/refcocog/images/' + i)\n",
    "        return (\n",
    "            (\n",
    "                self.img_transform(img),\n",
    "                self.prompt_transform(ps)\n",
    "            ),\n",
    "            self.bb_transform(b)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_h = max([ img['height'] for img in instances['images'] ])\n",
    "max_w = max([ img['width'] for img in instances['images'] ])\n",
    "    \n",
    "def padding(x: I) -> I:\n",
    "    z = torch.zeros((3, max_w, max_h), dtype=torch.uint8)\n",
    "    c, w, h = x.size()\n",
    "    z[0:c, 0:w, 0:h] = x\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest(ps: list[P]) -> str:\n",
    "    return min(map(lambda p: p['sent'], ps), key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_custom = DataLoader(\n",
    "    dataset=CocoDataset(\n",
    "        split='train',\n",
    "        img_transform=padding,\n",
    "        prompt_transform=shortest\n",
    "    ),  # use custom created train Dataset\n",
    "    batch_size=10,  # how many samples per batch?\n",
    "    shuffle=True,  # shuffle the data?\n",
    ")\n",
    "\n",
    "test_dataloader_custom = DataLoader(\n",
    "    dataset=CocoDataset(split='test'),  # use custom created test Dataset\n",
    "    batch_size=1,\n",
    "    shuffle=False,  # usually there is no need to shuffle testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(25, 10))\n",
    "\n",
    "(imgs, prompts), outputs = next(iter(train_dataloader_custom))\n",
    "\n",
    "bboxes = torchvision.ops.box_convert(outputs, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "for ax, img, prompt, bbox in zip(it.chain.from_iterable(axs), imgs, prompts, bboxes):\n",
    "    r, g, b = torch.randint(0, 256, [3]).tolist()\n",
    "    img_bbox = torchvision.utils.draw_bounding_boxes(\n",
    "        image=img,\n",
    "        boxes=bbox.unsqueeze(0),\n",
    "        colors=(r, g, b),\n",
    "        width=2,\n",
    "    )\n",
    "    ax.imshow(img_bbox.permute(1, 2, 0))\n",
    "    ax.set_title(prompt)\n",
    "    ax.set_axis_off()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
