{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEKfKE3-dU68"
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "In this lab session we are going to familiarize with the usage of *Batch Normalization (BN) layers* in our networks. BN is designed in order to **standardize** each feature within a mini-batch, in such a way as to have 0 mean and unit variance. It then scales and shifts the standardized activations with learnable parameters. BN is known for \n",
    "\n",
    "*   faster convergence properties\n",
    "*   improved performance\n",
    "\n",
    "More details can be found in the original [paper](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "$BN(x_{i, k}) = \\gamma_{k} \\frac{x_{i, k} - \\mu_{B, k}}{\\sqrt{\\sigma^{2}_{B,k} + \\epsilon}} + \\beta_{k}$\n",
    "\n",
    "The intuitive idea behind BN is as follows: a neural network is trained using mini-batches, and the distribution of inputs **varies** from one batch to the other. Difference in distributions between mini-batches can cause the training to be **unstable** and heavily **dependant on the initial weights** of the network. Therefore, this kind of transformation (transforming the inputs to have mean 0 and unit variance) guarantees that input distribution of each layer remains **unchanged across mini-batches**.\n",
    "\n",
    "More interestingly, we will learn how to code BN layer from scratch using PyTorch. Let's start by importing the necessary libraries, as usual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5chmLmKyfFVg"
   },
   "source": [
    "## BatchNorm1D\n",
    "This is the implementation of batch normalization for fully connected hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
    "\n",
    "Shape:\n",
    "  Input: (N, C)\n",
    "  Output: (N, C)\n",
    "\n",
    "Input Parameters:\n",
    "  in_features: number of features of the input activations\n",
    "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
    "  affine: whether to scale and shift the normalized activations. (default: True)\n",
    "  momentum: the momentum value for the moving average. (default: 0.9)\n",
    "\n",
    "Usage:\n",
    "  >>> # with learable parameters\n",
    "  >>> bn = BatchNorm1d(4)\n",
    "  >>> # without learable parameters\n",
    "  >>> bn = BatchNorm1d(4, affine=False)\n",
    "  >>> input = torch.rand(10, 4)\n",
    "  >>> out = bn(input)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BatchNorm1d(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, track_running_stats=True, affine=True, momentum=0.9\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.track_running_stats = track_running_stats\n",
    "        self.affine = affine\n",
    "        self.momentum = momentum\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
    "            # but which does not require to be trained, differently from nn.Parameter\n",
    "            self.register_buffer(\"running_mean\", torch.zeros(self.in_features, 1))\n",
    "            self.register_buffer(\"running_std\", torch.ones(self.in_features, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transpose (N, C) to (C, N)\n",
    "        x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
    "\n",
    "        # calculate batch mean\n",
    "        mean = x.mean(dim=1).view(-1, 1)\n",
    "\n",
    "        # calculate batch std\n",
    "        std = x.std(dim=1).view(-1, 1)\n",
    "\n",
    "        # during training keep running statistics (moving average of mean and std)\n",
    "        if self.training and self.track_running_stats:\n",
    "            # no computational graph is necessary to be built for this computation\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (\n",
    "                    self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "                )\n",
    "                self.running_std = (\n",
    "                    self.momentum * self.running_std + (1 - self.momentum) * std\n",
    "                )\n",
    "\n",
    "        # during inference time\n",
    "        if not self.training and self.track_running_stats:\n",
    "            mean = self.running_mean\n",
    "            std = self.running_std\n",
    "\n",
    "        # normalize the input activations\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        # scale and shift the normalized activations\n",
    "        if self.affine:\n",
    "            x = x * self.gamma + self.beta\n",
    "\n",
    "        return x.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiXDPKD2fUTB"
   },
   "source": [
    "## BatchNorm2D\n",
    "BN module for convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
    "\n",
    "Shape:\n",
    "  Input: (N, C, H, W)\n",
    "  Output: (N, C, H, W)\n",
    "\n",
    "Input Parameters:\n",
    "  in_features: number of features of the input activations\n",
    "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
    "  affine: whether to scale and shift the normalized activations. (default: True)\n",
    "  momentum: the momentum value for the moving average. (default: 0.9)\n",
    "\n",
    "Usage:\n",
    "  >>> # with learable parameters\n",
    "  >>> bn = BatchNorm2d(4)\n",
    "  >>> # without learable parameters\n",
    "  >>> bn = BatchNorm2d(4, affine=False)\n",
    "  >>> input = torch.rand(10, 4, 5, 5)\n",
    "  >>> out = bn(input)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BatchNorm2d(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, track_running_stats=True, affine=True, momentum=0.9\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.track_running_stats = track_running_stats\n",
    "        self.affine = affine\n",
    "        self.momentum = momentum\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
    "            # but which does not require to be trained, differently from nn.Parameter\n",
    "            self.register_buffer(\"running_mean\", torch.zeros(self.in_features, 1))\n",
    "            self.register_buffer(\"running_std\", torch.ones(self.in_features, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transpose (N, C, H, W) to (C, N, H, W)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # store the shape\n",
    "        c, bs, h, w = x.shape\n",
    "\n",
    "        # collapse all dimensions except the 'channel' dimension\n",
    "        x = x.contiguous().view(c, -1)\n",
    "\n",
    "        # calculate batch mean\n",
    "        mean = x.mean(dim=1).view(-1, 1)\n",
    "\n",
    "        # calculate batch std\n",
    "        std = x.std(dim=1).view(-1, 1)\n",
    "\n",
    "        # keep running statistics (moving average of mean and std)\n",
    "        if self.training and self.track_running_stats:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (\n",
    "                    self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "                )\n",
    "                self.running_std = (\n",
    "                    self.momentum * self.running_std + (1 - self.momentum) * std\n",
    "                )\n",
    "\n",
    "        # during inference time\n",
    "        if not self.training and self.track_running_stats:\n",
    "            mean = self.running_mean\n",
    "            std = self.running_std\n",
    "\n",
    "        # normalize the input activations\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        # scale and shift the normalized activations\n",
    "        if self.affine:\n",
    "            x = x * self.gamma + self.beta\n",
    "\n",
    "        return x.view(c, bs, h, w).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU0Ua9jWfjQ3"
   },
   "source": [
    "## LeNet-5 with bach normalization\n",
    "Here we will use BN layers for the LeNet-5 network. These layers are added right after the convolutional and fully connected layers, except the output ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self, norm=False):\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "        # input channel = 3, output channels = 6, kernel size = 5\n",
    "        # input image size = (32, 32), image output size = (28, 28)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5))\n",
    "        if self.norm:\n",
    "            self.bn1 = BatchNorm2d(6)\n",
    "\n",
    "        # input channel = 6, output channels = 16, kernel size = 5\n",
    "        # input image size = (14, 14), output image size = (10, 10)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
    "        if self.norm:\n",
    "            self.bn2 = BatchNorm2d(16)\n",
    "\n",
    "        # input dim = 5 * 5 * 16 ( H x W x C), output dim = 120\n",
    "        self.fc3 = torch.nn.Linear(in_features=5 * 5 * 16, out_features=120)\n",
    "        if self.norm:\n",
    "            self.bn3 = BatchNorm1d(120)\n",
    "\n",
    "        # input dim = 120, output dim = 84\n",
    "        self.fc4 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        if self.norm:\n",
    "            self.bn4 = BatchNorm1d(84)\n",
    "\n",
    "        # input dim = 84, output dim = 10\n",
    "        self.fc5 = torch.nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.norm:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        # Max Pooling with kernel size = 2\n",
    "        # output size = (14, 14)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if self.norm:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        # Max Pooling with kernel size = 2\n",
    "        # output size = (5, 5)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # flatten the feature maps into a long vector\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        if self.norm:\n",
    "            x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        if self.norm:\n",
    "            x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA3zpFvfgDb3"
   },
   "source": [
    "## Cost function and optimizer\n",
    "Similarly to the standard cases seen so far, we will employ cross-entropy loss and a SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function\n",
    "\n",
    "\n",
    "def get_optimizer(net, lr, wd, momentum):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(), lr=lr, weight_decay=wd, momentum=momentum\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rhFva0agQZF"
   },
   "source": [
    "## Training and test steps\n",
    "Let's defined our loops for the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(net, data_loader, cost_function, device=\"cuda:0\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # strictly needed if network contains layers which behave differently between the training and test steps\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # apply the loss\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # print statistics\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += (\n",
    "                loss.item()\n",
    "            )  # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda:0\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # strictly needed if network contains layers which behave differently between the training and test steps\n",
    "    net.train()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # apply the loss\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print statistics\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdcnWcfWgrso"
   },
   "source": [
    "## Data loading\n",
    "Let's now defined our data loading utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, test_batch_size=256, dataset=\"mnist\"):\n",
    "    # prepare data transformations and then combine them sequentially\n",
    "    if dataset == \"mnist\":\n",
    "        transform = list()\n",
    "        transform.append(T.ToTensor())  # convert Numpy to Pytorch Tensor\n",
    "        transform.append(\n",
    "            T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), \"constant\", 0))\n",
    "        )  # pad zeros to make MNIST 32 x 32\n",
    "        transform.append(\n",
    "            T.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        )  # make MNIST RGB instead of grayscale\n",
    "        transform.append(\n",
    "            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        )  # normalize the Tensors between [-1, 1]\n",
    "        transform = T.Compose(transform)  # compose the above transformations into one\n",
    "    elif dataset == \"svhn\":\n",
    "        transform = list()\n",
    "        transform.append(T.ToTensor())  # convert Numpy to Pytorch Tensor\n",
    "        transform.append(\n",
    "            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        )  # normalize the Tensors between [-1, 1]\n",
    "        transform = T.Compose(transform)  # compose the above transformations into one\n",
    "\n",
    "    # prepare dataset\n",
    "    if dataset == \"mnist\":\n",
    "        full_training_data = torchvision.datasets.MNIST(\n",
    "            \"./data/mnist\", train=True, transform=transform, download=True\n",
    "        )\n",
    "        test_data = torchvision.datasets.MNIST(\n",
    "            \"./data/mnist\", train=False, transform=transform, download=True\n",
    "        )\n",
    "    elif dataset == \"svhn\":\n",
    "        full_training_data = torchvision.datasets.SVHN(\n",
    "            \"./data/svhn\", split=\"train\", transform=transform, download=True\n",
    "        )\n",
    "        test_data = torchvision.datasets.SVHN(\n",
    "            \"./data/svhn\", split=\"test\", transform=transform, download=True\n",
    "        )\n",
    "\n",
    "    # create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int(num_samples * 0.8 + 1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "\n",
    "    training_data, validation_data = torch.utils.data.random_split(\n",
    "        full_training_data, [training_samples, validation_samples]\n",
    "    )\n",
    "\n",
    "    # initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        training_data, batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_data, test_batch_size, shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGVJfMEjhHxQ"
   },
   "source": [
    "## Wrap it up\n",
    "Let's now put everything together into a training procedure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input arguments\n",
    "  batch_size: Size of a mini-batch\n",
    "  device: GPU where you want to train your network\n",
    "  weight_decay: Weight decay co-efficient for regularization of weights\n",
    "  momentum: Momentum for SGD optimizer\n",
    "  epochs: Number of epochs for training the network\n",
    "  visualization_name: name of the tensorboard folder\n",
    "  dataset: which dataset to train\n",
    "  norm: whether to use batch normalization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main(\n",
    "    batch_size=128,\n",
    "    device=\"cuda:0\",\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.000001,\n",
    "    momentum=0.9,\n",
    "    epochs=50,\n",
    "    visualization_name=\"mnist\",\n",
    "    dataset=\"mnist\",\n",
    "    norm=False,\n",
    "):\n",
    "    # creates a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{visualization_name}\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        batch_size=batch_size, test_batch_size=batch_size, dataset=dataset\n",
    "    )\n",
    "\n",
    "    net = LeNet(norm=norm).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    print(\n",
    "        \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "            train_loss, train_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "            val_loss, val_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\"\\tTest loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # add values to plots\n",
    "    writer.add_scalar(\"train/loss\", train_loss, 0)\n",
    "    writer.add_scalar(\"val/loss\", val_loss, 0)\n",
    "    writer.add_scalar(\"train/accuracy\", train_accuracy, 0)\n",
    "    writer.add_scalar(\"val/accuracy\", val_accuracy, 0)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function\n",
    "        )\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        print(\"Epoch: {:d}\".format(e + 1))\n",
    "        print(\n",
    "            \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "                train_loss, train_accuracy\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "                val_loss, val_accuracy\n",
    "            )\n",
    "        )\n",
    "        print(\"-----------------------------------------------------\")\n",
    "\n",
    "        # Add values to plots\n",
    "        writer.add_scalar(\"train/loss\", train_loss, e + 1)\n",
    "        writer.add_scalar(\"val/loss\", val_loss, e + 1)\n",
    "        writer.add_scalar(\"train/accuracy\", train_accuracy, e + 1)\n",
    "        writer.add_scalar(\"val/accuracy\", val_accuracy, e + 1)\n",
    "\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    print(\n",
    "        \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "            train_loss, train_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "            val_loss, val_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\"\\tTest loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zuJGmYJhfDO"
   },
   "source": [
    "## Run\n",
    "Let's make it happen! First on MNIST without BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(visualization_name=\"mnist\", dataset=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAO7eTOqmRiV"
   },
   "source": [
    "Now on MNIST with BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(visualization_name=\"mnist_bn\", dataset=\"mnist\", norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhAtYdTvo-rK"
   },
   "source": [
    "SVHN without BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(visualization_name=\"svhn_bn\", dataset=\"svhn\", norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnCNoGsupBWy"
   },
   "source": [
    "SVHN with BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(visualization_name=\"svhn_bn\", dataset=\"svhn\", norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
