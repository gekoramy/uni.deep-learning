{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVQuq23Q9Hyo"
   },
   "source": [
    "# My first Neural Network\n",
    "\n",
    "Today we will how to build, train and test a simple neural network with PyTorch. In particular, we will train a **multi layer perceptron (MLP)** for digit recognition on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLLNs9dy9pAM"
   },
   "source": [
    "As a first step, let's import the modules we need. The `torch` module contains all the tools we need to build ad train the network, whereas `torchvision` contains several Computer Vision oriented utilities, such as shortcuts to standard benchmarks \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykdTcp1M99JE"
   },
   "source": [
    "## Dataset and Dataloaders\n",
    "PyTorch provides useful utilities to efficiently load training, testing and evaluation data, namely the `Dataset` and `Dataloader` modules. The former implements all the functionalities needed to load the dataset in the desired format, while the latter provides the corresponding iteration utilities. PyTorch provides an [implemented Dataset](https://pytorch.org/vision/stable/datasets.html#mnist) for MNIST; for the Dataloader, we can use the [default implementation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, test_batch_size=256):\n",
    "    # convert the PIL images to Tensors\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # load data\n",
    "    full_training_data = torchvision.datasets.MNIST(\n",
    "        \"./data\", train=True, transform=transform, download=True\n",
    "    )\n",
    "    test_data = torchvision.datasets.MNIST(\n",
    "        \"./data\", train=False, transform=transform, download=True\n",
    "    )\n",
    "\n",
    "    # create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int(num_samples * 0.5 + 1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "\n",
    "    training_data, validation_data = torch.utils.data.random_split(\n",
    "        full_training_data, [training_samples, validation_samples]\n",
    "    )\n",
    "\n",
    "    # initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_data, test_batch_size, shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obE13C39_2g1"
   },
   "source": [
    "## Network architecture\n",
    "In this block we want to define the architecture of our MLP. Let us define it as a module consisting of 2 fully connected linear layers. This type of layer can is provided by PyTorch through `torch.nn.Linear`. We also need to include an activation function between the two layers, e.g. Sigmoid (`torch.nn.Sigmoid`). There is plenty of [alternatives](https://pytorch.org/docs/stable/nn.html) that can be taken into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyFirstNetwork, self).__init__()\n",
    "\n",
    "        # first linear layer (input)\n",
    "        self.input_to_hidden = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "        # second linear layer (output)\n",
    "        self.hidden_to_output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # initialize bias\n",
    "        self.input_to_hidden.bias.data.fill_(0.0)\n",
    "        self.hidden_to_output.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # puts the output in (batch_size, input_dim) format\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # forward the input through the layers\n",
    "        x = self.input_to_hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden_to_output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2v8x0ZBG8y"
   },
   "source": [
    "## Optimizer\n",
    "The optimizer is the tool that takes care of actually carrying out the optimization of the paramters with respect to the chosen loss function. There is a variety of implemented optimizers in the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) module. Let's define our optimizer giving as input the network parameters, the learning rate, the weight decay coefficient and the momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, lr, wd, momentum):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(), lr=lr, weight_decay=wd, momentum=momentum\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az-K8L5oBu3F"
   },
   "source": [
    "## Loss function\n",
    "The loss/cost function expresses the value that you wish to minimize by optimizing the parameters of your network. In other words, it should efficiently express the prediction error. Given that we are addessing a multi-class classification task, a suitable choice is a cross-entropyw with softmax. This is available, along with many alternatives, in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions) module. Note that `torch.nn.CrossEntropyLoss` already applies the softmax function, i.e. we don't need to manually define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APuCISvfCrP6"
   },
   "source": [
    "## Training and test steps\n",
    "We are ready to define our training and test steps. These would be two separate functions which \n",
    "\n",
    "\n",
    "1.   **iterate** over a given set of data\n",
    "2.   **forward** the data through the neural network\n",
    "3.   **compare** the network output with the groung truth labels, to compute loss and/or evaluation metrics\n",
    "\n",
    "Additionally, during training, we need these steps to actually carry out the optimization\n",
    "\n",
    "1.   perform the backward pass (`loss.backward()`) to **compute gradients**\n",
    "2.   call the optimizer to consequently **update the weights** (`optimizer.step()`)\n",
    "3.   **reset** the gradients in order not to accumulate it (`optimizer.zero_grad()`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # iterate over the training set\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(\n",
    "            dim=1\n",
    "        )  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += (\n",
    "                loss.item()\n",
    "            )  # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGxHI0uxFCI0"
   },
   "source": [
    "## Put it all together!\n",
    "We need a compact procedure to apply all the components and functions defined so far into the actual optimization procedure. In particular, we want our model to iterate over training step and test step for multiple epochs, tracking the partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "# main funcition\n",
    "def main(\n",
    "    batch_size=128,\n",
    "    input_dim=28 * 28,\n",
    "    hidden_dim=100,\n",
    "    output_dim=10,\n",
    "    device=\"cuda:0\",\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.000001,\n",
    "    momentum=0.9,\n",
    "    epochs=10,\n",
    "):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    # get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(batch_size)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print(\n",
    "        \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "            train_loss, train_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "            val_loss, val_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\"\\tTest loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function\n",
    "        )\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        print(\"Epoch: {:d}\".format(e + 1))\n",
    "        print(\n",
    "            \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "                train_loss, train_accuracy\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "                val_loss, val_accuracy\n",
    "            )\n",
    "        )\n",
    "        print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"Train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"Validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "    print(\n",
    "        \"\\tTraining loss {:.5f}, Training accuracy {:.2f}\".format(\n",
    "            train_loss, train_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\tValidation loss {:.5f}, Validation accuracy {:.2f}\".format(\n",
    "            val_loss, val_accuracy\n",
    "        )\n",
    "    )\n",
    "    print(\"\\tTest loss {:.5f}, Test accuracy {:.2f}\".format(test_loss, test_accuracy))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJzgp_pOF-GM"
   },
   "source": [
    "## Run it!\n",
    "Let's run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
