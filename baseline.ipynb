{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwVGzdLNS3Gq"
   },
   "source": [
    "# Baseline algorithm\n",
    "In this notebook we propose a training free approach that combines CLIP zero-shot with a YOLO architecture. This method involves extracting\n",
    "all the bounding boxes proposed by YOLO and evaluating their similarity with the textual query with CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpU7k9CqWxl1"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List, Literal, Callable, TypedDict\n",
    "from jaxtyping import Array, Float, UInt\n",
    "import pickle\n",
    "import itertools as it\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pkg_resources import packaging\n",
    "import clip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Using {}.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn-eNVOpW-0Q"
   },
   "source": [
    "Download the dataset.\n",
    "**TODO:** prima di scaricare il dataset controllare che non esista già."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb\n",
    "!mv refcocog.tar.gz ./dataset/\n",
    "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
    "!rm dataset/refcocog.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9I1WtMaZLNW"
   },
   "source": [
    "Folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path(\"dataset/refcocog/\") #main dataset folder\n",
    "data_instances = Path(\"dataset/refcocog/annotations/instances.json\")  #instances.json\n",
    "data_refs = Path(\"dataset/refcocog/annotations/refs(umd).p\")  #refs(umd).p\n",
    "data_images = Path(\"dataset/refcocog/images\") #image folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKIXBKv0Y38W"
   },
   "source": [
    "Type declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split = Literal['train', 'test', 'val']\n",
    "\n",
    "\n",
    "class Info(TypedDict, total=True):\n",
    "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
    "    url: str  # http://mscoco.org/\n",
    "    version: str  # 1.0\n",
    "    year: int  # 2014\n",
    "    contributor: str  # Microsoft COCO group\n",
    "    data_created: str  # 2015-01-27 09:11:52.357475\n",
    "\n",
    "\n",
    "class Image(TypedDict, total=True):\n",
    "    license: int  # each image has an associated licence id\n",
    "    file_name: str  # file name of the image\n",
    "    coco_url: str  # example http://mscoco.org/images/131074\n",
    "    height: int\n",
    "    width: int\n",
    "    data_captured: str  # example '2013-11-21 01:03:06'\n",
    "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
    "    id: int  # id of the image\n",
    "\n",
    "\n",
    "class License(TypedDict, total=True):\n",
    "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
    "    id: int  # id of the licence\n",
    "    name: str  # example 'Attribution-NonCommercial-ShareAlike License'\n",
    "\n",
    "\n",
    "class Annotation(TypedDict, total=True):\n",
    "    segmentation: str  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
    "    area: int  # number of pixel of the described object\n",
    "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
    "    image_id: int  # id of the target image\n",
    "    bbox: UInt[Array, '4']  # bounding box coordinates [xmin, ymin, width, height]\n",
    "    category_id: int\n",
    "    id: int  # annotation id\n",
    "\n",
    "\n",
    "class Category(TypedDict, total=True):\n",
    "    supercategory: str  # example 'vehicle'\n",
    "    id: int  # category id\n",
    "    name: str  # example 'airplane'\n",
    "\n",
    "\n",
    "class Instances(TypedDict, total=True):\n",
    "    info: Info\n",
    "    images: list[Image]\n",
    "    licenses: list[License]\n",
    "    annotations: list[Annotation]\n",
    "    categories: list[Category]\n",
    "\n",
    "\n",
    "class Sentence(TypedDict, total=True):\n",
    "    tokens: list[str]  # tokenized version of referring expression\n",
    "    raw: str  # unprocessed referring expression\n",
    "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
    "    sent_it: int  # unique referring expression id\n",
    "\n",
    "\n",
    "class Ref(TypedDict, total=True):\n",
    "    image_id: int  # unique image id\n",
    "    split: Split\n",
    "    sentences: list[Sentence]\n",
    "    file_name: str  # file name of image relative to img_root\n",
    "    category_id: int  # object category label\n",
    "    ann_id: int  # id of object annotation in instance.json\n",
    "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
    "    ref_id: int  # unique id for refering expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpEU-IVsY-6U"
   },
   "source": [
    "Useful functions.\n",
    "**TODO**: commentare cosa fanno queste funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ref(x: Ref) -> Ref:\n",
    "    x['file_name'] = fix_filename(x['file_name'])\n",
    "    return x\n",
    "\n",
    "\n",
    "def fix_filename(x: str) -> str:\n",
    "    \"\"\"\n",
    "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
    "    :return:  COCO_..._[image_id].jpg\n",
    "    \"\"\"\n",
    "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w5aTzDnauXI"
   },
   "source": [
    "Read refs and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_refs, 'rb')\n",
    "refs: list[Ref] = [\n",
    "    fix_ref(ref)\n",
    "    for ref in pickle.load(f)\n",
    "]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_instances, 'r')\n",
    "instances: Instances = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVugARTka1HD"
   },
   "source": [
    "Create a mapping between annotation_id => annotation_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2annotation = {\n",
    "    x['id']: x\n",
    "    for x in instances['annotations']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ro_-cBJcCpx"
   },
   "source": [
    "Define custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "\n",
    "    #split: train, test or val\n",
    "    #img_transform: apply list of transformations on the processed images\n",
    "    #prompt_transform: apply list of transformations on the processed reference expressions\n",
    "    #bb_transform: apply list of transformations on the bounding box\n",
    "    def __init__(self,\n",
    "                 split,\n",
    "                 img_transform=lambda x: x,\n",
    "                 prompt_transform=lambda x: x[0]['sent'],\n",
    "                 bb_transform=lambda x: x\n",
    "                 ):\n",
    "        self.img_transform = img_transform\n",
    "        self.prompt_transform = prompt_transform\n",
    "        self.bb_transform = bb_transform\n",
    "\n",
    "        # Internally the dataset is a list of Tuple[Tuple[I, S], B]\n",
    "        # Such that:\n",
    "        # I: image filename\n",
    "        # S: list of reference expression objects\n",
    "        # B: bounding box UInt[torch.Tensor, '4']\n",
    "        self.items = [\n",
    "            ((i, ps), o)\n",
    "            for ref in refs\n",
    "            if ref['split'] == split\n",
    "            for i in [ref['file_name']]\n",
    "            for ps in [ref['sentences']]\n",
    "            for o in [torch.tensor(id2annotation[ref['ann_id']]['bbox'], dtype=torch.int)]\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    #return ((image, sentences), bounding box)\n",
    "    def __getitem__(self, item):\n",
    "        ((i, ps), b) = self.items[item]\n",
    "        img = read_image(os.path.join(data_images,Path(i)))\n",
    "        return (\n",
    "            (\n",
    "                self.img_transform(img),\n",
    "                self.prompt_transform(ps)\n",
    "            ),\n",
    "            self.bb_transform(b)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtIvse2VU9l"
   },
   "source": [
    "## Step 1: take an image from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJoOsCj5grYa"
   },
   "source": [
    "**TODO**: in realtà la baseline è training free. Quindi no ha molto senso splittare il dataset in training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CocoDataset(split=\"train\")\n",
    "test_dataset = CocoDataset(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, # use custom created train Dataset\n",
    "    batch_size=1,  # how many samples per batch?\n",
    "    num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "    shuffle=True   # shuffle the data?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, # use custom created test Dataset\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=False  # usually there is no need to shuffle testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INPUT\")\n",
    "\n",
    "# Get image and label from custom DataLoader\n",
    "(imgs, prompts), outputs = next(iter(train_dataloader))\n",
    "bboxes = torchvision.ops.box_convert(outputs, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "#convert the output image to pil to display the picture\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "image_pil = tensor_to_pil(imgs[0])\n",
    "image_pil.show()\n",
    "\n",
    "#print(prompts)  #TODO domanda per Luca, perché prompts è una tupla?\n",
    "\n",
    "referenceExpression = prompts[0]\n",
    "\n",
    "print(referenceExpression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNk9KlUUVkMO"
   },
   "source": [
    "## Step 2: find the bounding boxes inside the image with yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CnA4DK3UwQa"
   },
   "source": [
    "Get YOLO requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "!pip install -r yolov5/requirements.txt  # install\n",
    "!mkdir -p /root/.cache/torch/hub/  #create the folder\n",
    "!cp yolov5/requirements.txt /root/.cache/torch/hub/ #copy in the proper location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JeqNy7jU3Iv"
   },
   "source": [
    "Import the YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKvE9LIKVHQ_"
   },
   "source": [
    "Execute the model to find the bounding boxes inside the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo inference\n",
    "results = yolo_model(image_pil)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7gqltrCVYd-"
   },
   "source": [
    "## Step 3: compute the latent representation of the reference expression using clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmupgXDpWk9N"
   },
   "source": [
    "Load the CLIP model and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"RN50\")\n",
    "clip_model = clip_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KLujISGYjYU"
   },
   "source": [
    "Prepare input text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list()\n",
    "texts.append(referenceExpression)\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SF9qJSDZM4C"
   },
   "source": [
    "Execute the text encoder of CLIP to get the latent representation of the text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  texts_z = clip_model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGf5xzIHVfp7"
   },
   "source": [
    "## Step 4: for each bounding box: i. compute the representation of the crop in the latent space; ii. evaluate the similarity with the reference expression; Finally, consider only the bounding box with the higher similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtNuY65Gl-Fv"
   },
   "source": [
    "Get cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBoxes = results.xyxy  #list\n",
    "\n",
    "#boundingBoxes first image\n",
    "img0_boundingBoxes = boundingBoxes[0] #torch.Size([num_bounding_boxes, 6])\n",
    "                                      #xmin,ymin,xmax,ymax,confidence,class\n",
    "                                      #274.06390,231.20389,392.66345,372.59018,0.93251,23.00000\n",
    "\n",
    "cropped_images = list()\n",
    "displayImages = list()\n",
    "for bbox in img0_boundingBoxes:\n",
    "  rectangle = (int(bbox[0]),  #x of the top left corner\n",
    "               int(bbox[1]),  #y of the top left corner\n",
    "               int(bbox[2]),  #x of the bottom right corner\n",
    "               int(bbox[3])   #y of the bottom right corner\n",
    "               )\n",
    "  displayImages.append(image_pil.crop(rectangle))\n",
    "  cropped_images.append(preprocess(image_pil.crop(rectangle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in displayImages:\n",
    "  img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKEJYQBW89V4"
   },
   "source": [
    "Compute the representation of the crops in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cropped_images[0]), cropped_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_input_images = torch.tensor(np.stack(cropped_images)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_input_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    images_z = clip_model.encode_image(clip_input_images).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2NWEQRxBufT"
   },
   "source": [
    "Evaluate the cosine similarity between each bounding box and the reference expression. Finally, consider only the bounding box with the higher similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  logits_per_image, logits_per_text = clip_model(clip_input_images, text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQ0qdutDavX"
   },
   "source": [
    "Get index of the bounding box which is characterized by the highest similarity score with respect to the input reference expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch = torch.argmax(logits_per_text)\n",
    "bestBbox = img0_boundingBoxes[bestMatch]\n",
    "\n",
    "r, g, b = torch.randint(0, 256, [3]).tolist()\n",
    "img_bbox = torchvision.utils.draw_bounding_boxes(\n",
    "    image=imgs[0],\n",
    "    boxes=bestBbox[:4].unsqueeze(0),\n",
    "    colors=(r, g, b),\n",
    "    width=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pil = tensor_to_pil(img_bbox)\n",
    "output_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brRy1JaiGWv5"
   },
   "source": [
    "Output the groundtruth bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bbox = torchvision.utils.draw_bounding_boxes(\n",
    "    image=img_bbox,\n",
    "    boxes=bboxes[0].unsqueeze(0),\n",
    "    colors=(r, g, b),\n",
    "    width=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pil = tensor_to_pil(img_bbox)\n",
    "output_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NArurlg2WD3A"
   },
   "source": [
    "## Step 5: evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng2LLpE6f6Os"
   },
   "source": [
    "$$\n",
    "J(A, B) = \\frac {|A \\cap B|} {|A \\cup B|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A$ is the ground truth bbox\n",
    "- $B$ is our bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.ops.box_iou(bboxes.cuda(), bestBbox[:4].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W63dOsQ9f8d-"
   },
   "source": [
    "$$\n",
    "\\text{cosine similarity } A, B := \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}||  ||\\mathbf{B}||}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf A$ is the ground truth bbox in CLIP latin space\n",
    "- $\\mathbf B$ is out bbox in CLIP latin space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle = bboxes[0].tolist()\n",
    "ground_truth_crop = image_pil.crop(rectangle)\n",
    "\n",
    "rectangle = torch.tensor(bestBbox[:4], dtype=torch.int).tolist()\n",
    "best_crop = image_pil.crop(rectangle)\n",
    "\n",
    "A = clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).cuda().unsqueeze(0))\n",
    "B = clip_model.encode_image(torch.tensor(preprocess(best_crop)).cuda().unsqueeze(0))\n",
    "\n",
    "F.cosine_similarity(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4suXUxXhwMl"
   },
   "source": [
    "$$\n",
    "||\\mathbf x - \\mathbf y||\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(torch.tensor(A, dtype=torch.float), torch.tensor(B, dtype=torch.float), p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB773o8qh_8b"
   },
   "source": [
    "$$\n",
    "\\text{Pre} = \\frac{TP}{TP + FP} \\qquad \\text{Rec} = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
