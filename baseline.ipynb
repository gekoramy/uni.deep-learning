{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwVGzdLNS3Gq"
   },
   "source": [
    "# Baseline algorithm\n",
    "In this notebook we propose a training free approach that combines CLIP zero-shot with a YOLO architecture. This method involves extracting\n",
    "all the bounding boxes proposed by YOLO and evaluating their similarity with the textual query with CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpU7k9CqWxl1"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget \"https://raw.githubusercontent.com/ultralytics/yolov5/v7.0/requirements.txt\" -O \"yolo-requirements.txt\"\n",
    "mkdir -p /root/.cache/torch/hub\n",
    "cp yolo-requirements.txt /root/.cache/torch/hub/requirements.txt\n",
    "\n",
    "tee requirements.txt << END\n",
    "ftfy\n",
    "jaxtyping\n",
    "jupyter\n",
    "matplotlib\n",
    "pydantic\n",
    "regex\n",
    "torch\n",
    "torchvision\n",
    "tqdm\n",
    "END\n",
    "\n",
    "pip install -r requirements.txt\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "pip install -r yolo-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List, Literal, Callable, Optional, Mapping, TypeVar\n",
    "from jaxtyping import Array, Float, UInt, Int\n",
    "import pickle\n",
    "import itertools as it\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pkg_resources import packaging\n",
    "import clip\n",
    "import numpy as np\n",
    "from pydantic.dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn-eNVOpW-0Q"
   },
   "source": [
    "Download the dataset.\n",
    "**TODO:** prima di scaricare il dataset controllare che non esista già."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "if ! [ -d dataset ]; then\n",
    "  mkdir dataset &&\n",
    "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
    "  mv refcocog.tar.gz ./dataset/ &&\n",
    "  tar -xf dataset/refcocog.tar.gz -C dataset &&\n",
    "  rm dataset/refcocog.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9I1WtMaZLNW"
   },
   "source": [
    "Folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path(\"dataset/refcocog/\") #main dataset folder\n",
    "data_instances = Path(\"dataset/refcocog/annotations/instances.json\")  #instances.json\n",
    "data_refs = Path(\"dataset/refcocog/annotations/refs(umd).p\")  #refs(umd).p\n",
    "data_images = Path(\"dataset/refcocog/images\") #image folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKIXBKv0Y38W"
   },
   "source": [
    "Type declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split = Literal['train', 'test', 'val']\n",
    "\n",
    "@dataclass\n",
    "class Info:\n",
    "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
    "    url: str  # http://mscoco.org/\n",
    "    version: str  # 1.0\n",
    "    year: int  # 2014\n",
    "    contributor: str  # Microsoft COCO group\n",
    "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
    "\n",
    "@dataclass\n",
    "class Image:\n",
    "    license: int  # each image has an associated licence id\n",
    "    file_name: str  # file name of the image\n",
    "    coco_url: str  # example http://mscoco.org/images/131074\n",
    "    height: int\n",
    "    width: int\n",
    "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
    "    id: int  # id of the imag\n",
    "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
    "\n",
    "@dataclass\n",
    "class License:\n",
    "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
    "    id: int  # id of the licence\n",
    "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
    "\n",
    "@dataclass\n",
    "class Annotation:\n",
    "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
    "    area: int  # number of pixel of the described object\n",
    "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
    "    image_id: int  # id of the target image\n",
    "    bbox: tuple[int, int, int, int]  # bounding box coordinates [xmin, ymin, width, height]\n",
    "    category_id: int\n",
    "    id: int  # annotation id\n",
    "\n",
    "@dataclass\n",
    "class Category:\n",
    "    supercategory: str  # example 'vehicle'\n",
    "    id: int  # category id\n",
    "    name: str  # example 'airplane'\n",
    "\n",
    "@dataclass\n",
    "class Instances:\n",
    "    info: Info\n",
    "    images: list[Image]\n",
    "    licenses: list[License]\n",
    "    annotations: list[Annotation]\n",
    "    categories: list[Category]\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    tokens: list[str]  # tokenized version of referring expression\n",
    "    raw: str  # unprocessed referring expression\n",
    "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
    "    sent_id: int  # unique referring expression id\n",
    "\n",
    "@dataclass\n",
    "class Ref:\n",
    "    image_id: int  # unique image id\n",
    "    split: Split\n",
    "    sentences: list[Sentence]\n",
    "    file_name: str  # file name of image relative to img_root\n",
    "    category_id: int  # object category label\n",
    "    ann_id: int  # id of object annotation in instance.json\n",
    "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
    "    ref_id: int  # unique id for refering expression\n",
    "\n",
    "I = TypeVar('I')\n",
    "P = TypeVar('P')\n",
    "B = TypeVar('B')\n",
    "T = TypeVar('T')\n",
    "\n",
    "Img = UInt[torch.Tensor, 'C W H']\n",
    "BBox = UInt[torch.Tensor, '4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpEU-IVsY-6U"
   },
   "source": [
    "Useful functions.\n",
    "**TODO**: commentare cosa fanno queste funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ref(x: Ref) -> Ref:\n",
    "    x.file_name = fix_filename(x.file_name)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fix_filename(x: str) -> str:\n",
    "    \"\"\"\n",
    "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
    "    :return:  COCO_..._[image_id].jpg\n",
    "    \"\"\"\n",
    "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w5aTzDnauXI"
   },
   "source": [
    "Read refs and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_refs, 'rb')\n",
    "raw = pickle.load(f)\n",
    "f.close()\n",
    "refs: list[Ref] = [\n",
    "    fix_ref(Ref(**ref))\n",
    "    for ref in raw\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_instances, 'r')\n",
    "raw = json.load(f)\n",
    "f.close()\n",
    "instances: Instances = Instances(**raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVugARTka1HD"
   },
   "source": [
    "Create a mapping between annotation_id => annotation_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2annotation: Mapping[int, Annotation] = {\n",
    "    x.id: x\n",
    "    for x in instances.annotations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ro_-cBJcCpx"
   },
   "source": [
    "Define custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset[tuple[I, P, B]]):\n",
    "\n",
    "    #split: train, test or val\n",
    "    #img_transform: apply list of transformations on the processed images\n",
    "    #prompt_transform: apply list of transformations on the processed reference expressions\n",
    "    #bb_transform: apply list of transformations on the bounding box\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: Split,\n",
    "        img_transform: Callable[[Img], I] = lambda x: x,\n",
    "        prompt_transform: Callable[[list[Sentence]], P] = lambda ps: [ p.sent for p in ps ],\n",
    "        bb_transform: Callable[[UInt[torch.Tensor, '4']], B] = lambda x: x\n",
    "    ):\n",
    "        self.img_transform = img_transform\n",
    "        self.prompt_transform = prompt_transform\n",
    "        self.bb_transform = bb_transform\n",
    "\n",
    "        # Internally the dataset is a list of tuple[str, list[Sentence], UInt[torch.Tensor, '4']]\n",
    "        # Such that:\n",
    "        # str                     : image filename\n",
    "        # list[Sentence]          : list of reference expression objects\n",
    "        # UInt[torch.Tensor, '4'] : bounding box \n",
    "        self.items: list[tuple[str, list[Sentence], UInt[torch.Tensor, '4']]] = [\n",
    "            (i, ps, o)\n",
    "            for ref in refs\n",
    "            if ref.split == split\n",
    "            for i in [os.path.join(data_images, Path(ref.file_name))]\n",
    "            for ps in [ref.sentences]\n",
    "            for o in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.int)]\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[I, P, B]:\n",
    "        i, ps, b = self.items[item]\n",
    "        img = read_image(i)\n",
    "        return (\n",
    "            self.img_transform(img),\n",
    "            self.prompt_transform(ps),\n",
    "            self.bb_transform(b),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtIvse2VU9l"
   },
   "source": [
    "## Step 1: take an image from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJoOsCj5grYa"
   },
   "source": [
    "**TODO**: in realtà la baseline è training free. Quindi no ha molto senso splittare il dataset in training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CocoDataset(split='train')\n",
    "test_dataset = CocoDataset(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, # use custom created train Dataset\n",
    "    batch_size=1,  # how many samples per batch?\n",
    "    num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "    shuffle=False,   # shuffle the data?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, # use custom created test Dataset\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=False  # usually there is no need to shuffle testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INPUT\")\n",
    "\n",
    "# Get image and label from custom DataLoader\n",
    "[img], (prompts), [true_xywh] = next(iter(train_dataloader))\n",
    "\n",
    "[true_xyxy] = torchvision.ops.box_convert(true_xywh.unsqueeze(0), in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "# convert the output image to pil to display the picture\n",
    "img_pil = transforms.ToPILImage()(img)\n",
    "img_pil.show()\n",
    "\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNk9KlUUVkMO"
   },
   "source": [
    "## Step 2: find the bounding boxes inside the image with yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JeqNy7jU3Iv"
   },
   "source": [
    "Import the YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKvE9LIKVHQ_"
   },
   "source": [
    "Execute the model to find the bounding boxes inside the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = yolo_model(img_pil)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7gqltrCVYd-"
   },
   "source": [
    "## Step 3: compute the latent representation of the reference expression using clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmupgXDpWk9N"
   },
   "source": [
    "Load the CLIP model and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load('RN50')\n",
    "clip_model = clip_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KLujISGYjYU"
   },
   "source": [
    "Prepare input text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_tokens: Int[torch.Tensor, 'P 77'] = clip.tokenize([\n",
    "    template.format(prompt)\n",
    "    for template in [\"A photo of {}\", \"We can see {}\"]\n",
    "    for (prompt, ) in prompts  # <- ¯\\_(ツ)_/¯\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SF9qJSDZM4C"
   },
   "source": [
    "Execute the text encoder of CLIP to get the latent representation of the text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  prompts_z: Float[torch.Tensor, 'P 124'] = clip_model.encode_text(prompts_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGf5xzIHVfp7"
   },
   "source": [
    "## Step 4: for each bounding box: i. compute the representation of the crop in the latent space; ii. evaluate the similarity with the reference expression; Finally, consider only the bounding box with the higher similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtNuY65Gl-Fv"
   },
   "source": [
    "Get cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xmin,      ymin,      xmax,      ymax,      confidence, class\n",
    "# 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
    "bboxes: Float[torch.Tensor, 'X 6'] = predictions.xyxy\n",
    "\n",
    "crops = [\n",
    "    img_pil.crop((xmin, ymin, xmax, ymax))\n",
    "    for bbox in bboxes[0]\n",
    "    for [xmin, ymin, xmax, ymax, _, _]  in [bbox.tolist()]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop in crops:\n",
    "    crop.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKEJYQBW89V4"
   },
   "source": [
    "Compute the representation of the crops in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_crops: Float[torch.Tensor, 'X 3 244 244'] = torch.stack([\n",
    "    preprocess(crop)\n",
    "    for crop in crops\n",
    "]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    crops_z: Float[torch.Tensor, 'X 1024'] = clip_model.encode_image(preprocess_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2NWEQRxBufT"
   },
   "source": [
    "Evaluate the cosine similarity between each bounding box and the reference expression. Finally, consider only the bounding box with the higher similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ass_z: tuple[Float[torch.Tensor, 'X P'], Float[torch.Tensor, 'P X']] = clip_model(preprocess_crops, prompts_tokens)\n",
    "    logits_per_crop, logits_per_prompt = ass_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQ0qdutDavX"
   },
   "source": [
    "Get index of the bounding box which is characterized by the highest similarity score with respect to the input reference expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(logits_per_prompt, 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match = torch.argmax(torch.max(logits_per_prompt, 0).values)\n",
    "best_bbox = bboxes[0][best_match]\n",
    "\n",
    "r, g, b = torch.randint(0, 256, [3]).tolist()\n",
    "img_bbox = torchvision.utils.draw_bounding_boxes(\n",
    "    image=img,\n",
    "    boxes=best_bbox[:4].unsqueeze(0),\n",
    "    colors=(r, g, b),\n",
    "    width=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pil = tensor_to_pil(img_bbox)\n",
    "output_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brRy1JaiGWv5"
   },
   "source": [
    "Output the groundtruth bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bbox = torchvision.utils.draw_bounding_boxes(\n",
    "    image=img_bbox,\n",
    "    boxes=true_xyxy.unsqueeze(0),\n",
    "    colors=(r, g, b),\n",
    "    width=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pil = tensor_to_pil(img_bbox)\n",
    "output_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NArurlg2WD3A"
   },
   "source": [
    "## Step 5: evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng2LLpE6f6Os"
   },
   "source": [
    "$$\n",
    "J(A, B) = \\frac {|A \\cap B|} {|A \\cup B|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A$ is the ground truth bbox\n",
    "- $B$ is our bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.ops.box_iou(true_xyxy.unsqueeze(0), best_bbox[:4].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W63dOsQ9f8d-"
   },
   "source": [
    "$$\n",
    "\\text{cosine similarity } A, B := \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}||  ||\\mathbf{B}||}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf A$ is the ground truth bbox in CLIP latin space\n",
    "- $\\mathbf B$ is out bbox in CLIP latin space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle = true_xywh.tolist()\n",
    "ground_truth_crop = img_pil.crop(rectangle)\n",
    "\n",
    "rectangle = torch.tensor(best_bbox[:4], dtype=torch.int).tolist()\n",
    "best_crop = img_pil.crop(rectangle)\n",
    "\n",
    "A = clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).cuda().unsqueeze(0))\n",
    "B = clip_model.encode_image(torch.tensor(preprocess(best_crop)).cuda().unsqueeze(0))\n",
    "\n",
    "F.cosine_similarity(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4suXUxXhwMl"
   },
   "source": [
    "$$\n",
    "||\\mathbf x - \\mathbf y||\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(torch.tensor(A, dtype=torch.float), torch.tensor(B, dtype=torch.float), p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB773o8qh_8b"
   },
   "source": [
    "$$\n",
    "\\text{Pre} = \\frac{TP}{TP + FP} \\qquad \\text{Rec} = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
