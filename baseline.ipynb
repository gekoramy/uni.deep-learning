{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwVGzdLNS3Gq"
   },
   "source": [
    "Baseline algorithm\n",
    "---\n",
    "\n",
    "In this notebook we propose a training free approach that combines CLIP zero-shot with a YOLO architecture.\n",
    "This method involves extracting all the bounding boxes proposed by YOLO and evaluating their similarity with the textual query with CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpU7k9CqWxl1"
   },
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget \"https://raw.githubusercontent.com/ultralytics/yolov5/v7.0/requirements.txt\" -O \"yolo-requirements.txt\"\n",
    "mkdir -p /root/.cache/torch/hub\n",
    "cp yolo-requirements.txt /root/.cache/torch/hub/requirements.txt\n",
    "\n",
    "tee requirements.txt << END\n",
    "ftfy\n",
    "jaxtyping\n",
    "jupyter\n",
    "matplotlib\n",
    "pydantic\n",
    "regex\n",
    "torch\n",
    "torchvision\n",
    "tqdm\n",
    "END\n",
    "\n",
    "pip install -q -r requirements.txt\n",
    "pip install -q git+https://github.com/openai/CLIP.git\n",
    "pip install -q -r yolo-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from jaxtyping import Float, UInt, Int\n",
    "from pydantic.dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from typing import Literal, Callable, Mapping, TypeVar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: Literal['cpu', 'cuda'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn-eNVOpW-0Q"
   },
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "if ! [ -d dataset ]; then\n",
    "  mkdir dataset &&\n",
    "  gdown 1P8a1g76lDJ8cMIXjNDdboaRR5-HsVmUb &&\n",
    "  tar -xf refcocog.tar.gz -C dataset &&\n",
    "  rm refcocog.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('dataset', 'refcocog', '')\n",
    "data_instances = os.path.join(root, 'annotations', 'instances.json')\n",
    "data_refs = os.path.join(root, 'annotations', 'refs(umd).p')\n",
    "data_images = os.path.join(root, 'images', '') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKIXBKv0Y38W"
   },
   "source": [
    "Type declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = TypeVar('I')\n",
    "P = TypeVar('P')\n",
    "B = TypeVar('B')\n",
    "T = TypeVar('T')\n",
    "\n",
    "Img = UInt[torch.Tensor, 'C W H']\n",
    "BBox = UInt[torch.Tensor, '4']\n",
    "Split = Literal['train', 'test', 'val']\n",
    "\n",
    "@dataclass\n",
    "class Info:\n",
    "    description: str  # This is stable 1.0 version of the 2014 MS COCO dataset.\n",
    "    url: str  # http://mscoco.org/\n",
    "    version: str  # 1.0\n",
    "    year: int  # 2014\n",
    "    contributor: str  # Microsoft COCO group\n",
    "    date_created: datetime  # 2015-01-27 09:11:52.357475\n",
    "\n",
    "@dataclass\n",
    "class Image:\n",
    "    license: int  # each image has an associated licence id\n",
    "    file_name: str  # file name of the image\n",
    "    coco_url: str  # example http://mscoco.org/images/131074\n",
    "    height: int\n",
    "    width: int\n",
    "    flickr_url: str  # example http://farm9.staticflickr.com/8308/7908210548_33e\n",
    "    id: int  # id of the imag\n",
    "    date_captured: datetime  # example '2013-11-21 01:03:06'\n",
    "\n",
    "@dataclass\n",
    "class License:\n",
    "    url: str  # example http://creativecommons.org/licenses/by-nc-sa/2.0/\n",
    "    id: int  # id of the licence\n",
    "    name: str  # example 'Attribution-NonCommercial-ShareAlike License\n",
    "\n",
    "@dataclass\n",
    "class Annotation:\n",
    "    # segmentation: list[list[float]]  # description of the mask; example [[44.17, 217.83, 36.21, 219.37, 33.64, 214.49, 31.08, 204.74, 36.47, 202.68, 44.17, 203.2]]\n",
    "    area: int  # number of pixel of the described object\n",
    "    iscrowd: Literal[1, 0]  # Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people)\n",
    "    image_id: int  # id of the target image\n",
    "    bbox: tuple[int, int, int, int]  # bounding box coordinates [xmin, ymin, width, height]\n",
    "    category_id: int\n",
    "    id: int  # annotation id\n",
    "\n",
    "@dataclass\n",
    "class Category:\n",
    "    supercategory: str  # example 'vehicle'\n",
    "    id: int  # category id\n",
    "    name: str  # example 'airplane'\n",
    "\n",
    "@dataclass\n",
    "class Instances:\n",
    "    info: Info\n",
    "    images: list[Image]\n",
    "    licenses: list[License]\n",
    "    annotations: list[Annotation]\n",
    "    categories: list[Category]\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    tokens: list[str]  # tokenized version of referring expression\n",
    "    raw: str  # unprocessed referring expression\n",
    "    sent: str  # referring expression with mild processing, lower case, spell correction, etc.\n",
    "    sent_id: int  # unique referring expression id\n",
    "\n",
    "@dataclass\n",
    "class Ref:\n",
    "    image_id: int  # unique image id\n",
    "    split: Split\n",
    "    sentences: list[Sentence]\n",
    "    file_name: str  # file name of image relative to img_root\n",
    "    category_id: int  # object category label\n",
    "    ann_id: int  # id of object annotation in instance.json\n",
    "    sent_ids: list[int]  # same ids as nested sentences[...][sent_id]\n",
    "    ref_id: int  # unique id for refering expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpEU-IVsY-6U"
   },
   "source": [
    "Read the dataset infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ref(x: Ref) -> Ref:\n",
    "    x.file_name = fix_filename(x.file_name)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fix_filename(x: str) -> str:\n",
    "    \"\"\"\n",
    "    :param x: COCO_..._[image_id]_[annotation_id].jpg\n",
    "    :return:  COCO_..._[image_id].jpg\n",
    "    \"\"\"\n",
    "    return re.sub('_\\d+\\.jpg$', '.jpg', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_refs, 'rb')\n",
    "raw = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs: list[Ref] = [\n",
    "    fix_ref(Ref(**ref))\n",
    "    for ref in raw\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_instances, 'r')\n",
    "raw = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances: Instances = Instances(**raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2annotation: Mapping[int, Annotation] = {\n",
    "    x.id: x\n",
    "    for x in instances.annotations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ro_-cBJcCpx"
   },
   "source": [
    "Define custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset[tuple[I, P, B]]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: Split,\n",
    "        img_transform: Callable[[Img], I] = lambda x: x,\n",
    "        prompt_transform: Callable[[list[Sentence]], P] = lambda ps: [ p.sent for p in ps ],\n",
    "        bb_transform: Callable[[UInt[torch.Tensor, '4']], B] = lambda x: x\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param split: train, test or val\n",
    "        :param img_transform: apply transformation on the processed images\n",
    "        :param prompt_transform: apply transformation on the prompts\n",
    "        :param bb_transform: apply transformation on the bounding box\n",
    "        \"\"\"\n",
    "        self.img_transform = img_transform\n",
    "        self.prompt_transform = prompt_transform\n",
    "        self.bb_transform = bb_transform\n",
    "\n",
    "        # Internally the dataset is a list of tuple[str, list[Sentence], UInt[torch.Tensor, '4']]\n",
    "        # Such that:\n",
    "        # str                     : image filename\n",
    "        # list[Sentence]          : list of reference expression objects\n",
    "        # UInt[torch.Tensor, '4'] : bounding box \n",
    "        self.items: list[tuple[str, list[Sentence], UInt[torch.Tensor, '4']]] = [\n",
    "            (i, ps, o)\n",
    "            for ref in refs\n",
    "            if ref.split == split\n",
    "            for i in [os.path.join(data_images, ref.file_name)]\n",
    "            for ps in [ref.sentences]\n",
    "            for o in [torch.tensor(id2annotation[ref.ann_id].bbox, dtype=torch.int)]\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[I, P, B]:\n",
    "        i, ps, b = self.items[item]\n",
    "        return (\n",
    "            self.img_transform(read_image(i)),\n",
    "            self.prompt_transform(ps),\n",
    "            self.bb_transform(b),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJoOsCj5grYa"
   },
   "source": [
    "**TODO**: in realtà la baseline è training free. Quindi no ha molto senso splittare il dataset in training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='train')\n",
    "test_dataset: Dataset[tuple[Img, list[str], UInt[torch.Tensor, '4']]] = CocoDataset(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, # use custom created train Dataset\n",
    "    batch_size=1,  # how many samples per batch?\n",
    "    num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "    shuffle=False,   # shuffle the data?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, # use custom created test Dataset\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=False  # usually there is no need to shuffle testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "yolo_model.to(device=device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load('RN50')\n",
    "clip_model = clip_model.to(device=device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious: list[float] = []\n",
    "coss: list[float] = []\n",
    "euds: list[float] = []\n",
    "\n",
    "batch: tuple[UInt[torch.Tensor, '1 C W H'], tuple[list[tuple[str]]], UInt[torch.Tensor, '1 4']]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(iter(train_dataloader)):\n",
    "        [img], (prompts), [true_xywh] = batch\n",
    "\n",
    "        [true_xyxy] = torchvision.ops.box_convert(true_xywh.unsqueeze(0), in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "        img_pil: Image = transforms.ToPILImage()(img)\n",
    "\n",
    "        # yolo bboxes\n",
    "        predictions = yolo_model(img_pil)\n",
    "\n",
    "        # xmin,      ymin,      xmax,      ymax,      confidence, class\n",
    "        # 274.06390, 231.20389, 392.66345, 372.59018, 0.93251,    23.00000\n",
    "        bboxes: Float[torch.Tensor, 'X 6'] = predictions.xyxy[0]\n",
    "\n",
    "        # if empty, put a bbox equal to image size\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = torch.tensor([[0, 0, img.size()[1], img.size()[2], 0, 0]], dtype=torch.float)\n",
    "\n",
    "        # from yolo bboxes to cropped images\n",
    "        crops: list[Image] = [\n",
    "            img_pil.crop((xmin, ymin, xmax, ymax))\n",
    "            for bbox in bboxes\n",
    "            for [xmin, ymin, xmax, ymax, _, _] in [bbox.tolist()]\n",
    "        ]\n",
    "\n",
    "        # clip preprocess on cropped images\n",
    "        preprocess_crops: Float[torch.Tensor, 'X 3 244 244'] = torch.stack([\n",
    "            preprocess(crop)\n",
    "            for crop in crops\n",
    "        ])\n",
    "\n",
    "        # format each available prompt\n",
    "        prompts_tokens: Int[torch.Tensor, 'P 77'] = clip.tokenize([\n",
    "            template.format(prompt)\n",
    "            for template in [\"{}\", \"A photo of {}\", \"We can see {}\"]\n",
    "            for (prompt,) in prompts  # <- ¯\\_(ツ)_/¯\n",
    "        ])\n",
    "\n",
    "        # clip scores\n",
    "        ass_z: tuple[Float[torch.Tensor, 'X P'], Float[torch.Tensor, 'P X']] = clip_model(preprocess_crops, prompts_tokens)\n",
    "        _, logits_per_prompt = ass_z\n",
    "\n",
    "        # final prediction\n",
    "        best_match: int = torch.argmax(torch.max(logits_per_prompt, 0).values).item()\n",
    "        prediction_bbox: Float[torch.Tensor, '4'] = bboxes[best_match][:4]\n",
    "\n",
    "        # metrics\n",
    "        iou: float = torchvision.ops.box_iou(true_xyxy.unsqueeze(0), prediction_bbox.unsqueeze(0)).item()\n",
    "        ious.append(iou)\n",
    "\n",
    "        rectangle: tuple[int, int, int, int] = true_xyxy.tolist()\n",
    "        ground_truth_crop = img_pil.crop(rectangle)\n",
    "\n",
    "        rectangle: tuple[int, int, int, int] = torch.tensor(prediction_bbox, dtype=torch.int).tolist()\n",
    "        prediction_crop = img_pil.crop(rectangle)\n",
    "\n",
    "        # from float16 to float32\n",
    "        X: Float[torch.Tensor, '1'] = torch.tensor(\n",
    "            clip_model.encode_image(torch.tensor(preprocess(ground_truth_crop)).unsqueeze(0)),\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        Y: Float[torch.Tensor, '1'] = torch.tensor(\n",
    "            clip_model.encode_image(torch.tensor(preprocess(prediction_crop)).unsqueeze(0)),\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        cos: float = F.cosine_similarity(X, Y).item()\n",
    "        coss.append(cos)\n",
    "\n",
    "        eud: float = torch.cdist(X, Y, p=2).item()\n",
    "        euds.append(eud)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.tensor(ious, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.tensor(coss, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.tensor(euds, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng2LLpE6f6Os"
   },
   "source": [
    "$$\n",
    "J(X, Y) = \\frac {|X \\cap Y|} {|X \\cup Y|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X$ is the ground truth bbox\n",
    "- $B$ is our bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W63dOsQ9f8d-"
   },
   "source": [
    "$$\n",
    "\\text{cosine similarity } X, Y := \\cos(\\theta) = \\frac{\\mathbf{X} \\cdot \\mathbf{Y}}{||\\mathbf{X}||  ||\\mathbf{Y}||} \\qquad \\text{euclidean distance } X, Y := ||\\mathbf X - \\mathbf Y||\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf X$ is the ground truth bbox in CLIP latin space\n",
    "- $\\mathbf B$ is out bbox in CLIP latin space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB773o8qh_8b"
   },
   "source": [
    "$$\n",
    "\\text{Pre} = \\frac{TP}{TP + FP} \\qquad \\text{Rec} = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
